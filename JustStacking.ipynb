{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Just a nice stacking ensemble model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n",
      "clusters_cat.npy\n",
      "clusters_cat.npy_01.npy\n",
      "clusters_cat.npy_02.npy\n",
      "clusters_cont.npy\n",
      "clusters_cont.npy_01.npy\n",
      "clusters_cont.npy_02.npy\n",
      "clusters.npy\n",
      "clusters.npy_01.npy\n",
      "clusters.npy_02.npy\n",
      "grid_L2_KNN.pkl\n",
      "grid_L2_Lin.pkl\n",
      "grid_regr0.pkl\n",
      "grid_regr1.pkl\n",
      "grid_regr2.pkl\n",
      "grid_regr3.pkl\n",
      "MAE_tracking.npy\n",
      "oldmodels\n",
      "x_layer2.npy\n",
      "x_layer2.npy_01.npy\n",
      "x_layer2_w_clusters.npy\n",
      "x_layer2_w_clusters.npy_01.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "cachedir=\"./cache/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", cachedir]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift_quick(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(cachedir+filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(cachedir+filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        print(\"debug 1\")\n",
    "        grid_search.fit(x,y)\n",
    "        print \"debug2\"\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,cachedir+filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####unused!, but for a reference idea\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "        \n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    combineddata[feat] = combineddata[comb[0]] + combineddata[comb[1]]\n",
    "    #combineddata[feat] = combineddata[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "combineddata[\"cont1\"] = np.sqrt(combineddata[\"cont1\"])\n",
    "combineddata[\"cont4\"] = np.sqrt(combineddata[\"cont4\"])\n",
    "combineddata[\"cont5\"] = np.sqrt(combineddata[\"cont5\"])\n",
    "combineddata[\"cont8\"] = np.sqrt(combineddata[\"cont8\"])\n",
    "combineddata[\"cont10\"] = np.sqrt(combineddata[\"cont10\"])\n",
    "combineddata[\"cont11\"] = np.sqrt(combineddata[\"cont11\"])\n",
    "combineddata[\"cont12\"] = np.sqrt(combineddata[\"cont12\"])\n",
    "\n",
    "combineddata[\"cont6\"] = np.log(combineddata[\"cont6\"] + 0000.1)\n",
    "combineddata[\"cont7\"] = np.log(combineddata[\"cont7\"] + 0000.1)\n",
    "combineddata[\"cont9\"] = np.log(combineddata[\"cont9\"] + 0000.1)\n",
    "combineddata[\"cont13\"] = np.log(combineddata[\"cont13\"] + 0000.1)\n",
    "combineddata[\"cont14\"] = (np.maximum(combineddata[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    57\n",
      "1    29\n",
      "2    23\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(cachedir+filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(cachedir+filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift_quick(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],cachedir+filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters_cont loaded and attached\n",
      "0    14\n",
      "1    44\n",
      "2    53\n",
      "Name: clusters_cont, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for cont for each row\n",
    "filename='clusters_cont.npy'\n",
    "cats = [feat for feat in data.columns if 'cat' in feat]\n",
    "if os.path.isfile(cachedir+filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters_cont']=joblib.load(cachedir+filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters_cont']=kmeansPlusmeanshift_quick(combineddata.drop(['id','loss'],1).drop(cats,1))\n",
    "    joblib.dump(combineddata['clusters_cont'],cachedir+filename)\n",
    "print(\"clusters_cont loaded and attached\")\n",
    "print(combineddata.head(3)['clusters_cont'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters_cat loaded and attached\n",
      "0    31\n",
      "1     9\n",
      "2    29\n",
      "Name: clusters_cat, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for cont for each row\n",
    "filename='clusters_cat.npy'\n",
    "conts = [feat for feat in data.columns if 'cont' in feat]\n",
    "if os.path.isfile(cachedir+filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters_cat']=joblib.load(cachedir+filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters_cat']=kmeansPlusmeanshift_quick(combineddata.drop(['id','loss'],1).drop(conts,1))\n",
    "    joblib.dump(combineddata['clusters_cat'],cachedir+filename)\n",
    "print(\"clusters_cat loaded and attached\")\n",
    "print(combineddata.head(3)['clusters_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# scale the clusters\n",
    "\n",
    "#scale the columns we see\n",
    "cluster_scaler= MinMaxScaler()   \n",
    "combineddata['clusters']= cluster_scaler.fit_transform(combineddata['clusters'].values)\n",
    "combineddata['clusters_cont']= cluster_scaler.fit_transform(combineddata['clusters_cont'].values)\n",
    "combineddata['clusters_cat']= cluster_scaler.fit_transform(combineddata['clusters_cat'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 135 entries, cat1 to clusters_cat\n",
      "dtypes: float64(134), int64(1)\n",
      "memory usage: 194.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "      <th>clusters</th>\n",
       "      <th>clusters_cont</th>\n",
       "      <th>clusters_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.392405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.113924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.291139</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.367089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10      ...       \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0      ...        \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0      ...        \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0      ...        \n",
       "\n",
       "     cont10    cont11    cont12    cont13    cont14     loss  id  clusters  \\\n",
       "0  0.916140  0.744792  0.761787 -0.070392  0.984628  2213.18   1  0.721519   \n",
       "1  0.664384  0.560798  0.585682 -0.330645  0.343682  1283.60   2  0.367089   \n",
       "2  0.571049  0.599347  0.591963 -1.211326  1.018094  3005.09   5  0.291139   \n",
       "\n",
       "   clusters_cont  clusters_cat  \n",
       "0       0.177215      0.392405  \n",
       "1       0.556962      0.113924  \n",
       "2       0.670886      0.367089  \n",
       "\n",
       "[3 rows x 135 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "#y=data['loss'].values\n",
    "shift=200\n",
    "y = np.log(data['loss']+shift).ravel()\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "display(data.info())\n",
    "display(data.head(3))\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [3, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {'alpha': [0.05, 0.5, 1, 2, 4, 40, 140, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 7, 10, 25, 50, 200, 500]}]\n",
      "('number of scikitlearn regressors to use:', 3)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "#regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                       # dict(n_neighbors=[2,5,7,15],\n",
    "                             #leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.20,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "Full GridSearch run time:0.075s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.22672+0.00177631\ttest-mae:7.22672+0.00536231\n",
      "[100]\ttrain-mae:2.64648+0.000645226\ttest-mae:2.64657+0.00398503\n",
      "[200]\ttrain-mae:0.981322+0.000437563\ttest-mae:0.982543+0.00375753\n"
     ]
    }
   ],
   "source": [
    "# XGB!\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#my first tries:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:68.328s\n",
      "Mean abs error: 1212.15\n",
      "-predict time:4.074s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.811s\n",
      "Mean abs error: 1278.46\n",
      "-predict time:0.036s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:61.526s\n",
      "Mean abs error: 1203.76\n",
      "-predict time:3.564s\n",
      "XGB Mean abs error: 1138.36\n",
      "-XGB predict time:2.295s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 4)\n",
      "---Fold run time:391.571s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:68.714s\n",
      "Mean abs error: 1210.12\n",
      "-predict time:4.06s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.552s\n",
      "Mean abs error: 1273.58\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:61.537s\n",
      "Mean abs error: 1204.21\n",
      "-predict time:3.6s\n",
      "XGB Mean abs error: 1133.70\n",
      "-XGB predict time:2.303s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 4)\n",
      "---Fold run time:391.481s\n",
      "---Fold:75326 to 112989 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:58: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:69.095s\n",
      "Mean abs error: 1225.63\n",
      "-predict time:4.051s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.586s\n",
      "Mean abs error: 1292.36\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:62.691s\n",
      "Mean abs error: 1215.81\n",
      "-predict time:3.756s\n",
      "XGB Mean abs error: 1149.70\n",
      "-XGB predict time:2.467s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 4)\n",
      "---Fold run time:397.101s\n",
      "---Fold:112989 to 150652 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:70.919s\n",
      "Mean abs error: 1222.34\n",
      "-predict time:4.218s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.601s\n",
      "Mean abs error: 1296.75\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:62.526s\n",
      "Mean abs error: 1212.29\n",
      "-predict time:3.656s\n",
      "XGB Mean abs error: 1142.27\n",
      "-XGB predict time:2.425s\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 4)\n",
      "---Fold run time:398.756s\n",
      "---Fold:150652 to 188318 of: 188318\n",
      "\n",
      "---folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:69.727s\n",
      "Mean abs error: 1203.62\n",
      "-predict time:4.19s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.585s\n",
      "Mean abs error: 1270.93\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:61.93s\n",
      "Mean abs error: 1195.09\n",
      "-predict time:3.656s\n",
      "XGB Mean abs error: 1128.54\n",
      "-XGB predict time:2.436s\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 4)\n",
      "---Fold run time:394.457s\n",
      "----Full run time:1973.367s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile(cachedir+'x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load(cachedir+'x_layer2.npy') \n",
    "    MAE_tracking=joblib.load(cachedir+'MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            #gbdt=xgbfit(X_train,y_train)\n",
    "            gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,cachedir+'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,cachedir+'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1179.07\n",
      "length of new row: 5\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "#MAE=np.mean(abs(avg_column - y))\n",
    "MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.72564088,  7.33350105,  7.77098668,  7.69497013,  7.63127468],\n",
       "       [ 7.69969656,  7.65916132,  7.64628168,  7.56129837,  7.64160948],\n",
       "       [ 8.35786941,  8.50399124,  8.35953566,  8.39824104,  8.40490934]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:44.254s\n",
      "length of row: 5\n",
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,cachedir+'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load(cachedir+'x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c99992ff9e10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mparamater_grid_Lin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlayer2_Lin_regr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid_search_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_layer2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparamater_grid_Lin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregr_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L2_Lin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.22685+0.0019794\ttest-mae:7.22685+0.00597367\n",
      "[100]\ttrain-mae:2.64566+0.00071125\ttest-mae:2.64563+0.00460733\n",
      "[200]\ttrain-mae:0.979774+0.000350709\ttest-mae:0.979845+0.00400455\n",
      "[300]\ttrain-mae:0.478991+0.000504777\ttest-mae:0.480557+0.00251428\n",
      "[400]\ttrain-mae:0.387274+0.000477961\ttest-mae:0.390574+0.00116737\n",
      "[500]\ttrain-mae:0.373197+0.000445189\ttest-mae:0.377459+0.000634498\n",
      "[600]\ttrain-mae:0.370516+0.000390568\ttest-mae:0.375463+0.000484951\n",
      "[700]\ttrain-mae:0.369502+0.000349364\ttest-mae:0.375044+0.000446149\n",
      "[800]\ttrain-mae:0.368708+0.000327205\ttest-mae:0.374836+0.000428095\n",
      "[900]\ttrain-mae:0.36803+0.000348059\ttest-mae:0.374731+0.000450785\n",
      "[1000]\ttrain-mae:0.367395+0.000344808\ttest-mae:0.374663+0.000430406\n",
      "[1100]\ttrain-mae:0.366871+0.000346567\ttest-mae:0.37464+0.000429563\n",
      "[1200]\ttrain-mae:0.366356+0.000297069\ttest-mae:0.374615+0.000424738\n",
      "[1300]\ttrain-mae:0.365885+0.000301724\ttest-mae:0.374606+0.000421304\n",
      "CV time:242.812s\n",
      "CV-Mean: 0.37459925+0.000427096227448\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(x_layer2, label=y)\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1136.55\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1156.20\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1153.00\n",
      "XGB predict time:1.418s\n",
      "AVG Mean abs error: 1142.57\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1131.51\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1152.98\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1149.10\n",
      "XGB predict time:1.204s\n",
      "AVG Mean abs error: 1138.45\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1148.08\n",
      "Score: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:67: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1166.89\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1161.46\n",
      "XGB predict time:1.398s\n",
      "AVG Mean abs error: 1152.21\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1140.31\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1162.15\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1157.34\n",
      "XGB predict time:1.23s\n",
      "AVG Mean abs error: 1147.55\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1126.02\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1148.55\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1141.99\n",
      "XGB predict time:1.405s\n",
      "AVG Mean abs error: 1132.62\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_linear) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_KNeighbors) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('KNNLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_train, label=y_layer2_train)\n",
    "    dtest = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    \n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "    #MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_gbdt_predict) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    #MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_avg_predict) - np.exp(y_layer2_validation)))\n",
    "\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "XGB Mean abs error: 1140.60\n",
      "XGB predict time:1.46s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_train, label=y_layer3_train)\n",
    "dtest = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtest)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE=np.mean(abs(np.exp(layer3_gbdt_predict) - np.exp(y_layer3_validation)))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer3'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:XGB'\n",
      "  'run:37663-75326:0' 'run:37663-75326:1' 'run:37663-75326:2'\n",
      "  'run:37663-75326:XGB' 'run:75326-112989:0' 'run:75326-112989:1'\n",
      "  'run:75326-112989:2' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:XGB'\n",
      "  'run:150652-188318:0' 'run:150652-188318:1' 'run:150652-188318:2'\n",
      "  'run:150652-188318:XGB' 'run:linearLayer2' 'run:KNNLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:KNNLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:KNNLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:KNNLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:KNNLayer2'\n",
      "  'run:XGBLayer2' 'run:XGBLayer3']\n",
      " ['1212.14825319' '1278.45853919' '1203.76338741' '1138.35538876'\n",
      "  '1210.12277464' '1273.57948617' '1204.21009999' '1133.69840899'\n",
      "  '1225.63498722' '1292.35723022' '1215.81104253' '1149.70203441'\n",
      "  '1222.33895835' '1296.75083708' '1212.2913242' '1142.2722771'\n",
      "  '1203.62440183' '1270.93079987' '1195.08823713' '1128.54106508'\n",
      "  '1136.54788443' '1156.19983098' '1153.00422859' '1131.51250956'\n",
      "  '1152.97643182' '1149.09664616' '1148.07980999' '1166.88721004'\n",
      "  '1161.45995058' '1140.30559238' '1162.15156735' '1157.34009249'\n",
      "  '1126.02359051' '1148.55048933' '1141.9923442' '1140.5973845']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGDCAYAAAAWKgYNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqlWrvuj/A7vjbeoZ6tKVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efeebb7fa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:106.795s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.811s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:94.357s\n",
      "XGB predict time:299.856s\n",
      "AVG column added - length of new row: 6\n",
      "Fold run time:411.477s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:36.228s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([14, 30, 60, 27, 47, 22, 39, 43, 75,  5, 64, 43, 26, 67, 67], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.53882837,  7.36421311,  7.56065475,  7.46098852,  7.48117119],\n",
       "       [ 7.75194175,  7.60895517,  7.67783033,  7.67762709,  7.67908859],\n",
       "       [ 9.05288467,  9.43546791,  8.97044867,  9.06942368,  9.13205623]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  7.53882837,   7.36421311,   7.56065475,   7.46098852,\n",
       "          7.48117119,  14.        ],\n",
       "       [  7.75194175,   7.60895517,   7.67783033,   7.67762709,\n",
       "          7.67908859,  30.        ],\n",
       "       [  9.05288467,   9.43546791,   8.97044867,   9.06942368,\n",
       "          9.13205623,  60.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n",
      "run time:36.24s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.003s\n",
      "KNeighbors predict time:4.849s\n",
      "XGB predict time:4.724s\n",
      "AVG predict time:0.001s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1542.84979867\\n',\n",
       " '6,1995.8449252\\n',\n",
       " '9,8495.7542086\\n',\n",
       " '12,6139.89763266\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spit out that good scoring linear result...\n",
    "test_data['loss']=np.exp(layer3_predict_linear)-200\n",
    "\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_linear.csv\"\n",
    "display(writeData(result,output_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1792.22949219\\n',\n",
       " '6,1998.32128906\\n',\n",
       " '9,9441.10449219\\n',\n",
       " '12,6233.34521484\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=np.exp(layer3_gbdt.predict(dtest))-200\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1686.750977\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
