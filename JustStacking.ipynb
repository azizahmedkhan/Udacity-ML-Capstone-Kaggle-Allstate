{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####unused!, but for a reference idea\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "        \n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    combineddata[feat] = combineddata[comb[0]] + combineddata[comb[1]]\n",
    "    #combineddata[feat] = combineddata[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "combineddata[\"cont1\"] = np.sqrt(combineddata[\"cont1\"])\n",
    "combineddata[\"cont4\"] = np.sqrt(combineddata[\"cont4\"])\n",
    "combineddata[\"cont5\"] = np.sqrt(combineddata[\"cont5\"])\n",
    "combineddata[\"cont8\"] = np.sqrt(combineddata[\"cont8\"])\n",
    "combineddata[\"cont10\"] = np.sqrt(combineddata[\"cont10\"])\n",
    "combineddata[\"cont11\"] = np.sqrt(combineddata[\"cont11\"])\n",
    "combineddata[\"cont12\"] = np.sqrt(combineddata[\"cont12\"])\n",
    "\n",
    "combineddata[\"cont6\"] = np.log(combineddata[\"cont6\"] + 0000.1)\n",
    "combineddata[\"cont7\"] = np.log(combineddata[\"cont7\"] + 0000.1)\n",
    "combineddata[\"cont9\"] = np.log(combineddata[\"cont9\"] + 0000.1)\n",
    "combineddata[\"cont13\"] = np.log(combineddata[\"cont13\"] + 0000.1)\n",
    "combineddata[\"cont14\"] = (np.maximum(combineddata[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 133 entries, cat1 to id\n",
      "dtypes: float64(132), int64(1)\n",
      "memory usage: 191.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>clusters</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>-0.255633</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>-0.792214</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>-1.016372</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10 ...     cont8  \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0 ...  0.296826   \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0 ...  0.698552   \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0 ...  0.220662   \n",
       "\n",
       "      cont9    cont10    cont11    cont12    cont13    cont14  clusters  \\\n",
       "0 -0.255633  0.916140  0.744792  0.761787 -0.070392  0.984628  0.341772   \n",
       "1 -0.792214  0.664384  0.560798  0.585682 -0.330645  0.343682  0.164557   \n",
       "2 -1.016372  0.571049  0.599347  0.591963 -1.211326  1.018094  0.000000   \n",
       "\n",
       "      loss  id  \n",
       "0  2213.18   1  \n",
       "1  1283.60   2  \n",
       "2  3005.09   5  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "y=data['loss'].values\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "display(data.info())\n",
    "display(data.head(3))\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.004s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:2973.21+6.6702\ttest-mae:2973.19+20.3854\n",
      "[100]\ttrain-mae:1381.55+5.72146\ttest-mae:1438.97+19.3135\n",
      "[200]\ttrain-mae:1089.28+9.5155\ttest-mae:1242.44+13.0097\n",
      "[300]\ttrain-mae:997.453+12.0014\ttest-mae:1220.87+10.3515\n",
      "[400]\ttrain-mae:942.865+14.256\ttest-mae:1215.23+9.68692\n",
      "[500]\ttrain-mae:902.93+17.8892\ttest-mae:1211.54+9.5774\n",
      "[600]\ttrain-mae:871.755+22.5303\ttest-mae:1208.26+9.84651\n",
      "[700]\ttrain-mae:846.302+26.5719\ttest-mae:1205.44+10.2789\n",
      "[800]\ttrain-mae:825.859+29.2546\ttest-mae:1203.33+10.3927\n",
      "[900]\ttrain-mae:808.848+31.4377\ttest-mae:1202.29+10.9249\n",
      "[1000]\ttrain-mae:794.599+32.3203\ttest-mae:1201.58+11.2083\n",
      "[1100]\ttrain-mae:782.145+33.4889\ttest-mae:1201.37+11.5221\n",
      "[1200]\ttrain-mae:770.373+34.308\ttest-mae:1201.04+11.6488\n",
      "[1300]\ttrain-mae:758.829+34.6796\ttest-mae:1201+11.8179\n",
      "CV time:119.053s\n",
      "CV-Mean: 1200.9609375+11.800158931\n"
     ]
    }
   ],
   "source": [
    "# XGB!\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#my first tries:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:85.495s\n",
      "Mean abs error: 1242.84\n",
      "-predict time:4.383s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.92s\n",
      "Mean abs error: 1337.12\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:75.618s\n",
      "Mean abs error: 1235.50\n",
      "-predict time:3.977s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:10.053s\n",
      "Mean abs error: 1311.75\n",
      "-predict time:439.627s\n",
      "XGB Mean abs error: 1179.12\n",
      "-XGB predict time:1.722s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "---Fold run time:788.558s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            #gbdt=xgbfit(X_train,y_train)\n",
    "            gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1208.66\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "MAE=np.mean(abs(avg_column - y))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2208.59926   ,   881.87145066,  2295.51764   ,  1986.33866667,\n",
       "         2092.04638672,  1892.87468081],\n",
       "       [ 2116.38352   ,  2540.24239237,  1957.11594   ,  2854.52066667,\n",
       "         1946.93688965,  2283.03988174],\n",
       "       [ 4583.53224   ,  5184.9735887 ,  4381.52288   ,  4810.44866667,\n",
       "         4618.65478516,  4715.8264321 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:49.148s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.012s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1174.83\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1194.36\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3007.76+4.51422\ttest-mae:3007.78+13.5952\n",
      "[100]\ttrain-mae:1352.92+1.99166\ttest-mae:1373.31+9.52442\n",
      "[200]\ttrain-mae:1111.16+1.75978\ttest-mae:1163.6+7.61905\n",
      "CV time:65.289s\n",
      "CV-Mean: 1157.46658325+7.88236244559\n",
      "Fit time:17.184s\n",
      "XGB Mean abs error: 1159.43\n",
      "XGB predict time:0.341s\n",
      "AVG Mean abs error: 1160.14\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1159.52\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1179.54\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3006.37+4.72112\ttest-mae:3006.38+14.1417\n",
      "[100]\ttrain-mae:1352.58+1.80018\ttest-mae:1372.97+7.4527\n",
      "[200]\ttrain-mae:1111.91+2.11187\ttest-mae:1164.57+8.15137\n",
      "CV time:66.38s\n",
      "CV-Mean: 1158.6372375+8.51779539459\n",
      "Fit time:16.862s\n",
      "XGB Mean abs error: 1151.27\n",
      "XGB predict time:0.316s\n",
      "AVG Mean abs error: 1148.12\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1174.78\n",
      "Score: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:60: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1192.39\n",
      "Score: 0.58\n",
      "[0]\ttrain-mae:3003.98+2.04905\ttest-mae:3003.99+6.1356\n",
      "[100]\ttrain-mae:1350.22+1.13413\ttest-mae:1370.51+3.62661\n",
      "[200]\ttrain-mae:1108.72+1.51885\ttest-mae:1161+6.57476\n",
      "CV time:66.647s\n",
      "CV-Mean: 1154.72158825+7.13364088913\n",
      "Fit time:17.299s\n",
      "XGB Mean abs error: 1165.86\n",
      "XGB predict time:0.341s\n",
      "AVG Mean abs error: 1163.33\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1175.44\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1198.83\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3005.41+3.61202\ttest-mae:3005.41+10.8841\n",
      "[100]\ttrain-mae:1350.93+1.59773\ttest-mae:1371.1+4.60499\n",
      "[200]\ttrain-mae:1109.73+1.08397\ttest-mae:1162.18+4.45708\n",
      "CV time:66.347s\n",
      "CV-Mean: 1155.8359985+4.32642491145\n",
      "Fit time:16.988s\n",
      "XGB Mean abs error: 1164.14\n",
      "XGB predict time:0.32s\n",
      "AVG Mean abs error: 1164.19\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1159.38\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1183.27\n",
      "Score: 0.58\n",
      "[0]\ttrain-mae:3009.32+5.20248\ttest-mae:3009.31+15.6944\n",
      "[100]\ttrain-mae:1354.17+2.28093\ttest-mae:1374.54+9.1126\n",
      "[200]\ttrain-mae:1112.39+1.74354\ttest-mae:1165.65+7.72604\n",
      "CV time:66.224s\n",
      "CV-Mean: 1159.4015505+7.21323146426\n",
      "Fit time:17.001s\n",
      "XGB Mean abs error: 1148.29\n",
      "XGB predict time:0.34s\n",
      "AVG Mean abs error: 1148.26\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    #layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)    \n",
    "    dtrain = xgb.DMatrix(X_layer2_train, label=y_layer2_train)\n",
    "    dtest = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    \n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "    MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:3007.73+4.12893\ttest-mae:3007.73+12.4475\n",
      "[100]\ttrain-mae:1357.23+2.28006\ttest-mae:1370.31+8.76322\n",
      "[200]\ttrain-mae:1127.33+1.53143\ttest-mae:1161.01+5.36895\n",
      "CV time:53.072s\n",
      "CV-Mean: 1155.73242175+4.52968533995\n",
      "Fit time:13.062s\n",
      "XGB Mean abs error: 1153.05\n",
      "XGB predict time:0.373s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "#layer3_gbdt=xgbfit(X_layer3_train,y_layer3_train)\n",
    "dtrain = xgb.DMatrix(X_layer3_train, label=y_layer3_train)\n",
    "dtest = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt==xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtest)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2']\n",
      " ['1240.85767662' '1337.11944058' '1235.50402739' '1311.74796396'\n",
      "  '1176.06295544' '1230.94077935' '1326.11306781' '1224.23808242'\n",
      "  '1307.51585151' '1162.6351499' '1244.92223571' '1334.22559827'\n",
      "  '1236.16122191' '1319.77523175' '1177.98589887' '1245.75502416'\n",
      "  '1340.40499486' '1240.02598087' '1320.61588521' '1178.24219284'\n",
      "  '1229.97259867' '1328.73756119' '1224.96004848' '1302.20116698'\n",
      "  '1162.71862596' '1174.82977883' '1194.36365104' '1159.43395074'\n",
      "  '1159.518946' '1179.53696314' '1151.26871269' '1174.78330534'\n",
      "  '1192.39122789' '1165.86422634' '1175.43602015' '1198.83176118'\n",
      "  '1164.142635' '1159.38115899' '1183.27179816' '1148.29140311'\n",
      "  '1153.04791254']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGPCAYAAABCs5ejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWa3KD4qsrKysrFb9f9Mz+1+tnWdbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6844984450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:126.205s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.959s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:112.978s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1726.175s\n",
      "[0]\ttrain-mae:3006.68+7.8705\ttest-mae:3006.69+23.852\n",
      "[100]\ttrain-mae:1418.42+4.11557\ttest-mae:1440.12+16.1942\n",
      "[200]\ttrain-mae:1174.48+2.83585\ttest-mae:1234.41+9.84181\n",
      "[300]\ttrain-mae:1119.63+1.99832\ttest-mae:1209.06+7.07578\n",
      "[400]\ttrain-mae:1088.66+1.25263\ttest-mae:1200.17+6.19475\n",
      "[500]\ttrain-mae:1064.4+1.50934\ttest-mae:1193.81+5.39336\n",
      "[600]\ttrain-mae:1043.96+1.60221\ttest-mae:1188.91+5.0985\n",
      "[700]\ttrain-mae:1026.78+1.5858\ttest-mae:1185.06+4.87686\n",
      "[800]\ttrain-mae:1012+1.58063\ttest-mae:1182.25+4.81315\n",
      "[900]\ttrain-mae:998.889+1.99535\ttest-mae:1180.17+4.77151\n",
      "[1000]\ttrain-mae:987.167+2.16752\ttest-mae:1178.56+4.60987\n",
      "[1100]\ttrain-mae:976.855+1.95445\ttest-mae:1177.35+4.68435\n",
      "[1200]\ttrain-mae:967.074+1.71727\ttest-mae:1176.39+4.63705\n",
      "[1300]\ttrain-mae:957.66+1.84078\ttest-mae:1175.63+4.65556\n",
      "[1400]\ttrain-mae:948.972+1.83495\ttest-mae:1175.05+4.65725\n",
      "[1500]\ttrain-mae:940.514+1.98793\ttest-mae:1174.63+4.67285\n",
      "[1600]\ttrain-mae:932.349+2.10967\ttest-mae:1174.27+4.64015\n",
      "[1700]\ttrain-mae:924.326+2.58398\ttest-mae:1173.91+4.68864\n",
      "[1800]\ttrain-mae:916.609+2.67773\ttest-mae:1173.7+4.64423\n",
      "[1900]\ttrain-mae:909.115+2.52783\ttest-mae:1173.44+4.59372\n",
      "[2000]\ttrain-mae:901.833+2.55314\ttest-mae:1173.27+4.6075\n",
      "CV time:947.503s\n",
      "CV-Mean: 1173.2702025+4.60750409511\n",
      "Fit time:309.193s\n",
      "XGB predict time:2992.134s\n",
      "AVG column added - length of new row: 7\n",
      "Fold run time:3236.286s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:44.417s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0, 54,  2, 46, 55, 53, 23, 49, 41, 12, 12, 50,  7, 73, 73], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  2018.97904   ,   1200.12553545,   2089.18938   ,   1900.38466667,\n",
       "          1624.26000977,   1766.58772638],\n",
       "       [  2478.99378   ,   2060.49351746,   2302.10914   ,   2689.62133333,\n",
       "          2092.49169922,   2324.741894  ],\n",
       "       [  9161.69122   ,  11351.43724916,   8880.99814   ,   8392.17666667,\n",
       "          9915.11035156,   9540.28272548]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.01897904e+03,   1.20012554e+03,   2.08918938e+03,\n",
       "          1.90038467e+03,   1.62426001e+03,   1.76658773e+03,\n",
       "          0.00000000e+00],\n",
       "       [  2.47899378e+03,   2.06049352e+03,   2.30210914e+03,\n",
       "          2.68962133e+03,   2.09249170e+03,   2.32474189e+03,\n",
       "          5.40000000e+01],\n",
       "       [  9.16169122e+03,   1.13514372e+04,   8.88099814e+03,\n",
       "          8.39217667e+03,   9.91511035e+03,   9.54028273e+03,\n",
       "          2.00000000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 7\n",
      "run time:44.431s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.003s\n",
      "KNeighbors predict time:4.84s\n",
      "XGB predict time:1.143s\n",
      "AVG predict time:0.001s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1489.15039062\\n',\n",
       " '6,2001.5604248\\n',\n",
       " '9,9392.69335938\\n',\n",
       " '12,6854.11523438\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=layer3_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1970.535522\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
