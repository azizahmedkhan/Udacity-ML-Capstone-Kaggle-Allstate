{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "#my first tries:\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "    #params from:\n",
    "    #https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.3085,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 10,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 4.2922,\n",
    "        'eval_metric': 'mae',\n",
    "        'eta':0.001,\n",
    "        'gamma': 0.5290,\n",
    "        'subsample':0.9930,\n",
    "        'max_delta_step':0,\n",
    "        'booster':'gbtree',\n",
    "        'nrounds': 1001\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "y=data['loss'].values\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.035s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:83.225s\n",
      "Mean abs error: 1238.94\n",
      "-predict time:4.463s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.15s\n",
      "Mean abs error: 1334.23\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:77.898s\n",
      "Mean abs error: 1234.17\n",
      "-predict time:4.16s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:80.618s\n",
      "Mean abs error: 1308.32\n",
      "-predict time:401.273s\n",
      "[0]\ttrain-mae:3007.84+4.5137\ttest-mae:3007.83+13.6187\n",
      "[100]\ttrain-mae:1416.64+1.00995\ttest-mae:1441.4+10.7442\n",
      "[200]\ttrain-mae:1168.76+1.88186\ttest-mae:1236.13+8.1965\n",
      "[300]\ttrain-mae:1108.07+2.5823\ttest-mae:1208.99+8.56748\n",
      "[400]\ttrain-mae:1073.93+1.56439\ttest-mae:1200.19+8.50815\n",
      "[500]\ttrain-mae:1046.39+2.45533\ttest-mae:1193.44+8.74533\n",
      "[600]\ttrain-mae:1023.73+4.26936\ttest-mae:1188.41+9.0767\n",
      "[700]\ttrain-mae:1005.17+5.63907\ttest-mae:1184.7+9.22464\n",
      "[800]\ttrain-mae:988.857+6.72983\ttest-mae:1181.78+9.30094\n",
      "[900]\ttrain-mae:974.63+7.95037\ttest-mae:1179.68+9.31224\n",
      "[1000]\ttrain-mae:960.948+8.25334\ttest-mae:1178.16+9.29646\n",
      "[1100]\ttrain-mae:948.175+8.80721\ttest-mae:1176.97+9.35932\n",
      "[1200]\ttrain-mae:936.773+9.03676\ttest-mae:1176.09+9.27215\n",
      "[1300]\ttrain-mae:926.372+8.69549\ttest-mae:1175.43+9.31761\n",
      "[1400]\ttrain-mae:916.419+8.38664\ttest-mae:1174.84+9.34737\n",
      "[1500]\ttrain-mae:907.092+8.06549\ttest-mae:1174.44+9.31538\n",
      "[1600]\ttrain-mae:898.267+7.73498\ttest-mae:1174.19+9.30316\n",
      "[1700]\ttrain-mae:889.54+7.49974\ttest-mae:1173.96+9.25651\n",
      "[1800]\ttrain-mae:880.956+7.31152\ttest-mae:1173.73+9.24389\n",
      "[1900]\ttrain-mae:873.211+6.96543\ttest-mae:1173.61+9.25174\n",
      "[2000]\ttrain-mae:865.38+6.75651\ttest-mae:1173.52+9.25564\n",
      "CV time:760.525s\n",
      "CV-Mean: 1173.51779175+9.25563911378\n",
      "Fit time:248.066s\n",
      "XGB Mean abs error: 1173.21\n",
      "-XGB predict time:2.629s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "---Fold run time:1667.759s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:85.102s\n",
      "Mean abs error: 1228.11\n",
      "-predict time:4.452s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.826s\n",
      "Mean abs error: 1321.26\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:76.514s\n",
      "Mean abs error: 1222.02\n",
      "-predict time:4.157s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:83.862s\n",
      "Mean abs error: 1304.28\n",
      "-predict time:404.187s\n",
      "[0]\ttrain-mae:3006.45+4.72444\ttest-mae:3006.45+14.1584\n",
      "[100]\ttrain-mae:1415.67+2.18001\ttest-mae:1440.81+8.39295\n",
      "[200]\ttrain-mae:1167+2.71126\ttest-mae:1236.41+9.07895\n",
      "[300]\ttrain-mae:1106.48+3.60924\ttest-mae:1210.01+9.3512\n",
      "[400]\ttrain-mae:1072.3+2.87219\ttest-mae:1201.55+9.25229\n",
      "[500]\ttrain-mae:1044.58+2.24438\ttest-mae:1194.99+9.24256\n",
      "[600]\ttrain-mae:1021.37+2.53033\ttest-mae:1190.05+9.36702\n",
      "[700]\ttrain-mae:1002.35+3.19835\ttest-mae:1186.48+9.27298\n",
      "[800]\ttrain-mae:985.801+4.09187\ttest-mae:1183.65+9.28192\n",
      "[900]\ttrain-mae:971.797+5.0447\ttest-mae:1181.66+9.30059\n",
      "[1000]\ttrain-mae:958.568+5.60028\ttest-mae:1180.17+9.35802\n",
      "[1100]\ttrain-mae:946.312+6.24415\ttest-mae:1179.03+9.36512\n",
      "[1200]\ttrain-mae:935.063+6.6949\ttest-mae:1178.09+9.35134\n",
      "[1300]\ttrain-mae:924.71+6.93514\ttest-mae:1177.41+9.38263\n",
      "[1400]\ttrain-mae:915.024+7.17297\ttest-mae:1176.92+9.44021\n",
      "[1500]\ttrain-mae:906.179+6.91272\ttest-mae:1176.61+9.49409\n",
      "[1600]\ttrain-mae:897.459+6.39127\ttest-mae:1176.29+9.49682\n",
      "[1700]\ttrain-mae:888.907+6.09137\ttest-mae:1176.02+9.45257\n",
      "[1800]\ttrain-mae:880.29+5.94647\ttest-mae:1175.81+9.4359\n",
      "[1900]\ttrain-mae:872.266+5.98652\ttest-mae:1175.62+9.50826\n",
      "[2000]\ttrain-mae:864.653+5.98785\ttest-mae:1175.53+9.45705\n",
      "CV time:761.119s\n",
      "CV-Mean: 1175.52850325+9.45704691667\n",
      "Fit time:247.068s\n",
      "XGB Mean abs error: 1160.76\n",
      "-XGB predict time:2.709s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 5)\n",
      "---Fold run time:1673.712s\n",
      "---Fold:75326 to 112989 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:86.614s\n",
      "Mean abs error: 1240.94\n",
      "-predict time:4.568s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.892s\n",
      "Mean abs error: 1329.69\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:77.28s\n",
      "Mean abs error: 1235.05\n",
      "-predict time:4.262s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:86.109s\n",
      "Mean abs error: 1315.09\n",
      "-predict time:412.221s\n",
      "[0]\ttrain-mae:3004.06+2.0507\ttest-mae:3004.04+6.06384\n",
      "[100]\ttrain-mae:1413.19+1.40522\ttest-mae:1438.08+5.0058\n",
      "[200]\ttrain-mae:1164.39+2.54993\ttest-mae:1232.76+7.70385\n",
      "[300]\ttrain-mae:1103.5+3.50267\ttest-mae:1205.68+8.43423\n",
      "[400]\ttrain-mae:1069.54+2.87013\ttest-mae:1197.21+8.5762\n",
      "[500]\ttrain-mae:1042.39+2.37492\ttest-mae:1190.72+8.59541\n",
      "[600]\ttrain-mae:1020.19+2.54137\ttest-mae:1185.75+8.57781\n",
      "[700]\ttrain-mae:1002.01+3.68704\ttest-mae:1182.07+8.36395\n",
      "[800]\ttrain-mae:986.54+4.27722\ttest-mae:1179.16+8.34394\n",
      "[900]\ttrain-mae:973.338+5.4906\ttest-mae:1177.04+8.35262\n",
      "[1000]\ttrain-mae:961.347+6.43053\ttest-mae:1175.55+8.37356\n",
      "[1100]\ttrain-mae:950.196+6.98141\ttest-mae:1174.35+8.39913\n",
      "[1200]\ttrain-mae:939.78+7.07922\ttest-mae:1173.44+8.38237\n",
      "[1300]\ttrain-mae:930.073+7.16751\ttest-mae:1172.82+8.42058\n",
      "[1400]\ttrain-mae:920.648+7.71073\ttest-mae:1172.21+8.48858\n",
      "[1500]\ttrain-mae:911.648+7.68069\ttest-mae:1171.75+8.50233\n",
      "[1600]\ttrain-mae:903.495+7.44103\ttest-mae:1171.42+8.41874\n",
      "[1700]\ttrain-mae:895.375+7.37713\ttest-mae:1171.15+8.41234\n",
      "[1800]\ttrain-mae:886.895+7.22344\ttest-mae:1170.95+8.34477\n",
      "[1900]\ttrain-mae:879.154+7.32001\ttest-mae:1170.82+8.35565\n",
      "[2000]\ttrain-mae:871.605+7.32808\ttest-mae:1170.69+8.27111\n",
      "CV time:760.627s\n",
      "CV-Mean: 1170.69210825+8.27110664126\n",
      "Fit time:249.254s\n",
      "XGB Mean abs error: 1174.40\n",
      "-XGB predict time:2.753s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 5)\n",
      "---Fold run time:1688.313s\n",
      "---Fold:112989 to 150652 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:86.916s\n",
      "Mean abs error: 1243.20\n",
      "-predict time:4.561s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.911s\n",
      "Mean abs error: 1337.51\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:75.278s\n",
      "Mean abs error: 1238.97\n",
      "-predict time:4.185s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:91.221s\n",
      "Mean abs error: 1315.60\n",
      "-predict time:411.485s\n",
      "[0]\ttrain-mae:3005.5+3.60934\ttest-mae:3005.47+10.819\n",
      "[100]\ttrain-mae:1414.67+2.02744\ttest-mae:1439.7+6.58538\n",
      "[200]\ttrain-mae:1166.62+3.75046\ttest-mae:1235.15+4.55378\n",
      "[300]\ttrain-mae:1106.22+4.32974\ttest-mae:1208.07+5.15879\n",
      "[400]\ttrain-mae:1072.46+3.84249\ttest-mae:1199.32+5.25201\n",
      "[500]\ttrain-mae:1045.43+2.56372\ttest-mae:1192.59+5.40273\n",
      "[600]\ttrain-mae:1023.46+1.63552\ttest-mae:1187.57+5.17647\n",
      "[700]\ttrain-mae:1005.39+1.91337\ttest-mae:1183.74+4.91082\n",
      "[800]\ttrain-mae:990.227+1.88035\ttest-mae:1180.87+4.74636\n",
      "[900]\ttrain-mae:977.393+2.38482\ttest-mae:1178.85+4.65677\n",
      "[1000]\ttrain-mae:965.48+2.9514\ttest-mae:1177.36+4.60025\n",
      "[1100]\ttrain-mae:954.077+4.13842\ttest-mae:1176.16+4.5861\n",
      "[1200]\ttrain-mae:943.192+5.27875\ttest-mae:1175.29+4.55775\n",
      "[1300]\ttrain-mae:933.36+5.72906\ttest-mae:1174.55+4.55\n",
      "[1400]\ttrain-mae:923.776+6.46441\ttest-mae:1173.95+4.60255\n",
      "[1500]\ttrain-mae:914.898+6.93662\ttest-mae:1173.5+4.64928\n",
      "[1600]\ttrain-mae:906.728+7.06564\ttest-mae:1173.12+4.62058\n",
      "[1700]\ttrain-mae:898.273+7.26639\ttest-mae:1172.83+4.61629\n",
      "[1800]\ttrain-mae:890.111+7.66389\ttest-mae:1172.53+4.51246\n",
      "[1900]\ttrain-mae:882.165+8.23352\ttest-mae:1172.38+4.4576\n",
      "[2000]\ttrain-mae:874.365+8.44364\ttest-mae:1172.23+4.35634\n",
      "CV time:756.292s\n",
      "CV-Mean: 1172.226196+4.35634485002\n",
      "Fit time:246.251s\n",
      "XGB Mean abs error: 1175.04\n",
      "-XGB predict time:2.698s\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 5)\n",
      "---Fold run time:1683.875s\n",
      "---Fold:150652 to 188318 of: 188318\n",
      "\n",
      "---folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:86.939s\n",
      "Mean abs error: 1228.92\n",
      "-predict time:4.666s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.831s\n",
      "Mean abs error: 1325.46\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:77.908s\n",
      "Mean abs error: 1223.20\n",
      "-predict time:4.168s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:86.859s\n",
      "Mean abs error: 1298.03\n",
      "-predict time:409.288s\n",
      "[0]\ttrain-mae:3009.4+5.20725\ttest-mae:3009.41+15.6949\n",
      "[100]\ttrain-mae:1417.54+2.86719\ttest-mae:1442.54+11.0266\n",
      "[200]\ttrain-mae:1169.86+2.49953\ttest-mae:1237.93+6.88156\n",
      "[300]\ttrain-mae:1109.89+2.74408\ttest-mae:1210.95+6.11959\n",
      "[400]\ttrain-mae:1076.85+2.01845\ttest-mae:1202.48+6.82299\n",
      "[500]\ttrain-mae:1050.1+1.94394\ttest-mae:1195.79+6.99112\n",
      "[600]\ttrain-mae:1028.18+2.55033\ttest-mae:1191.01+7.11985\n",
      "[700]\ttrain-mae:1010.02+3.91406\ttest-mae:1187.36+7.13137\n",
      "[800]\ttrain-mae:994.401+4.90924\ttest-mae:1184.64+7.21287\n",
      "[900]\ttrain-mae:981.459+5.82584\ttest-mae:1182.61+7.21784\n",
      "[1000]\ttrain-mae:969.164+6.93903\ttest-mae:1181.09+7.30196\n",
      "[1100]\ttrain-mae:957.36+7.38401\ttest-mae:1179.88+7.44915\n",
      "[1200]\ttrain-mae:946.436+7.71249\ttest-mae:1179+7.44905\n",
      "[1300]\ttrain-mae:936.42+8.25003\ttest-mae:1178.31+7.50014\n",
      "[1400]\ttrain-mae:926.875+8.53102\ttest-mae:1177.78+7.52294\n",
      "[1500]\ttrain-mae:917.943+8.79796\ttest-mae:1177.34+7.49498\n",
      "[1600]\ttrain-mae:909.438+9.18893\ttest-mae:1177.06+7.49227\n",
      "[1700]\ttrain-mae:900.713+9.7475\ttest-mae:1176.79+7.49884\n",
      "[1800]\ttrain-mae:891.738+9.98985\ttest-mae:1176.56+7.47593\n",
      "[1900]\ttrain-mae:883.4+10.4757\ttest-mae:1176.48+7.51015\n",
      "[2000]\ttrain-mae:875.39+10.6846\ttest-mae:1176.37+7.49804\n",
      "CV time:759.132s\n",
      "CV-Mean: 1176.37149025+7.49804380249\n",
      "Fit time:247.178s\n",
      "XGB Mean abs error: 1159.98\n",
      "-XGB predict time:2.939s\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 5)\n",
      "---Fold run time:1683.671s\n",
      "----Full run time:8397.33s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1205.92\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "MAE=np.mean(abs(avg_column - y))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2390.4398    ,   938.07532411,  2328.7196    ,  2117.37933333,\n",
       "         2190.38916016,  1993.00064352],\n",
       "       [ 2088.35054   ,  2445.06798587,  1905.09504   ,  2508.722     ,\n",
       "         2004.50683594,  2190.34848036],\n",
       "       [ 4712.25866   ,  5188.57631961,  4470.3441    ,  4776.422     ,\n",
       "         4754.61230469,  4780.44267686]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:48.261s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.003s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1172.31\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1190.69\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3007.76+4.51344\ttest-mae:3007.77+13.5646\n",
      "[100]\ttrain-mae:1351.25+1.85015\ttest-mae:1371.68+9.18179\n",
      "[200]\ttrain-mae:1108+1.57166\ttest-mae:1161.02+7.27251\n",
      "CV time:73.887s\n",
      "CV-Mean: 1154.61032125+7.56023278937\n",
      "Fit time:18.481s\n",
      "XGB Mean abs error: 1154.27\n",
      "XGB predict time:0.398s\n",
      "AVG Mean abs error: 1155.89\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1157.80\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1179.59\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3006.37+4.72359\ttest-mae:3006.37+14.1225\n",
      "[100]\ttrain-mae:1350.4+1.53286\ttest-mae:1370.94+7.2004\n",
      "[200]\ttrain-mae:1108.14+1.72795\ttest-mae:1161.35+7.88266\n",
      "CV time:75.058s\n",
      "CV-Mean: 1155.19018525+8.47682348791\n",
      "Fit time:18.77s\n",
      "XGB Mean abs error: 1147.91\n",
      "XGB predict time:0.348s\n",
      "AVG Mean abs error: 1146.12\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1171.07\n",
      "Score: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:60: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1192.26\n",
      "Score: 0.58\n",
      "[0]\ttrain-mae:3003.98+2.053\ttest-mae:3003.98+6.15165\n",
      "[100]\ttrain-mae:1348.48+1.17642\ttest-mae:1368.79+2.95421\n",
      "[200]\ttrain-mae:1105.8+1.65568\ttest-mae:1158.23+6.36469\n",
      "CV time:71.71s\n",
      "CV-Mean: 1151.900055+6.86833231944\n",
      "Fit time:18.989s\n",
      "XGB Mean abs error: 1161.88\n",
      "XGB predict time:0.4s\n",
      "AVG Mean abs error: 1159.97\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1172.38\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1196.87\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3005.41+3.61298\ttest-mae:3005.42+10.878\n",
      "[100]\ttrain-mae:1349.12+1.66441\ttest-mae:1369.52+4.68306\n",
      "[200]\ttrain-mae:1106.34+1.16548\ttest-mae:1159.61+4.4041\n",
      "CV time:72.783s\n",
      "CV-Mean: 1153.04083275+4.38152699082\n",
      "Fit time:19.338s\n",
      "XGB Mean abs error: 1159.97\n",
      "XGB predict time:0.352s\n",
      "AVG Mean abs error: 1161.18\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1156.96\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1179.18\n",
      "Score: 0.58\n",
      "[0]\ttrain-mae:3009.3+5.20547\ttest-mae:3009.31+15.6939\n",
      "[100]\ttrain-mae:1352.32+2.14842\ttest-mae:1373.04+8.40913\n",
      "[200]\ttrain-mae:1108.8+1.95441\ttest-mae:1162.81+7.77699\n",
      "CV time:71.08s\n",
      "CV-Mean: 1156.5639345+7.40046912879\n",
      "Fit time:18.239s\n",
      "XGB Mean abs error: 1145.73\n",
      "XGB predict time:0.404s\n",
      "AVG Mean abs error: 1145.48\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtrain)\n",
    "    MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:3007.73+4.12895\ttest-mae:3007.74+12.4436\n",
      "[100]\ttrain-mae:1355.94+2.15784\ttest-mae:1369.22+9.39951\n",
      "[200]\ttrain-mae:1125.11+1.15301\ttest-mae:1159.07+5.74416\n",
      "CV time:56.055s\n",
      "CV-Mean: 1153.460785+4.81810996264\n",
      "Fit time:14.595s\n",
      "XGB Mean abs error: 1149.83\n",
      "XGB predict time:0.403s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgbfit(X_layer3_train,y_layer3_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtrain)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2']\n",
      " ['1238.94220096' '1334.22892128' '1234.16539149' '1308.32171075'\n",
      "  '1173.20922534' '1228.11308451' '1321.26434323' '1222.01540034'\n",
      "  '1304.27859455' '1160.76331449' '1240.94362976' '1329.6941864'\n",
      "  '1235.04853526' '1315.08723108' '1174.39504726' '1243.19789205'\n",
      "  '1337.50871287' '1238.9658797' '1315.59540886' '1175.03864223'\n",
      "  '1228.92235947' '1325.45831784' '1223.20486793' '1298.02863498'\n",
      "  '1159.97829474' '1172.30921691' '1190.68874843' '1154.27401625'\n",
      "  '1157.80124313' '1179.58874738' '1147.90753912' '1171.06506206'\n",
      "  '1192.2593048' '1161.87612976' '1172.38339965' '1196.86896632'\n",
      "  '1159.97220517' '1156.9585749' '1179.18086833' '1145.73416113'\n",
      "  '1149.82882109']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGPCAYAAABCs5ejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nymrV/wctmM7psl2XggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcd3d78bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:146.53s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:5.148s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:128.395s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1727.8s\n",
      "[0]\ttrain-mae:3006.68+7.8705\ttest-mae:3006.69+23.8521\n",
      "[100]\ttrain-mae:1417.12+4.09597\ttest-mae:1438.86+16.2937\n",
      "[200]\ttrain-mae:1171.99+2.82186\ttest-mae:1232.14+9.9311\n",
      "[300]\ttrain-mae:1116.84+2.03574\ttest-mae:1206.65+7.09138\n",
      "[400]\ttrain-mae:1085.57+1.44876\ttest-mae:1197.62+6.16002\n",
      "[500]\ttrain-mae:1060.89+1.75805\ttest-mae:1191.07+5.32494\n",
      "[600]\ttrain-mae:1040.24+1.74026\ttest-mae:1185.97+5.0822\n",
      "[700]\ttrain-mae:1022.81+1.81769\ttest-mae:1182.1+4.82835\n",
      "[800]\ttrain-mae:1007.88+1.9111\ttest-mae:1179.26+4.76518\n",
      "[900]\ttrain-mae:994.947+2.09521\ttest-mae:1177.14+4.77141\n",
      "[1000]\ttrain-mae:983.142+2.42416\ttest-mae:1175.58+4.65195\n",
      "[1100]\ttrain-mae:972.153+2.31604\ttest-mae:1174.3+4.58409\n",
      "[1200]\ttrain-mae:962.536+2.27491\ttest-mae:1173.31+4.52373\n",
      "[1300]\ttrain-mae:953.254+2.61875\ttest-mae:1172.58+4.54377\n",
      "[1400]\ttrain-mae:944.793+2.52934\ttest-mae:1171.98+4.5085\n",
      "[1500]\ttrain-mae:936.356+2.5227\ttest-mae:1171.58+4.5094\n",
      "[1600]\ttrain-mae:928.145+2.90471\ttest-mae:1171.2+4.42602\n",
      "[1700]\ttrain-mae:920.389+3.25067\ttest-mae:1170.91+4.43371\n",
      "[1800]\ttrain-mae:913.002+3.15695\ttest-mae:1170.68+4.43287\n",
      "[1900]\ttrain-mae:905.525+2.85596\ttest-mae:1170.44+4.41657\n",
      "[2000]\ttrain-mae:898.71+2.88823\ttest-mae:1170.27+4.38657\n",
      "CV time:970.8s\n",
      "CV-Mean: 1170.26858525+4.38657387694\n",
      "Fit time:314.893s\n",
      "XGB predict time:3024.364s\n",
      "AVG column added - length of new row: 7\n",
      "Fold run time:3304.448s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:51.158s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([39, 74,  2, 27, 68, 58, 47,  4, 14, 14, 50, 26, 24, 13, 42], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  1976.01754   ,   1090.86633405,   2166.7021    ,   1853.79933333,\n",
       "          1642.57336426,   1745.99173433],\n",
       "       [  2452.88312   ,   1955.48186205,   2415.1609    ,   2971.15266667,\n",
       "          2047.38024902,   2368.41175955],\n",
       "       [  9394.0663    ,  11267.87839405,   8773.54792   ,   8961.596     ,\n",
       "          9804.50097656,   9640.31791812]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.97601754e+03,   1.09086633e+03,   2.16670210e+03,\n",
       "          1.85379933e+03,   1.64257336e+03,   1.74599173e+03,\n",
       "          3.90000000e+01],\n",
       "       [  2.45288312e+03,   1.95548186e+03,   2.41516090e+03,\n",
       "          2.97115267e+03,   2.04738025e+03,   2.36841176e+03,\n",
       "          7.40000000e+01],\n",
       "       [  9.39406630e+03,   1.12678784e+04,   8.77354792e+03,\n",
       "          8.96159600e+03,   9.80450098e+03,   9.64031792e+03,\n",
       "          2.00000000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 7\n",
      "run time:51.172s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.004s\n",
      "KNeighbors predict time:4.816s\n",
      "XGB predict time:1.337s\n",
      "AVG predict time:0.001s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1555.85241699\\n',\n",
       " '6,2028.85498047\\n',\n",
       " '9,10572.3496094\\n',\n",
       " '12,6288.25244141\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=layer3_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1982.744385\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
