{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####unused!, but for a reference idea\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "        \n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    combineddata[feat] = combineddata[comb[0]] + combineddata[comb[1]]\n",
    "    #combineddata[feat] = combineddata[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "combineddata[\"cont1\"] = np.sqrt(combineddata[\"cont1\"])\n",
    "combineddata[\"cont4\"] = np.sqrt(combineddata[\"cont4\"])\n",
    "combineddata[\"cont5\"] = np.sqrt(combineddata[\"cont5\"])\n",
    "combineddata[\"cont8\"] = np.sqrt(combineddata[\"cont8\"])\n",
    "combineddata[\"cont10\"] = np.sqrt(combineddata[\"cont10\"])\n",
    "combineddata[\"cont11\"] = np.sqrt(combineddata[\"cont11\"])\n",
    "combineddata[\"cont12\"] = np.sqrt(combineddata[\"cont12\"])\n",
    "\n",
    "combineddata[\"cont6\"] = np.log(combineddata[\"cont6\"] + 0000.1)\n",
    "combineddata[\"cont7\"] = np.log(combineddata[\"cont7\"] + 0000.1)\n",
    "combineddata[\"cont9\"] = np.log(combineddata[\"cont9\"] + 0000.1)\n",
    "combineddata[\"cont13\"] = np.log(combineddata[\"cont13\"] + 0000.1)\n",
    "combineddata[\"cont14\"] = (np.maximum(combineddata[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 133 entries, cat1 to id\n",
      "dtypes: float64(132), int64(1)\n",
      "memory usage: 191.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>clusters</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>-0.255633</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>-0.792214</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>-1.016372</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10 ...     cont8  \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0 ...  0.296826   \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0 ...  0.698552   \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0 ...  0.220662   \n",
       "\n",
       "      cont9    cont10    cont11    cont12    cont13    cont14  clusters  \\\n",
       "0 -0.255633  0.916140  0.744792  0.761787 -0.070392  0.984628  0.341772   \n",
       "1 -0.792214  0.664384  0.560798  0.585682 -0.330645  0.343682  0.164557   \n",
       "2 -1.016372  0.571049  0.599347  0.591963 -1.211326  1.018094  0.000000   \n",
       "\n",
       "      loss  id  \n",
       "0  2213.18   1  \n",
       "1  1283.60   2  \n",
       "2  3005.09   5  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "#y=data['loss'].values\n",
    "y = np.log(data['loss']).ravel()\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "display(data.info())\n",
    "display(data.head(3))\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "#regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                       # dict(n_neighbors=[2,5,7,15],\n",
    "                             #leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.20,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.035s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.11322+0.00189574\ttest-mae:7.11323+0.00572075\n",
      "[100]\ttrain-mae:2.60582+0.00077029\ttest-mae:2.60588+0.00448456\n",
      "[200]\ttrain-mae:0.983877+0.000576019\ttest-mae:0.986147+0.00415985\n",
      "[300]\ttrain-mae:0.520782+0.000332036\ttest-mae:0.532829+0.00320902\n",
      "[400]\ttrain-mae:0.423562+0.000421764\ttest-mae:0.445216+0.00240648\n",
      "[500]\ttrain-mae:0.398615+0.000624741\ttest-mae:0.427596+0.00207862\n",
      "[600]\ttrain-mae:0.387494+0.000799236\ttest-mae:0.422581+0.00189632\n",
      "[700]\ttrain-mae:0.37991+0.000825092\ttest-mae:0.420351+0.00191068\n",
      "[800]\ttrain-mae:0.373903+0.000906556\ttest-mae:0.419096+0.0019114\n",
      "[900]\ttrain-mae:0.368832+0.00101925\ttest-mae:0.418284+0.00189456\n",
      "[1000]\ttrain-mae:0.36431+0.00095956\ttest-mae:0.417721+0.00189106\n",
      "[1100]\ttrain-mae:0.360257+0.000917152\ttest-mae:0.417245+0.00191358\n",
      "[1200]\ttrain-mae:0.356707+0.00102486\ttest-mae:0.416892+0.00188702\n",
      "[1300]\ttrain-mae:0.353533+0.00103758\ttest-mae:0.416634+0.00187893\n",
      "[1400]\ttrain-mae:0.350433+0.00108076\ttest-mae:0.416401+0.00185242\n",
      "[1500]\ttrain-mae:0.347586+0.00102404\ttest-mae:0.416242+0.00185482\n",
      "[1600]\ttrain-mae:0.344949+0.00112908\ttest-mae:0.416161+0.00186503\n",
      "[1700]\ttrain-mae:0.342401+0.0011619\ttest-mae:0.416093+0.00186011\n",
      "[1800]\ttrain-mae:0.339863+0.00132914\ttest-mae:0.416034+0.0018658\n",
      "[1900]\ttrain-mae:0.337459+0.00129684\ttest-mae:0.416014+0.00185729\n",
      "CV time:780.788s\n",
      "CV-Mean: 0.415987+0.00185152950287\n"
     ]
    }
   ],
   "source": [
    "# XGB!\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#my first tries:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_layer2.npy  exists, importing \n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            #gbdt=xgbfit(X_train,y_train)\n",
    "            gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1197.97\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "#MAE=np.mean(abs(avg_column - y))\n",
    "MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.60698134,  7.18058055,  7.6576309 ,  7.44979226,  7.58566904,\n",
       "         7.49613082],\n",
       "       [ 7.53860758,  7.54981222,  7.45393555,  7.6827108 ,  7.40365601,\n",
       "         7.52574443],\n",
       "       [ 8.33684191,  8.48278577,  8.33109439,  8.42085832,  8.43325806,\n",
       "         8.40096769]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:52.025s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.022s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.11344+0.00215178\ttest-mae:7.11344+0.00648849\n",
      "[100]\ttrain-mae:2.60484+0.000831239\ttest-mae:2.60486+0.00472222\n",
      "[200]\ttrain-mae:0.978697+0.000443534\ttest-mae:0.979498+0.00387321\n",
      "[300]\ttrain-mae:0.510261+0.000370824\ttest-mae:0.516078+0.00205101\n",
      "[400]\ttrain-mae:0.421635+0.00024537\ttest-mae:0.43171+0.000471585\n",
      "[500]\ttrain-mae:0.405404+0.000175685\ttest-mae:0.418289+0.000180192\n",
      "[600]\ttrain-mae:0.401015+0.000331302\ttest-mae:0.415881+0.000335793\n",
      "[700]\ttrain-mae:0.398823+0.000285784\ttest-mae:0.41528+0.000377593\n",
      "[800]\ttrain-mae:0.397153+0.000177388\ttest-mae:0.415098+0.000417126\n",
      "[900]\ttrain-mae:0.395625+0.000181981\ttest-mae:0.415027+0.000410955\n",
      "[1000]\ttrain-mae:0.394214+0.00018977\ttest-mae:0.415+0.000421275\n",
      "CV time:315.295s\n",
      "CV-Mean: 0.414995+0.000417453590235\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(x_layer2, label=y)\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1137.86\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1161.21\n",
      "Score: 0.55\n",
      "XGB Mean abs error: 1145.70\n",
      "XGB predict time:1.106s\n",
      "AVG Mean abs error: 1142.29\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1133.19\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1154.81\n",
      "Score: 0.55\n",
      "XGB Mean abs error: 1141.41\n",
      "XGB predict time:1.085s\n",
      "AVG Mean abs error: 1137.34\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1150.38\n",
      "Score: 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:67: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1168.52\n",
      "Score: 0.55\n",
      "XGB Mean abs error: 1155.57\n",
      "XGB predict time:1.123s\n",
      "AVG Mean abs error: 1152.36\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1141.46\n",
      "Score: 0.56\n",
      "KNeighborsRegressor Mean abs error: 1164.58\n",
      "Score: 0.55\n",
      "XGB Mean abs error: 1148.92\n",
      "XGB predict time:1.1s\n",
      "AVG Mean abs error: 1146.08\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1126.94\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1148.90\n",
      "Score: 0.55\n",
      "XGB Mean abs error: 1135.70\n",
      "XGB predict time:1.123s\n",
      "AVG Mean abs error: 1131.94\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_linear) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_KNeighbors) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('KNNLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_train, label=y_layer2_train)\n",
    "    dtest = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    \n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "    #MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_gbdt_predict) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    #MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_avg_predict) - np.exp(y_layer2_validation)))\n",
    "\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "XGB Mean abs error: 1140.77\n",
      "XGB predict time:1.2s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_train, label=y_layer3_train)\n",
    "dtest = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtest)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE=np.mean(abs(np.exp(layer3_gbdt_predict) - np.exp(y_layer3_validation)))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer3'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer3']\n",
      " ['1211.16359107' '1290.79229838' '1203.51703512' '1327.17835628'\n",
      "  '1139.72274556' '1210.42460928' '1287.05795129' '1205.19546554'\n",
      "  '1323.7377478' '1135.09150089' '1225.65310937' '1307.14183353'\n",
      "  '1217.3040537' '1339.84871154' '1151.86011471' '1220.49547329'\n",
      "  '1313.17576517' '1212.45011481' '1336.46831907' '1143.31702423'\n",
      "  '1202.74457996' '1282.37492497' '1197.2848137' '1320.12692288'\n",
      "  '1129.45944331' '1137.86447381' '1161.21443002' '1145.70278911'\n",
      "  '1133.1918292' '1154.80875739' '1141.4073769' '1150.37937655'\n",
      "  '1168.52141706' '1155.56652618' '1141.46231528' '1164.58296394'\n",
      "  '1148.92456782' '1126.93533806' '1148.89761545' '1135.70093984'\n",
      "  '1140.76978296']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGPCAYAAABCs5ejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee69a42e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:123.939s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:5.101s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:107.9s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1661.727s\n",
      "XGB predict time:1830.924s\n",
      "AVG column added - length of new row: 7\n",
      "Fold run time:2067.88s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:51.169s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([74, 46, 52, 62,  7, 13, 72, 68,  3, 51, 51, 35, 35, 13, 18], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.45185611,  7.22354344,  7.53799811,  7.4210465 ,  7.34254217,\n",
       "         7.39539727],\n",
       "       [ 7.70357059,  7.47171944,  7.57829824,  7.65526132,  7.56067944,\n",
       "         7.59390581],\n",
       "       [ 9.01941589,  9.43550501,  8.93359826,  8.94614793,  9.05255508,\n",
       "         9.07744443]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  7.45185611,   7.22354344,   7.53799811,   7.4210465 ,\n",
       "          7.34254217,   7.39539727,  74.        ],\n",
       "       [  7.70357059,   7.47171944,   7.57829824,   7.65526132,\n",
       "          7.56067944,   7.59390581,  46.        ],\n",
       "       [  9.01941589,   9.43550501,   8.93359826,   8.94614793,\n",
       "          9.05255508,   9.07744443,  52.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 7\n",
      "run time:51.185s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.003s\n",
      "KNeighbors predict time:5.529s\n",
      "XGB predict time:3.995s\n",
      "AVG predict time:0.003s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1546.57802764\\n',\n",
       " '6,1975.85611938\\n',\n",
       " '9,8550.07748241\\n',\n",
       " '12,6168.29801318\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spit out that good scoring linear result...\n",
    "test_data['loss']=np.exp(layer3_predict_linear)\n",
    "\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_linear.csv\"\n",
    "display(writeData(result,output_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1715.84985352\\n',\n",
       " '6,1993.97631836\\n',\n",
       " '9,6332.97119141\\n',\n",
       " '12,6663.73095703\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=np.exp(layer3_gbdt.predict(dtest))\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1730.584595\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
