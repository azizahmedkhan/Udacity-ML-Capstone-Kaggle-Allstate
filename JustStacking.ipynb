{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=False #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n",
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)\n",
    "\n",
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "test_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n",
    "\n",
    "\n",
    "x_test = test_data.copy()\n",
    "x_test.drop(['id'],1,inplace=True)\n",
    "\n",
    "# we don't want the ID columns in X, and of course not loss either\n",
    "x=data.drop(['id','loss'],1)\n",
    "# loss is our label\n",
    "y=data['loss']\n",
    "\n",
    "#minmax scaler\n",
    "scaler= MinMaxScaler() \n",
    "x = scaler.fit_transform(x)\n",
    "x_test_data = scaler.fit_transform(x_test)\n",
    "\n",
    "#display(x[:5])\n",
    "#display(y.head(5))\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1,),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl not present, running a gridsearch\n",
      "run time:1068.429s\n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl not present, running a gridsearch\n",
      "run time:29.168s\n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl not present, running a gridsearch\n",
      "run time:951.229s\n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl not present, running a gridsearch\n",
      "run time:2725.207s\n",
      "Full GridSearch run time:4774.187s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regrList[i]))\n",
    "    filename= 'grid_regr{}.pkl'.format(i)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        regrList[i]=joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regrList[i],\n",
    "                                   param_grid= paramater_grid[i],\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(X_train,y_train)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regrList[i].set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regrList[i],filename) \n",
    "        del grid_search\n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "#Full GridSearch run time:4774.187s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-787994473cfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:170.514s\n",
      "predict time:8.279s\n",
      "Mean abs error: 1239.52\n",
      "Score: 0.54\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.812s\n",
      "predict time:0.011s\n",
      "Mean abs error: 1334.19\n",
      "Score: 0.49\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:25: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:158.612s\n",
      "predict time:7.688s\n",
      "Mean abs error: 1233.05\n",
      "Score: 0.56\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:95.459s\n",
      "predict time:706.871s\n",
      "Mean abs error: 1305.79\n",
      "Score: 0.43\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 4)\n",
      "Fold run time:1883.488s\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:181.284s\n",
      "predict time:8.783s\n",
      "Mean abs error: 1228.36\n",
      "Score: 0.53\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.855s\n",
      "predict time:0.011s\n",
      "Mean abs error: 1321.26\n",
      "Score: 0.48\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:159.464s\n",
      "predict time:7.57s\n",
      "Mean abs error: 1222.97\n",
      "Score: 0.54\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:123.744s\n",
      "predict time:687.381s\n",
      "Mean abs error: 1301.48\n",
      "Score: 0.42\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 4)\n",
      "Fold run time:1877.72s\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:48: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:192.712s\n",
      "predict time:8.471s\n",
      "Mean abs error: 1241.81\n",
      "Score: 0.53\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.821s\n",
      "predict time:0.011s\n",
      "Mean abs error: 1329.70\n",
      "Score: 0.48\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:169.692s\n",
      "predict time:7.899s\n",
      "Mean abs error: 1234.14\n",
      "Score: 0.55\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:126.854s\n",
      "predict time:684.833s\n",
      "Mean abs error: 1315.59\n",
      "Score: 0.41\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 4)\n",
      "Fold run time:1896.321s\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:197.759s\n",
      "predict time:8.569s\n",
      "Mean abs error: 1241.93\n",
      "Score: 0.53\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.852s\n",
      "predict time:0.01s\n",
      "Mean abs error: 1337.47\n",
      "Score: 0.48\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:171.301s\n",
      "predict time:7.683s\n",
      "Mean abs error: 1237.68\n",
      "Score: 0.55\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:124.197s\n",
      "predict time:688.434s\n",
      "Mean abs error: 1315.02\n",
      "Score: 0.43\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 4)\n",
      "Fold run time:1913.056s\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:196.044s\n",
      "predict time:8.592s\n",
      "Mean abs error: 1227.48\n",
      "Score: 0.53\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.044s\n",
      "predict time:0.011s\n",
      "Mean abs error: 1325.46\n",
      "Score: 0.48\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:170.046s\n",
      "predict time:7.579s\n",
      "Mean abs error: 1222.14\n",
      "Score: 0.55\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:146.131s\n",
      "predict time:735.001s\n",
      "Mean abs error: 1297.19\n",
      "Score: 0.42\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 4)\n",
      "Fold run time:2019.728s\n",
      "Full run time:9590.314s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_test = x[fold_start:fold_end].copy()\n",
    "    y_test = y[fold_start:fold_end].copy()\n",
    "    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "    \n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        print(regrList[i])\n",
    "        start_time = time.time()\n",
    "        estimator=skclone(regrList[i], safe=True)\n",
    "        estimator.fit(X_train,y_train)\n",
    "        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        start_time = time.time()\n",
    "        curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "        if fold_result == []:\n",
    "            fold_result = curr_predict\n",
    "        else:\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        #show some stats on that last regressions run\n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "        print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) \n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    if use_xgb == True:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "        # now do a prediction and spit out a score(MAE) that means something\n",
    "        start_time = time.time()\n",
    "        curr_predict=gbdt.predict(dtest)\n",
    "        fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "        print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    if x_layer2 == []:\n",
    "        x_layer2=fold_result\n",
    "    else:\n",
    "        x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "        \n",
    "    print \"--layer2 length:\",len(x_layer2)\n",
    "    print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "print(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "#preserve the run\n",
    "joblib.dump(x_layer2,'x_layer2.npy') \n",
    "joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preserve the run\n",
    "x_layer2_test=joblib.load('x_layer2.npy') \n",
    "MAE_tracking_test=joblib.load('MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "Mean abs error: 1210.26\n",
      "Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "layer2_regr=LinearRegression()\n",
    "\n",
    "layer2_regr.fit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "layer2_predict=layer2_regr.predict(X_layer2_validation)\n",
    "\n",
    "#show some stats on that last regressions run    \n",
    "MAE=np.mean(abs(layer2_predict - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "\n",
    "\n",
    "#with LinearReg: Mean abs error: 1238.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.88+3.82817\ttest-mae:2810.89+12.1292\n",
      "fit time:6.589s\n",
      "CV-Mean: 1182.7045285+5.0009994342\n",
      "Train time:1.302s\n",
      "XGB Mean abs error: 1180.20\n",
      "XGB predict time:0.028s\n"
     ]
    }
   ],
   "source": [
    "# The XGB version of layer 2\n",
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "dtest = xgb.DMatrix(X_layer2_validation)\n",
    "layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "MAE=np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#with LinearReg: XGB Mean abs error: 1205.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:37663-75326:0' 'run:37663-75326:1' 'run:37663-75326:2'\n",
      "  'run:37663-75326:3' 'run:75326-112989:0' 'run:75326-112989:1'\n",
      "  'run:75326-112989:2' 'run:75326-112989:3' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:150652-188318:0' 'run:150652-188318:1' 'run:150652-188318:2'\n",
      "  'run:150652-188318:3' 'run:linearLayer2' 'run:XGBLayer2']\n",
      " ['1239.52437906' '1334.1900977' '1233.0545803' '1305.79148245'\n",
      "  '1228.36144787' '1321.25851736' '1222.97164236' '1301.48464019'\n",
      "  '1241.8060916' '1329.70355892' '1234.14033894' '1315.59238198'\n",
      "  '1241.92983848' '1337.47489181' '1237.68055689' '1315.02180823'\n",
      "  '1227.47584909' '1325.45917541' '1222.13737196' '1297.18923264'\n",
      "  '1210.2595272' '1180.19677026']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFVCAYAAAANA4MgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53d48b9490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:311.531s\n",
      "[ 1921.75426  2375.14088  8940.81018 ...,  3077.68832  1700.51962\n",
      "  4344.00586]\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.995s\n",
      "[  1142.19433558   1979.93470285  11311.72223853 ...,   2802.86052994\n",
      "   1072.83469143   4663.59137722]\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:274.347s\n",
      "[ 2195.58346  2396.87302  8799.4347  ...,  3064.59858  1467.5673\n",
      "  4057.47442]\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:2864.025s\n",
      "[ 2084.45333333  2971.15266667  8685.05066667 ...,  2277.23666667\n",
      "   870.70266667  2782.492     ]\n",
      "Fold run time:3454.908s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y)\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    print curr_predict\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'size of original test data:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "125546"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(125546, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(188318, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample of layer2 test:\\n', array([[  1921.75426   ,   1142.19433558,   2195.58346   ,   2084.45333333],\n",
      "       [  2375.14088   ,   1979.93470285,   2396.87302   ,   2971.15266667],\n",
      "       [  8940.81018   ,  11311.72223853,   8799.4347    ,   8685.05066667],\n",
      "       [  6323.90484   ,   5738.95189031,   6031.24178   ,   5568.222     ]]))\n",
      "('x_layer2_test mean:', array([ 3049.70784056,  3050.73014542,  3060.75996807,  2761.66618055]))\n",
      "('x_layer2 mean:', array([ 3044.33105976,  3037.43928403,  3061.83635783,  2761.900898  ]))\n",
      "('x_layer2_test std:', array([ 1935.88667597,  2000.04481782,  1944.34831261,  1703.53605281]))\n",
      "('x_layer2 std:', array([ 1944.22592584,  2010.55719235,  1943.15911596,  1705.01957507]))\n",
      "('num outliers:', 0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-de97bb901c5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mproblem_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_layer2_col0_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproblem_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#check outliers again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'o' is not defined"
     ]
    }
   ],
   "source": [
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,2063.51200136\\n',\n",
       " '6,2271.08325193\\n',\n",
       " '9,9506.23522094\\n',\n",
       " '12,6294.64083612\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['loss']=layer2_regr.predict(x_layer2_test)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1701.38256836\\n',\n",
       " '6,2037.29638672\\n',\n",
       " '9,8611.26855469\\n',\n",
       " '12,5642.18945312\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "test_data['loss']=layer2_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1922.785522\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
