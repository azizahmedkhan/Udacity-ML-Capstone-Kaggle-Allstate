{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.05)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.25).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "test_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n",
    "\n",
    "\n",
    "x_test = test_data.copy()\n",
    "x_test.drop(['id'],1,inplace=True)\n",
    "\n",
    "# we don't want the ID columns in X, and of course not loss either\n",
    "x=data.drop(['id','loss'],1)\n",
    "# loss is our label\n",
    "y=data['loss']\n",
    "\n",
    "#minmax scaler\n",
    "scaler= MinMaxScaler() \n",
    "x = scaler.fit_transform(x)\n",
    "x_test_data = scaler.fit_transform(x_test)\n",
    "\n",
    "#display(x[:5])\n",
    "#display(y.head(5))\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1,),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.024s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regrList[i]))\n",
    "    filename= 'grid_regr{}.pkl'.format(i)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        regrList[i]=joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regrList[i],\n",
    "                                   param_grid= paramater_grid[i],\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(X_train,y_train)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regrList[i].set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regrList[i],filename) \n",
    "        del grid_search\n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "#Full GridSearch run time:4774.187s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.963s\n",
      "Mean abs error: 1238.91\n",
      "predict time:4.172s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.905s\n",
      "Mean abs error: 1334.21\n",
      "predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:67.582s\n",
      "Mean abs error: 1233.78\n",
      "predict time:3.877s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:52.87s\n",
      "Mean abs error: 1306.24\n",
      "predict time:403.055s\n",
      "[0]\ttrain-mae:2810.38+4.10425\ttest-mae:2810.39+13.8376\n",
      "[100]\ttrain-mae:1161+2.17175\ttest-mae:1205.7+9.12972\n",
      "[200]\ttrain-mae:1120.54+2.29236\ttest-mae:1191.1+8.86479\n",
      "[300]\ttrain-mae:1094.03+3.17877\ttest-mae:1187.68+8.64846\n",
      "[400]\ttrain-mae:1070.56+2.93079\ttest-mae:1186.26+8.62755\n",
      "[500]\ttrain-mae:1049.05+2.90489\ttest-mae:1185.27+8.80342\n",
      "fit time:237.665s\n",
      "CV-Mean: 1185.259796+8.80555038302\n",
      "Train time:75.298s\n",
      "XGB Mean abs error: 1191.91\n",
      "XGB predict time:0.256s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "Fold run time:923.394s\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.385s\n",
      "Mean abs error: 1228.72\n",
      "predict time:4.279s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.866s\n",
      "Mean abs error: 1321.26\n",
      "predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:66.283s\n",
      "Mean abs error: 1223.09\n",
      "predict time:3.895s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:54.485s\n",
      "Mean abs error: 1307.02\n",
      "predict time:416.198s\n",
      "[0]\ttrain-mae:2809.09+4.38936\ttest-mae:2808.99+13.544\n",
      "[100]\ttrain-mae:1162.22+2.89428\ttest-mae:1208.22+7.71245\n",
      "[200]\ttrain-mae:1122.08+2.95302\ttest-mae:1193.48+7.92345\n",
      "[300]\ttrain-mae:1095.03+3.57973\ttest-mae:1189.54+7.41527\n",
      "[400]\ttrain-mae:1071.45+3.06431\ttest-mae:1188.35+7.74407\n",
      "fit time:209.49s\n",
      "CV-Mean: 1187.92297375+7.54878603704\n",
      "Train time:65.122s\n",
      "XGB Mean abs error: 1175.55\n",
      "XGB predict time:0.215s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 5)\n",
      "Fold run time:897.956s\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:48: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:74.557s\n",
      "Mean abs error: 1243.73\n",
      "predict time:4.291s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.81s\n",
      "Mean abs error: 1329.68\n",
      "predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:67.073s\n",
      "Mean abs error: 1234.18\n",
      "predict time:3.881s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:57.376s\n",
      "Mean abs error: 1316.09\n",
      "predict time:363.199s\n",
      "[0]\ttrain-mae:2806.92+1.9964\ttest-mae:2807.01+5.73712\n",
      "[100]\ttrain-mae:1159.19+1.9946\ttest-mae:1203.68+8.02486\n",
      "[200]\ttrain-mae:1118.95+2.24918\ttest-mae:1188.95+7.41114\n",
      "[300]\ttrain-mae:1091.93+2.19028\ttest-mae:1185.41+7.31866\n",
      "[400]\ttrain-mae:1068.9+2.23857\ttest-mae:1183.71+7.74311\n",
      "fit time:199.453s\n",
      "CV-Mean: 1183.5943605+7.68670666971\n",
      "Train time:62.087s\n",
      "XGB Mean abs error: 1184.20\n",
      "XGB predict time:0.216s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 5)\n",
      "Fold run time:836.683s\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:74.868s\n",
      "Mean abs error: 1243.18\n",
      "predict time:4.178s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.853s\n",
      "Mean abs error: 1337.48\n",
      "predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:67.667s\n",
      "Mean abs error: 1237.70\n",
      "predict time:3.878s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:56.767s\n",
      "Mean abs error: 1317.76\n",
      "predict time:363.696s\n",
      "[0]\ttrain-mae:2808.43+3.51691\ttest-mae:2808.62+10.5181\n",
      "[100]\ttrain-mae:1161.3+2.28435\ttest-mae:1206.48+5.59568\n",
      "[200]\ttrain-mae:1120.35+1.1985\ttest-mae:1191.01+4.66037\n",
      "[300]\ttrain-mae:1093.9+1.70554\ttest-mae:1186.98+4.38954\n",
      "[400]\ttrain-mae:1070.88+1.9875\ttest-mae:1185.95+4.64859\n",
      "fit time:192.158s\n",
      "CV-Mean: 1185.90231325+4.63893993499\n",
      "Train time:59.929s\n",
      "XGB Mean abs error: 1188.25\n",
      "XGB predict time:0.2s\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 5)\n",
      "Fold run time:827.94s\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:74.874s\n",
      "Mean abs error: 1227.11\n",
      "predict time:4.164s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.82s\n",
      "Mean abs error: 1325.48\n",
      "predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:67.332s\n",
      "Mean abs error: 1222.23\n",
      "predict time:3.773s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:54.546s\n",
      "Mean abs error: 1300.59\n",
      "predict time:363.938s\n",
      "[0]\ttrain-mae:2812.2+5.00212\ttest-mae:2812.21+15.5263\n",
      "[100]\ttrain-mae:1164.64+3.5773\ttest-mae:1209.99+5.75262\n",
      "[200]\ttrain-mae:1123.41+2.52242\ttest-mae:1194.48+7.48092\n",
      "[300]\ttrain-mae:1096.51+2.68772\ttest-mae:1190.2+7.6952\n",
      "[400]\ttrain-mae:1072.4+2.85538\ttest-mae:1188.19+7.90829\n",
      "fit time:197.966s\n",
      "CV-Mean: 1188.048401+7.84645725138\n",
      "Train time:61.731s\n",
      "XGB Mean abs error: 1175.01\n",
      "XGB predict time:0.213s\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 5)\n",
      "Fold run time:833.082s\n",
      "Full run time:4319.056s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MAE_tracking.npy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_test = x[fold_start:fold_end].copy()\n",
    "    y_test = y[fold_start:fold_end].copy()\n",
    "    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "    \n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        print(regrList[i])\n",
    "        start_time = time.time()\n",
    "        estimator=skclone(regrList[i], safe=True)\n",
    "        estimator.fit(X_train,y_train)\n",
    "        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        start_time = time.time()\n",
    "        curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "        if fold_result == []:\n",
    "            fold_result = curr_predict\n",
    "        else:\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "        #show some stats on that last regressions run\n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "        print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "        \n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    if use_xgb == True:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "        # now do a prediction and spit out a score(MAE) that means something\n",
    "        start_time = time.time()\n",
    "        curr_predict=gbdt.predict(dtest)\n",
    "        fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "        print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    if x_layer2 == []:\n",
    "        x_layer2=fold_result\n",
    "    else:\n",
    "        x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "        \n",
    "    print \"--layer2 length:\",len(x_layer2)\n",
    "    print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "print(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "#preserve the run\n",
    "joblib.dump(x_layer2,'x_layer2.npy') \n",
    "joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preserve the run\n",
    "x_layer2=joblib.load('x_layer2.npy') \n",
    "MAE_tracking=joblib.load('MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2335.61218   ,   931.1027475 ,  2389.70176   ,  2263.95866667,\n",
       "         2003.27526855],\n",
       "       [ 2030.37446   ,  2431.8660271 ,  2019.39784   ,  1960.644     ,\n",
       "         2156.67480469],\n",
       "       [ 4616.19036   ,  5166.53495085,  4397.67864   ,  4233.80933333,\n",
       "         4164.74316406]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:39.867s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([28, 10,  6, 39, 63, 78, 21, 47, 52, 37,  6, 47, 72, 53, 11], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2335.61218   ,   931.1027475 ,  2389.70176   ,  2263.95866667,\n",
       "         2003.27526855],\n",
       "       [ 2030.37446   ,  2431.8660271 ,  2019.39784   ,  1960.644     ,\n",
       "         2156.67480469],\n",
       "       [ 4616.19036   ,  5166.53495085,  4397.67864   ,  4233.80933333,\n",
       "         4164.74316406]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2335.61218   ,   931.1027475 ,  2389.70176   ,  2263.95866667,\n",
       "         2003.27526855,    28.        ],\n",
       "       [ 2030.37446   ,  2431.8660271 ,  2019.39784   ,  1960.644     ,\n",
       "         2156.67480469,    10.        ],\n",
       "       [ 4616.19036   ,  5166.53495085,  4397.67864   ,  4233.80933333,\n",
       "         4164.74316406,     6.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print len(newcenters)\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') \n",
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "Mean abs error: 1172.63\n",
      "Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "layer2_regr=LinearRegression()\n",
    "\n",
    "layer2_regr.fit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "layer2_predict=layer2_regr.predict(X_layer2_validation)\n",
    "\n",
    "#show some stats on that last regressions run    \n",
    "MAE=np.mean(abs(layer2_predict - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "\n",
    "\n",
    "#with LinearReg: Mean abs error: 1172.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.84+3.83914\ttest-mae:2810.88+12.0644\n",
      "fit time:5.246s\n",
      "CV-Mean: 1156.7436215+4.69451309423\n",
      "Train time:1.167s\n",
      "XGB Mean abs error: 1152.15\n",
      "XGB predict time:0.017s\n"
     ]
    }
   ],
   "source": [
    "# The XGB version of layer 2\n",
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "dtest = xgb.DMatrix(X_layer2_validation)\n",
    "layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "MAE=np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1154.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:XGBLayer2']\n",
      " ['1238.91342907' '1334.21046032' '1233.78249714' '1306.23785443'\n",
      "  '1191.90504822' '1228.7171597' '1321.25625289' '1223.09232861'\n",
      "  '1307.01735259' '1175.54769665' '1243.72564417' '1329.68253777'\n",
      "  '1234.17833305' '1316.08915549' '1184.20364623' '1243.17581381'\n",
      "  '1337.47626755' '1237.69654993' '1317.75716611' '1188.25202739'\n",
      "  '1227.10523139' '1325.48235814' '1222.23013618' '1300.59350197'\n",
      "  '1175.0098914' '1172.63463677' '1152.14644419']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAFsCAYAAADSY/6PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nderUqVX/HydHgLCzsn5XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f409a13c350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y)\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    print curr_predict\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['loss']=layer2_regr.predict(x_layer2_test)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "test_data['loss']=layer2_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
