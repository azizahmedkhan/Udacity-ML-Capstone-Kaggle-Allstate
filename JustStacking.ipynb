{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "#my first tries:\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "    #params from:\n",
    "    #https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.3085,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 7,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 4.2922,\n",
    "        'eval_metric': 'mae',\n",
    "        'eta':0.001,\n",
    "        'gamma': 0.5290,\n",
    "        'subsample':0.9930,\n",
    "        'max_delta_step':0,\n",
    "        'booster':'gbtree',\n",
    "        'nrounds': 1001\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "y=data['loss'].values\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.003s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:83.304s\n",
      "Mean abs error: 1239.77\n",
      "-predict time:4.707s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.024s\n",
      "Mean abs error: 1334.23\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:73.105s\n",
      "Mean abs error: 1234.17\n",
      "-predict time:4.196s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:71.925s\n",
      "Mean abs error: 1308.32\n",
      "-predict time:436.351s\n",
      "[0]\ttrain-mae:3007.71+4.51306\ttest-mae:3007.71+13.6525\n",
      "[100]\ttrain-mae:1453.1+1.22438\ttest-mae:1460.35+11.0091\n",
      "[200]\ttrain-mae:1246.06+2.0418\ttest-mae:1266.07+8.35223\n",
      "[300]\ttrain-mae:1205.61+2.38415\ttest-mae:1236.07+8.27584\n",
      "[400]\ttrain-mae:1183.81+2.03735\ttest-mae:1222.57+8.25325\n",
      "[500]\ttrain-mae:1165.56+1.41196\ttest-mae:1211.78+8.64674\n",
      "[600]\ttrain-mae:1150.82+1.11398\ttest-mae:1203.76+8.9582\n",
      "[700]\ttrain-mae:1138.81+1.03051\ttest-mae:1197.84+8.9838\n",
      "[800]\ttrain-mae:1128.79+1.22852\ttest-mae:1193.4+9.03631\n",
      "[900]\ttrain-mae:1120.39+1.18267\ttest-mae:1190.1+9.12947\n",
      "[1000]\ttrain-mae:1112.95+1.21282\ttest-mae:1187.64+9.13857\n",
      "[1100]\ttrain-mae:1106.25+1.17076\ttest-mae:1185.56+9.0134\n",
      "[1200]\ttrain-mae:1100.43+1.0489\ttest-mae:1184.04+8.98449\n",
      "[1300]\ttrain-mae:1095.13+1.03751\ttest-mae:1182.88+8.97728\n",
      "[1400]\ttrain-mae:1090.06+1.04238\ttest-mae:1181.88+8.99164\n",
      "[1500]\ttrain-mae:1085.28+1.05754\ttest-mae:1181.03+8.98402\n",
      "[1600]\ttrain-mae:1080.76+1.08354\ttest-mae:1180.31+8.9747\n",
      "[1700]\ttrain-mae:1076.25+1.13009\ttest-mae:1179.68+8.92254\n",
      "[1800]\ttrain-mae:1071.95+1.30318\ttest-mae:1179.11+8.87395\n",
      "[1900]\ttrain-mae:1067.87+1.2512\ttest-mae:1178.57+8.85606\n",
      "[2000]\ttrain-mae:1063.92+1.33502\ttest-mae:1178.11+8.79767\n",
      "fit time:538.734s\n",
      "CV-Mean: 1178.1104735+8.79767418922\n",
      "Train time:173.152s\n",
      "XGB Mean abs error: 1180.71\n",
      "-XGB predict time:1.385s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "---Fold run time:1391.684s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:86.588s\n",
      "Mean abs error: 1228.11\n",
      "-predict time:4.679s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.084s\n",
      "Mean abs error: 1321.26\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:75.824s\n",
      "Mean abs error: 1222.02\n",
      "-predict time:4.28s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:76.016s\n",
      "Mean abs error: 1304.28\n",
      "-predict time:433.841s\n",
      "[0]\ttrain-mae:3006.32+4.72061\ttest-mae:3006.31+14.2165\n",
      "[100]\ttrain-mae:1452.12+2.20943\ttest-mae:1459.15+9.26942\n",
      "[200]\ttrain-mae:1245.2+2.6558\ttest-mae:1265.44+9.32338\n",
      "[300]\ttrain-mae:1205.33+3.07178\ttest-mae:1236.37+9.10483\n",
      "[400]\ttrain-mae:1183.66+2.96632\ttest-mae:1223.56+8.79299\n",
      "[500]\ttrain-mae:1165.44+2.34871\ttest-mae:1213.2+8.99019\n",
      "[600]\ttrain-mae:1150.71+1.96978\ttest-mae:1205.32+9.22229\n",
      "[700]\ttrain-mae:1138.93+1.94288\ttest-mae:1199.67+9.12525\n",
      "[800]\ttrain-mae:1128.91+2.03173\ttest-mae:1195.32+9.0632\n",
      "[900]\ttrain-mae:1120.56+2.04106\ttest-mae:1192.14+9.04675\n",
      "[1000]\ttrain-mae:1113.21+1.97199\ttest-mae:1189.74+9.08005\n",
      "[1100]\ttrain-mae:1106.65+1.94773\ttest-mae:1187.86+9.0017\n",
      "[1200]\ttrain-mae:1101.03+1.68664\ttest-mae:1186.41+9.00107\n",
      "[1300]\ttrain-mae:1095.71+1.60606\ttest-mae:1185.18+8.98343\n",
      "[1400]\ttrain-mae:1090.56+1.62031\ttest-mae:1184.13+9.06462\n",
      "[1500]\ttrain-mae:1085.79+1.46605\ttest-mae:1183.22+9.0203\n",
      "[1600]\ttrain-mae:1081.39+1.46444\ttest-mae:1182.5+8.93355\n",
      "[1700]\ttrain-mae:1076.91+1.41922\ttest-mae:1181.78+8.95883\n",
      "[1800]\ttrain-mae:1072.5+1.23257\ttest-mae:1181.12+8.93921\n",
      "[1900]\ttrain-mae:1068.44+1.07004\ttest-mae:1180.57+8.93417\n",
      "[2000]\ttrain-mae:1064.39+1.01837\ttest-mae:1180.14+8.95226\n",
      "fit time:536.043s\n",
      "CV-Mean: 1180.14462275+8.95226115913\n",
      "Train time:168.149s\n",
      "XGB Mean abs error: 1167.27\n",
      "-XGB predict time:1.4s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 5)\n",
      "---Fold run time:1391.752s\n",
      "---Fold:75326 to 112989 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:82.889s\n",
      "Mean abs error: 1240.94\n",
      "-predict time:4.476s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.019s\n",
      "Mean abs error: 1329.69\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.307s\n",
      "Mean abs error: 1235.05\n",
      "-predict time:3.974s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:80.295s\n",
      "Mean abs error: 1315.09\n",
      "-predict time:428.642s\n",
      "[0]\ttrain-mae:3003.93+2.05215\ttest-mae:3003.92+6.10321\n",
      "[100]\ttrain-mae:1449.9+1.30176\ttest-mae:1457+6.03121\n",
      "[200]\ttrain-mae:1242.81+2.30429\ttest-mae:1262.66+8.01724\n",
      "[300]\ttrain-mae:1202.72+3.02283\ttest-mae:1233.07+7.86224\n",
      "[400]\ttrain-mae:1180.96+2.92693\ttest-mae:1220.23+7.9804\n",
      "[500]\ttrain-mae:1162.84+2.5723\ttest-mae:1209.77+8.07332\n",
      "[600]\ttrain-mae:1148.09+2.44114\ttest-mae:1201.88+8.18221\n",
      "[700]\ttrain-mae:1136.42+2.60135\ttest-mae:1196.05+8.09132\n",
      "[800]\ttrain-mae:1126.74+2.74752\ttest-mae:1191.67+7.93012\n",
      "[900]\ttrain-mae:1118.48+2.81967\ttest-mae:1188.33+7.95315\n",
      "[1000]\ttrain-mae:1111.21+2.72951\ttest-mae:1185.82+7.9503\n",
      "[1100]\ttrain-mae:1104.83+2.57877\ttest-mae:1183.78+7.8793\n",
      "[1200]\ttrain-mae:1099.38+2.43457\ttest-mae:1182.35+7.85792\n",
      "[1300]\ttrain-mae:1094.31+2.26822\ttest-mae:1181.15+7.91675\n",
      "[1400]\ttrain-mae:1089.43+2.19857\ttest-mae:1180.13+8.03376\n",
      "[1500]\ttrain-mae:1084.81+2.1021\ttest-mae:1179.27+8.12091\n",
      "[1600]\ttrain-mae:1080.53+2.10088\ttest-mae:1178.53+8.10742\n",
      "[1700]\ttrain-mae:1076.28+2.18683\ttest-mae:1177.89+8.09896\n",
      "[1800]\ttrain-mae:1072.17+2.24099\ttest-mae:1177.3+8.07153\n",
      "[1900]\ttrain-mae:1068.3+2.2675\ttest-mae:1176.77+8.13677\n",
      "[2000]\ttrain-mae:1064.51+2.22148\ttest-mae:1176.35+8.18161\n",
      "fit time:534.66s\n",
      "CV-Mean: 1176.35211175+8.1816090598\n",
      "Train time:169.924s\n",
      "XGB Mean abs error: 1180.40\n",
      "-XGB predict time:1.406s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 5)\n",
      "---Fold run time:1384.429s\n",
      "---Fold:112989 to 150652 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:86.791s\n",
      "Mean abs error: 1243.20\n",
      "-predict time:4.595s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.99s\n",
      "Mean abs error: 1337.51\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.491s\n",
      "Mean abs error: 1238.97\n",
      "-predict time:4.075s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:82.845s\n",
      "Mean abs error: 1315.60\n",
      "-predict time:428.249s\n",
      "[0]\ttrain-mae:3005.37+3.61224\ttest-mae:3005.37+10.8623\n",
      "[100]\ttrain-mae:1451.32+1.88781\ttest-mae:1458.43+7.16016\n",
      "[200]\ttrain-mae:1244.59+3.43622\ttest-mae:1264.94+4.36464\n",
      "[300]\ttrain-mae:1204.29+3.42236\ttest-mae:1235.41+4.24696\n",
      "[400]\ttrain-mae:1182.6+2.99015\ttest-mae:1222.42+4.35612\n",
      "[500]\ttrain-mae:1164.46+2.2737\ttest-mae:1211.69+4.60844\n",
      "[600]\ttrain-mae:1150.02+1.83356\ttest-mae:1203.76+4.5767\n",
      "[700]\ttrain-mae:1138.31+1.49931\ttest-mae:1197.88+4.43756\n",
      "[800]\ttrain-mae:1128.66+1.4274\ttest-mae:1193.45+4.29284\n",
      "[900]\ttrain-mae:1120.5+1.17538\ttest-mae:1190.04+4.26165\n",
      "[1000]\ttrain-mae:1113.35+1.06611\ttest-mae:1187.56+4.16735\n",
      "[1100]\ttrain-mae:1106.93+0.743326\ttest-mae:1185.55+4.19173\n",
      "[1200]\ttrain-mae:1101.33+0.625638\ttest-mae:1183.95+4.21459\n",
      "[1300]\ttrain-mae:1096.23+0.593322\ttest-mae:1182.71+4.28814\n",
      "[1400]\ttrain-mae:1091.25+0.578746\ttest-mae:1181.61+4.33184\n",
      "[1500]\ttrain-mae:1086.62+0.790564\ttest-mae:1180.75+4.34472\n",
      "[1600]\ttrain-mae:1082.27+1.04191\ttest-mae:1180.01+4.27139\n",
      "[1700]\ttrain-mae:1077.95+1.14925\ttest-mae:1179.26+4.2469\n",
      "[1800]\ttrain-mae:1073.72+1.30703\ttest-mae:1178.6+4.25099\n",
      "[1900]\ttrain-mae:1069.66+1.52858\ttest-mae:1178.11+4.2705\n",
      "[2000]\ttrain-mae:1065.62+1.46077\ttest-mae:1177.63+4.24648\n",
      "fit time:536.388s\n",
      "CV-Mean: 1177.62973025+4.24647792023\n",
      "Train time:169.938s\n",
      "XGB Mean abs error: 1180.32\n",
      "-XGB predict time:1.402s\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 5)\n",
      "---Fold run time:1392.613s\n",
      "---Fold:150652 to 188318 of: 188318\n",
      "\n",
      "---folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:83.715s\n",
      "Mean abs error: 1228.92\n",
      "-predict time:4.498s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.041s\n",
      "Mean abs error: 1325.46\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.378s\n",
      "Mean abs error: 1223.20\n",
      "-predict time:4.063s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:82.292s\n",
      "Mean abs error: 1298.03\n",
      "-predict time:417.029s\n",
      "[0]\ttrain-mae:3009.26+5.20511\ttest-mae:3009.27+15.7072\n",
      "[100]\ttrain-mae:1454.2+2.90197\ttest-mae:1461.42+11.4255\n",
      "[200]\ttrain-mae:1247.81+1.93918\ttest-mae:1267.72+7.17111\n",
      "[300]\ttrain-mae:1207.73+2.42809\ttest-mae:1238.02+5.91948\n",
      "[400]\ttrain-mae:1186.22+2.2128\ttest-mae:1225.05+6.29085\n",
      "[500]\ttrain-mae:1168.42+2.34185\ttest-mae:1214.56+6.68506\n",
      "[600]\ttrain-mae:1153.84+2.98973\ttest-mae:1206.57+6.88666\n",
      "[700]\ttrain-mae:1141.99+3.44772\ttest-mae:1200.44+7.10758\n",
      "[800]\ttrain-mae:1132.16+3.63759\ttest-mae:1195.96+7.23953\n",
      "[900]\ttrain-mae:1123.92+3.81863\ttest-mae:1192.54+7.40867\n",
      "[1000]\ttrain-mae:1116.82+3.81461\ttest-mae:1190.1+7.45608\n",
      "[1100]\ttrain-mae:1110.32+3.64589\ttest-mae:1188.09+7.72631\n",
      "[1200]\ttrain-mae:1104.47+3.71636\ttest-mae:1186.5+7.82777\n",
      "[1300]\ttrain-mae:1099+3.77431\ttest-mae:1185.2+7.93815\n",
      "[1400]\ttrain-mae:1093.87+3.94873\ttest-mae:1184.09+8.02585\n",
      "[1500]\ttrain-mae:1089.02+4.03149\ttest-mae:1183.17+8.07031\n",
      "[1600]\ttrain-mae:1084.46+4.0174\ttest-mae:1182.39+8.17451\n",
      "[1700]\ttrain-mae:1080.08+3.94519\ttest-mae:1181.7+8.21338\n",
      "[1800]\ttrain-mae:1075.77+3.80602\ttest-mae:1181.12+8.27636\n",
      "[1900]\ttrain-mae:1071.71+3.93239\ttest-mae:1180.6+8.29033\n",
      "[2000]\ttrain-mae:1067.75+3.89852\ttest-mae:1180.13+8.29478\n",
      "fit time:526.35s\n",
      "CV-Mean: 1180.13473525+8.29478169639\n",
      "Train time:169.171s\n",
      "XGB Mean abs error: 1168.29\n",
      "-XGB predict time:1.417s\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 5)\n",
      "---Fold run time:1366.772s\n",
      "----Full run time:6927.252s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1207.69\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "MAE=np.mean(abs(avg_column - y))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2310.24808   ,   938.07532411,  2328.7196    ,  2117.37933333,\n",
       "         2054.74121094,  1949.83270968],\n",
       "       [ 2020.34518   ,  2445.06798587,  1905.09504   ,  2508.722     ,\n",
       "         2099.59960938,  2195.76596305],\n",
       "       [ 4828.13458   ,  5188.57631961,  4470.3441    ,  4776.422     ,\n",
       "         4456.13964844,  4743.92332961]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:47.916s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.002s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regrl2_grid)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1178.10\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1198.49\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3007.69+4.51238\ttest-mae:3007.68+13.5805\n",
      "[100]\ttrain-mae:1364.42+1.869\ttest-mae:1370.9+9.62824\n",
      "[200]\ttrain-mae:1146.62+1.59205\ttest-mae:1163.3+7.47931\n",
      "CV time:42.86s\n",
      "CV-Mean: 1157.3493345+7.5035644092\n",
      "Fit time:11.673s\n",
      "XGB Mean abs error: 1158.12\n",
      "XGB predict time:0.155s\n",
      "AVG Mean abs error: 1161.03\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1163.32\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1185.98\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3006.3+4.7174\ttest-mae:3006.31+14.1448\n",
      "[100]\ttrain-mae:1363.81+1.63925\ttest-mae:1370.43+7.40496\n",
      "[200]\ttrain-mae:1147.41+1.69895\ttest-mae:1164.34+7.57997\n",
      "CV time:42.246s\n",
      "CV-Mean: 1158.76065075+7.91714653832\n",
      "Fit time:11.641s\n",
      "XGB Mean abs error: 1151.42\n",
      "XGB predict time:0.152s\n",
      "AVG Mean abs error: 1151.23\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1176.02\n",
      "Score: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:60: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1195.68\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3003.91+2.05229\ttest-mae:3003.92+6.15739\n",
      "[100]\ttrain-mae:1361.54+1.13695\ttest-mae:1368.1+3.12671\n",
      "[200]\ttrain-mae:1144.26+1.623\ttest-mae:1160.87+6.20706\n",
      "CV time:43.664s\n",
      "CV-Mean: 1155.1474305+6.485062782\n",
      "Fit time:12.452s\n",
      "XGB Mean abs error: 1165.72\n",
      "XGB predict time:0.148s\n",
      "AVG Mean abs error: 1164.80\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1176.73\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1197.20\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3005.35+3.61056\ttest-mae:3005.36+10.8724\n",
      "[100]\ttrain-mae:1362.47+1.5413\ttest-mae:1368.81+4.29249\n",
      "[200]\ttrain-mae:1145.6+0.965404\ttest-mae:1162.01+4.05611\n",
      "CV time:42.112s\n",
      "CV-Mean: 1156.03872675+4.07808872568\n",
      "Fit time:11.347s\n",
      "XGB Mean abs error: 1162.59\n",
      "XGB predict time:0.154s\n",
      "AVG Mean abs error: 1163.48\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1163.77\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1184.23\n",
      "Score: 0.57\n",
      "[0]\ttrain-mae:3009.25+5.20193\ttest-mae:3009.26+15.7003\n",
      "[100]\ttrain-mae:1365.66+2.16304\ttest-mae:1371.96+8.29459\n",
      "[200]\ttrain-mae:1148.2+1.90662\ttest-mae:1164.98+7.03192\n",
      "CV time:42.112s\n",
      "CV-Mean: 1159.15338125+6.55531738406\n",
      "Fit time:12.832s\n",
      "XGB Mean abs error: 1150.07\n",
      "XGB predict time:0.151s\n",
      "AVG Mean abs error: 1150.79\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtrain)\n",
    "    MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:3007.71+4.13071\ttest-mae:3007.72+12.4544\n",
      "[100]\ttrain-mae:1363.08+1.92503\ttest-mae:1368.25+9.30303\n",
      "[200]\ttrain-mae:1147.21+1.1818\ttest-mae:1160.97+5.29401\n",
      "CV time:33.202s\n",
      "CV-Mean: 1155.8162535+4.59577552648\n",
      "Fit time:9.843s\n",
      "XGB Mean abs error: 1151.48\n",
      "XGB predict time:0.177s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgbfit(X_layer3_train,y_layer3_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtrain)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:XGBLayer2']\n",
      " ['1239.77397433' '1334.22892128' '1234.16539149' '1308.32171075'\n",
      "  '1180.71331536' '1228.11308451' '1321.26434323' '1222.01540034'\n",
      "  '1304.27859455' '1167.27278951' '1240.94362976' '1329.6941864'\n",
      "  '1235.04853526' '1315.08723108' '1180.39724176' '1243.19789205'\n",
      "  '1337.50871287' '1238.9658797' '1315.59540886' '1180.32300379'\n",
      "  '1228.92235947' '1325.45831784' '1223.20486793' '1298.02863498'\n",
      "  '1168.29491469' '1178.10347239' '1198.48615629' '1158.11878253'\n",
      "  '1163.32036003' '1185.98159341' '1151.41503931' '1176.01768804'\n",
      "  '1195.67779446' '1165.71807139' '1176.73396185' '1197.19955886'\n",
      "  '1162.59427936' '1163.76856523' '1184.22813712' '1150.06860055'\n",
      "  '1151.48016932' '1151.48016932' '1178.10347239' '1198.48615629'\n",
      "  '1158.11878253' '1163.32036003' '1185.98159341' '1151.41503931'\n",
      "  '1176.01768804' '1195.67779446' '1165.71807139' '1176.73396185'\n",
      "  '1197.19955886' '1162.59427936' '1163.76856523' '1184.22813712'\n",
      "  '1150.06860055' '1151.48016932']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAHJCAYAAACbsn7SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrKysRuUXRVZWVlZWo/4/dhd5iq9G940AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe94313c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-9ab1b66ab77e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregrList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# use the estimator from the training, but refit to the whole data set!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mcurr_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict time:{}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 290\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[1;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1643.59790039\\n',\n",
       " '6,2007.50891113\\n',\n",
       " '9,8909.9375\\n',\n",
       " '12,5939.16650391\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=layer3_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1964.057617\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
