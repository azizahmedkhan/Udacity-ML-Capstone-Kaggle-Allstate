{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "#my first tries:\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "    #params from:\n",
    "    #https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.3085,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 7,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 4.2922,\n",
    "        'eval_metric': 'mae',\n",
    "        'eta':0.1,\n",
    "        'gamma': 0.5290,\n",
    "        'subsample':0.9930,\n",
    "        'max_delta_step':0,\n",
    "        'booster':'gbtree',\n",
    "        'nrounds': 1001\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_layer2_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "y=data['loss'].values\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:37663'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.80,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=i)\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.03s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regrList[i]))\n",
    "    filename= 'grid_regr{}.pkl'.format(i)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        regrList[i]=joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regrList[i],\n",
    "                                   param_grid= paramater_grid[i],\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(X_train,y_train)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regrList[i].set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regrList[i],filename) \n",
    "        del grid_search\n",
    "        \n",
    "\n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "#Full GridSearch run time:4774.187s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_layer2.npy  exists, importing \n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1206.92\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "MAE=np.mean(abs(avg_column - y))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2270.87052   ,   938.07532411,  2328.7196    ,  2117.37933333,\n",
       "         1878.32763672,  1906.67448283],\n",
       "       [ 2135.41948   ,  2445.06798587,  1905.09504   ,  2508.722     ,\n",
       "         2148.07275391,  2228.47545196],\n",
       "       [ 4621.747     ,  5188.57631961,  4470.3441    ,  4776.422     ,\n",
       "         4398.32861328,  4691.08360658]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:54.137s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_layer2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_layer2_KNN.pkl not present, running a gridsearch\n",
      "run time:63.422s\n",
      "Full GridSearch run time:63.424s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regrl2_grid=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regrl2_grid)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1184.32\n",
      "Score: 0.56\n",
      "KNeighborsRegressor Mean abs error: 1225.80\n",
      "Score: 0.55\n",
      "[0]\ttrain-mae:2810.32+4.1187\ttest-mae:2810.34+13.1787\n",
      "fit time:6.769s\n",
      "CV-Mean: 1154.9040835+7.6799327051\n",
      "Train time:1.382s\n",
      "XGB Mean abs error: 1156.76\n",
      "XGB predict time:0.011s\n",
      "AVG Mean abs error: 1165.75\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1167.89\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1211.93\n",
      "Score: 0.55\n",
      "[0]\ttrain-mae:2809+4.33845\ttest-mae:2808.99+13.3285\n",
      "fit time:7.108s\n",
      "CV-Mean: 1157.04394525+7.52055387162\n",
      "Train time:1.463s\n",
      "XGB Mean abs error: 1148.70\n",
      "XGB predict time:0.01s\n",
      "AVG Mean abs error: 1154.38\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1177.32\n",
      "Score: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:60: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1213.47\n",
      "Score: 0.56\n",
      "[0]\ttrain-mae:2806.92+2.02533\ttest-mae:2806.91+5.65561\n",
      "fit time:6.966s\n",
      "CV-Mean: 1154.0330505+7.41840817136\n",
      "Train time:1.397s\n",
      "XGB Mean abs error: 1162.63\n",
      "XGB predict time:0.01s\n",
      "AVG Mean abs error: 1163.84\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1184.15\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1226.40\n",
      "Score: 0.55\n",
      "[0]\ttrain-mae:2808.44+3.44844\ttest-mae:2808.46+10.1934\n",
      "fit time:6.824s\n",
      "CV-Mean: 1153.6203615+4.55242084491\n",
      "Train time:1.346s\n",
      "XGB Mean abs error: 1162.00\n",
      "XGB predict time:0.011s\n",
      "AVG Mean abs error: 1169.10\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1167.69\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1204.91\n",
      "Score: 0.56\n",
      "[0]\ttrain-mae:2812.13+4.92401\ttest-mae:2812.23+15.1154\n",
      "fit time:6.998s\n",
      "CV-Mean: 1158.19061275+7.05999267941\n",
      "Train time:1.366s\n",
      "XGB Mean abs error: 1148.06\n",
      "XGB predict time:0.01s\n",
      "AVG Mean abs error: 1152.32\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtrain)\n",
    "    MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.75+3.85218\ttest-mae:2810.9+11.9859\n",
      "fit time:5.833s\n",
      "CV-Mean: 1156.85769675+4.23318510592\n",
      "Train time:1.2s\n",
      "XGB Mean abs error: 1152.06\n",
      "XGB predict time:0.014s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgbfit(X_layer3_train,y_layer3_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtrain)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2']\n",
      " ['1240.23526958' '1334.22892128' '1234.16539149' '1308.32171075'\n",
      "  '1190.38638325' '1228.11308451' '1321.26434323' '1222.01540034'\n",
      "  '1304.27859455' '1175.41720663' '1240.94362976' '1329.6941864'\n",
      "  '1235.04853526' '1315.08723108' '1183.42793168' '1243.19789205'\n",
      "  '1337.50871287' '1238.9658797' '1315.59540886' '1190.29574511'\n",
      "  '1228.92235947' '1325.45831784' '1223.20486793' '1298.02863498'\n",
      "  '1174.50782147' '1184.31970882' '1299.23787022' '1156.75778136'\n",
      "  '1167.81103894' '1287.69516618' '1148.70446198' '1177.12084662'\n",
      "  '1287.21991498' '1162.62741133' '1184.20619666' '1301.99115264'\n",
      "  '1161.99745784' '1167.62367746' '1283.50225577' '1148.06071869'\n",
      "  '1153.00879206' '1184.32126456' '1225.7951485' '1156.75778136'\n",
      "  '1167.88706656' '1211.92591684' '1148.70446198' '1177.31667056'\n",
      "  '1213.47374746' '1162.62741133' '1184.14537739' '1226.40070292'\n",
      "  '1161.99745784' '1167.69084421' '1204.90992414' '1148.06071869'\n",
      "  '1152.05766208']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAG9CAYAAAALATEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrKysRuUHRVZWVlZWo/KDIisrKyurUf8fCKV15gx87oEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd37340fd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:131.704s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:5.177s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:113.863s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1706.439s\n",
      "[0]\ttrain-mae:2809.84+7.17522\ttest-mae:2809.9+23.9038\n",
      "[100]\ttrain-mae:1167.71+1.53088\ttest-mae:1205.64+5.13143\n",
      "[200]\ttrain-mae:1129.86+1.52816\ttest-mae:1189.54+5.37024\n",
      "[300]\ttrain-mae:1106.08+1.70655\ttest-mae:1185.62+5.31038\n",
      "fit time:223.36s\n",
      "CV-Mean: 1184.47369375+5.28535043252\n",
      "Train time:69.018s\n",
      "XGB predict time:2000.831s\n",
      "AVG column added - length of new row: 7\n",
      "Fold run time:2251.587s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:48.434s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, 65, 47,  2, 76, 78, 54, 35,  9, 33, 33, 29, 68, 78, 53], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  1976.01754   ,   1090.86633405,   2166.7021    ,   1853.79933333,\n",
       "          1729.20751953,   1763.31856538],\n",
       "       [  2452.88312   ,   1955.48186205,   2415.1609    ,   2971.15266667,\n",
       "          2069.85009766,   2372.90572928],\n",
       "       [  9394.0663    ,  11267.87839405,   8773.54792   ,   8961.596     ,\n",
       "         10355.04101562,   9750.42592593]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.97601754e+03,   1.09086633e+03,   2.16670210e+03,\n",
       "          1.85379933e+03,   1.72920752e+03,   1.76331857e+03,\n",
       "          1.00000000e+00],\n",
       "       [  2.45288312e+03,   1.95548186e+03,   2.41516090e+03,\n",
       "          2.97115267e+03,   2.06985010e+03,   2.37290573e+03,\n",
       "          6.50000000e+01],\n",
       "       [  9.39406630e+03,   1.12678784e+04,   8.77354792e+03,\n",
       "          8.96159600e+03,   1.03550410e+04,   9.75042593e+03,\n",
       "          4.70000000e+01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 7\n",
      "run time:48.449s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.004s\n",
      "KNeighbors predict time:3.19s\n",
      "XGB predict time:0.043s\n",
      "AVG predict time:0.003s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1617.82836914\\n',\n",
       " '6,1919.07067871\\n',\n",
       " '9,9038.97753906\\n',\n",
       " '12,5837.64990234\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=layer3_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1961.541748\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
