{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=False #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n",
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)\n",
    "\n",
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "test_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n",
    "\n",
    "\n",
    "x_test = test_data.copy()\n",
    "x_test.drop(['id'],1,inplace=True)\n",
    "\n",
    "# we don't want the ID columns in X, and of course not loss either\n",
    "x=data.drop(['id','loss'],1)\n",
    "# loss is our label\n",
    "y=data['loss']\n",
    "\n",
    "#minmax scaler\n",
    "scaler= MinMaxScaler() \n",
    "x = scaler.fit_transform(x)\n",
    "x_test_data = scaler.fit_transform(x_test)\n",
    "\n",
    "#display(x[:5])\n",
    "#display(y.head(5))\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7], 'leaf_size': [3, 15, 30, 50]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1,),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7],\n",
    "                             leaf_size =[3,15,30,50])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time0 = time.time()\n",
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.85,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "for i in range(len(regrList)):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regrList[i]))\n",
    "    filename= 'grid_regr{}.pkl'.format(i)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        regrList[i]=joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regrList[i], param_grid= paramater_grid[i], n_jobs= -1, scoring=make_scorer(mean_absolute_error)) \n",
    "        grid_search.fit(X_train,y_train)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regrList[i].set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regrList[i],filename) \n",
    "\n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:23.995s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.215s\n",
      "Mean abs error: 1327.36\n",
      "Score: 0.46\n",
      "\n",
      "fit time:3.408s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1335.61\n",
      "Score: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:26.723s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.224s\n",
      "Mean abs error: 1328.68\n",
      "Score: 0.46\n",
      "\n",
      "fit time:83.938s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:293.368s\n",
      "Mean abs error: 1374.38\n",
      "Score: 0.40\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 4)\n",
      "Fold run time:728.843s\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:23.963s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.234s\n",
      "Mean abs error: 1308.11\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.404s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1322.17\n",
      "Score: 0.48\n",
      "\n",
      "fit time:27.073s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.215s\n",
      "Mean abs error: 1302.42\n",
      "Score: 0.49\n",
      "\n",
      "fit time:78.394s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:301.992s\n",
      "Mean abs error: 1371.71\n",
      "Score: 0.38\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 4)\n",
      "Fold run time:737.675s\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:51: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:25.566s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.217s\n",
      "Mean abs error: 1323.98\n",
      "Score: 0.47\n",
      "\n",
      "fit time:3.454s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1330.79\n",
      "Score: 0.48\n",
      "\n",
      "fit time:27.954s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.237s\n",
      "Mean abs error: 1317.28\n",
      "Score: 0.48\n",
      "\n",
      "fit time:82.133s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:301.588s\n",
      "Mean abs error: 1392.41\n",
      "Score: 0.38\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 4)\n",
      "Fold run time:743.094s\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:25.931s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.227s\n",
      "Mean abs error: 1326.36\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.494s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1338.83\n",
      "Score: 0.48\n",
      "\n",
      "fit time:26.534s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.242s\n",
      "Mean abs error: 1323.89\n",
      "Score: 0.49\n",
      "\n",
      "fit time:82.186s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:313.701s\n",
      "Mean abs error: 1389.29\n",
      "Score: 0.39\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 4)\n",
      "Fold run time:766.867s\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "\n",
      "fit time:25.437s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.216s\n",
      "Mean abs error: 1314.57\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.383s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1326.81\n",
      "Score: 0.48\n",
      "\n",
      "fit time:27.061s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.36s\n",
      "Mean abs error: 1311.63\n",
      "Score: 0.50\n",
      "\n",
      "fit time:77.084s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:289.417s\n",
      "Mean abs error: 1367.27\n",
      "Score: 0.39\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 4)\n",
      "Fold run time:713.285s\n",
      "Full run time:3689.766s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_test = x[fold_start:fold_end].copy()\n",
    "    y_test = y[fold_start:fold_end].copy()\n",
    "    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "    \n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()\n",
    "        regrList[i].fit(X_train,y_train)\n",
    "        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(regrList[i])\n",
    "        curr_predict=regrList[i].predict(X_test)\n",
    "        if fold_result == []:\n",
    "            fold_result = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))\n",
    "        \n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        #show some stats on that last regressions run\n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "        print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))\n",
    "    \n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    \n",
    "    if use_xgb == True:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "        # now do a prediction and spit out a score(MAE) that means something\n",
    "        start_time = time.time()\n",
    "        curr_predict=gbdt.predict(dtest)\n",
    "        fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "        print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    if x_layer2 == []:\n",
    "        x_layer2=fold_result\n",
    "    else:\n",
    "        x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "        \n",
    "    print \"--layer2 length:\",len(x_layer2)\n",
    "    print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "print(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "Mean abs error: 1233.34\n",
      "Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "layer2_regr=LinearRegression()\n",
    "\n",
    "layer2_regr.fit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "layer2_predict=layer2_regr.predict(X_layer2_validation)\n",
    "\n",
    "#show some stats on that last regressions run    \n",
    "MAE=np.mean(abs(layer2_predict - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "\n",
    "\n",
    "#with LinearReg: Mean abs error: 1238.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.82+3.85525\ttest-mae:2810.83+12.0151\n",
      "fit time:5.969s\n",
      "CV-Mean: 1204.1655575+5.2453239012\n",
      "Train time:1.229s\n",
      "XGB Mean abs error: 1201.00\n",
      "XGB predict time:0.028s\n"
     ]
    }
   ],
   "source": [
    "# The XGB version of layer 2\n",
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "dtest = xgb.DMatrix(X_layer2_validation)\n",
    "layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "MAE=np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#with LinearReg: XGB Mean abs error: 1205.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:37663-75326:0' 'run:37663-75326:1' 'run:37663-75326:2'\n",
      "  'run:37663-75326:3' 'run:75326-112989:0' 'run:75326-112989:1'\n",
      "  'run:75326-112989:2' 'run:75326-112989:3' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:150652-188318:0' 'run:150652-188318:1' 'run:150652-188318:2'\n",
      "  'run:150652-188318:3' 'run:linearLayer2' 'run:XGBLayer2']\n",
      " ['1327.36027305' '1335.6109609' '1328.67511744' '1374.37524594'\n",
      "  '1308.1138336' '1322.16612723' '1302.41998256' '1371.70692547'\n",
      "  '1323.97581982' '1330.79485737' '1317.28013905' '1392.41162106'\n",
      "  '1326.35518582' '1338.83235314' '1323.88590739' '1389.28851228'\n",
      "  '1314.57310389' '1326.80644028' '1311.62821712' '1367.2665912'\n",
      "  '1233.34382615' '1201.0000153']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFVCAYAAAANA4MgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1qcEX2m7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89c6d1bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:35.36s\n",
      "[ 1842.459  2306.525  9190.11  ...,  2764.702  1228.724  3246.617]\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.52s\n",
      "[  1163.44414896   1967.44005674  11395.08046029 ...,   2829.61141038\n",
      "   1064.38684159   4669.52455308]\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:37.292s\n",
      "[ 1414.256  2970.944  9324.873 ...,  2667.608  1470.346  4034.921]\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1298.702s\n",
      "[  1720.48    2996.604  11623.686 ...,   2236.55     768.226   2704.898]\n",
      "Fold run time:1375.883s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()\n",
    "    print(regrList[i])\n",
    "    regrList[i].fit(x,y)\n",
    "    curr_predict=regrList[i].predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    print curr_predict\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    gbdt=xgbfit(x,y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'size of original test data:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "125546"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(125546, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(188318, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample of layer2 test:\\n', array([[  1842.459     ,   1163.44414896,   1414.256     ,   1720.48      ],\n",
      "       [  2306.525     ,   1967.44005674,   2970.944     ,   2996.604     ],\n",
      "       [  9190.11      ,  11395.08046029,   9324.873     ,  11623.686     ],\n",
      "       [  6042.771     ,   5747.09269744,   4307.957     ,   5526.742     ]]))\n",
      "('x_layer2_test mean:', array([ 3101.19923724,  3054.89763167,  3090.45871574,  2809.82701254]))\n",
      "('x_layer2 mean:', array([ 3095.46559111,  3037.45612893,  3093.94178225,  2814.07853901]))\n",
      "('x_layer2_test std:', array([ 2245.49674282,  2010.00564212,  2256.91863819,  1913.89063739]))\n",
      "('x_layer2 std:', array([ 2268.84650707,  2017.89115492,  2266.12894136,  1929.42056015]))\n",
      "('num outliers:', 9)\n",
      "3095.46559111\n",
      "('num outliers:', 0)\n",
      "3095.46559111\n"
     ]
    }
   ],
   "source": [
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1371.71063\\n',\n",
       " '6,2462.56943492\\n',\n",
       " '9,10493.2610586\\n',\n",
       " '12,5372.17853016\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['loss']=layer2_regr.predict(x_layer2_test)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1358.60302734\\n',\n",
       " '6,2158.29614258\\n',\n",
       " '9,9105.84667969\\n',\n",
       " '12,4786.39501953\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "test_data['loss']=layer2_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1869.353149\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
