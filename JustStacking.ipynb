{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Just a nice stacking ensemble model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        print(\"debug 1\")\n",
    "        grid_search.fit(x,y)\n",
    "        print \"debug2\"\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####unused!, but for a reference idea\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "        \n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    combineddata[feat] = combineddata[comb[0]] + combineddata[comb[1]]\n",
    "    #combineddata[feat] = combineddata[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "combineddata[\"cont1\"] = np.sqrt(combineddata[\"cont1\"])\n",
    "combineddata[\"cont4\"] = np.sqrt(combineddata[\"cont4\"])\n",
    "combineddata[\"cont5\"] = np.sqrt(combineddata[\"cont5\"])\n",
    "combineddata[\"cont8\"] = np.sqrt(combineddata[\"cont8\"])\n",
    "combineddata[\"cont10\"] = np.sqrt(combineddata[\"cont10\"])\n",
    "combineddata[\"cont11\"] = np.sqrt(combineddata[\"cont11\"])\n",
    "combineddata[\"cont12\"] = np.sqrt(combineddata[\"cont12\"])\n",
    "\n",
    "combineddata[\"cont6\"] = np.log(combineddata[\"cont6\"] + 0000.1)\n",
    "combineddata[\"cont7\"] = np.log(combineddata[\"cont7\"] + 0000.1)\n",
    "combineddata[\"cont9\"] = np.log(combineddata[\"cont9\"] + 0000.1)\n",
    "combineddata[\"cont13\"] = np.log(combineddata[\"cont13\"] + 0000.1)\n",
    "combineddata[\"cont14\"] = (np.maximum(combineddata[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    57\n",
      "1    29\n",
      "2    23\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 133 entries, cat1 to clusters\n",
      "dtypes: float64(131), int32(1), int64(1)\n",
      "memory usage: 190.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>-0.255633</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>-0.792214</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>-1.016372</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10    ...     \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0    ...      \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0    ...      \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0    ...      \n",
       "\n",
       "      cont8     cont9    cont10    cont11    cont12    cont13    cont14  \\\n",
       "0  0.296826 -0.255633  0.916140  0.744792  0.761787 -0.070392  0.984628   \n",
       "1  0.698552 -0.792214  0.664384  0.560798  0.585682 -0.330645  0.343682   \n",
       "2  0.220662 -1.016372  0.571049  0.599347  0.591963 -1.211326  1.018094   \n",
       "\n",
       "      loss  id  clusters  \n",
       "0  2213.18   1        57  \n",
       "1  1283.60   2        29  \n",
       "2  3005.09   5        23  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 133 entries, cat1 to clusters\n",
      "dtypes: float64(131), int32(1), int64(1)\n",
      "memory usage: 190.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>-0.255633</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>-0.792214</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>-1.016372</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10    ...     \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0    ...      \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0    ...      \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0    ...      \n",
       "\n",
       "      cont8     cont9    cont10    cont11    cont12    cont13    cont14  \\\n",
       "0  0.296826 -0.255633  0.916140  0.744792  0.761787 -0.070392  0.984628   \n",
       "1  0.698552 -0.792214  0.664384  0.560798  0.585682 -0.330645  0.343682   \n",
       "2  0.220662 -1.016372  0.571049  0.599347  0.591963 -1.211326  1.018094   \n",
       "\n",
       "      loss  id  clusters  \n",
       "0  2213.18   1        57  \n",
       "1  1283.60   2        29  \n",
       "2  3005.09   5        23  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "#y=data['loss'].values\n",
    "shift=200\n",
    "y = np.log(data['loss']+shift).ravel()\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "display(data.info())\n",
    "display(data.head(3))\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [3, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {'alpha': [0.05, 0.5, 1, 2, 4, 40, 140, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 7, 10, 25, 50, 200, 500]}]\n",
      "('number of scikitlearn regressors to use:', 3)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "#regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                       # dict(n_neighbors=[2,5,7,15],\n",
    "                             #leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.20,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "Full GridSearch run time:0.003s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.22672+0.00177661\ttest-mae:7.22672+0.00536082\n",
      "[100]\ttrain-mae:2.64646+0.000664896\ttest-mae:2.64652+0.0040325\n",
      "[200]\ttrain-mae:0.981343+0.00043104\ttest-mae:0.982513+0.00372322\n",
      "[300]\ttrain-mae:0.482078+0.00030159\ttest-mae:0.491482+0.00298199\n",
      "[400]\ttrain-mae:0.381453+0.000386051\ttest-mae:0.399713+0.00220338\n",
      "[500]\ttrain-mae:0.358078+0.00054222\ttest-mae:0.383014+0.0019149\n",
      "[600]\ttrain-mae:0.348268+0.000668235\ttest-mae:0.378671+0.0017794\n",
      "[700]\ttrain-mae:0.341646+0.000723297\ttest-mae:0.37673+0.00176573\n",
      "[800]\ttrain-mae:0.336394+0.00071282\ttest-mae:0.375575+0.00175646\n",
      "[900]\ttrain-mae:0.331972+0.000853241\ttest-mae:0.374785+0.00174788\n",
      "[1000]\ttrain-mae:0.327997+0.00076042\ttest-mae:0.374228+0.00172384\n",
      "[1100]\ttrain-mae:0.324384+0.000653856\ttest-mae:0.373753+0.001728\n",
      "[1200]\ttrain-mae:0.32122+0.000776685\ttest-mae:0.373405+0.00168456\n",
      "[1300]\ttrain-mae:0.318464+0.000740533\ttest-mae:0.373148+0.00166993\n",
      "[1400]\ttrain-mae:0.315853+0.00082116\ttest-mae:0.372949+0.00166819\n",
      "[1500]\ttrain-mae:0.313396+0.000829451\ttest-mae:0.372787+0.00164082\n",
      "[1600]\ttrain-mae:0.31113+0.000880067\ttest-mae:0.37269+0.0016278\n",
      "[1700]\ttrain-mae:0.308997+0.000885187\ttest-mae:0.372592+0.00162041\n",
      "[1800]\ttrain-mae:0.30699+0.00102222\ttest-mae:0.372521+0.00162003\n",
      "[1900]\ttrain-mae:0.304982+0.000981834\ttest-mae:0.372471+0.00161941\n",
      "[2000]\ttrain-mae:0.303116+0.00098786\ttest-mae:0.372429+0.0016163\n",
      "CV time:752.488s\n",
      "CV-Mean: 0.372429+0.00161629561034\n"
     ]
    }
   ],
   "source": [
    "# XGB!\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#my first tries:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_layer2.npy  exists, importing \n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            #gbdt=xgbfit(X_train,y_train)\n",
    "            gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            #MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1200.93\n",
      "length of new row: 5\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "#MAE=np.mean(abs(avg_column - y))\n",
    "MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.61935725,  7.17841596,  7.66262324,  7.62487841,  7.52131872],\n",
       "       [ 7.55698677,  7.54668212,  7.53019273,  7.39799309,  7.50796368],\n",
       "       [ 8.34345942,  8.47645884,  8.29735968,  8.38960838,  8.37672158]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:38.465s\n",
      "length of row: 5\n",
      "length of row: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.004s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:7.22685+0.0019794\ttest-mae:7.22685+0.00597323\n",
      "[100]\ttrain-mae:2.64571+0.000711953\ttest-mae:2.6457+0.00442378\n",
      "[200]\ttrain-mae:0.979653+0.000361954\ttest-mae:0.97979+0.00379201\n",
      "[300]\ttrain-mae:0.47802+0.000437162\ttest-mae:0.479575+0.00256361\n",
      "[400]\ttrain-mae:0.386649+0.000427133\ttest-mae:0.389782+0.00113905\n",
      "[500]\ttrain-mae:0.372868+0.0004486\ttest-mae:0.376904+0.000538678\n",
      "[600]\ttrain-mae:0.370321+0.00043008\ttest-mae:0.375051+0.000439115\n",
      "[700]\ttrain-mae:0.369337+0.000434173\ttest-mae:0.374737+0.000437825\n",
      "[800]\ttrain-mae:0.368644+0.000388037\ttest-mae:0.374594+0.000430539\n",
      "[900]\ttrain-mae:0.368037+0.000359678\ttest-mae:0.374545+0.00045202\n",
      "[1000]\ttrain-mae:0.367445+0.000339081\ttest-mae:0.37452+0.00044562\n",
      "CV time:205.716s\n",
      "CV-Mean: 0.37451375+0.000453683466196\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(x_layer2, label=y)\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1139.11\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1153.92\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1156.03\n",
      "XGB predict time:1.213s\n",
      "AVG Mean abs error: 1143.97\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1134.21\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1153.69\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1147.37\n",
      "XGB predict time:1.104s\n",
      "AVG Mean abs error: 1138.65\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1150.75\n",
      "Score: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:67: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1171.55\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1163.79\n",
      "XGB predict time:1.153s\n",
      "AVG Mean abs error: 1155.78\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1144.31\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1163.38\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1158.08\n",
      "XGB predict time:1.081s\n",
      "AVG Mean abs error: 1149.28\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1127.71\n",
      "Score: 0.58\n",
      "KNeighborsRegressor Mean abs error: 1150.17\n",
      "Score: 0.56\n",
      "XGB Mean abs error: 1141.88\n",
      "XGB predict time:1.176s\n",
      "AVG Mean abs error: 1133.69\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_linear) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    #MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_predict_KNeighbors) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('KNNLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_train, label=y_layer2_train)\n",
    "    dtest = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    \n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "    #MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_gbdt_predict) - np.exp(y_layer2_validation)))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    #MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    MAE=np.mean(abs(np.exp(layer2_avg_predict) - np.exp(y_layer2_validation)))\n",
    "\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "XGB Mean abs error: 1141.59\n",
      "XGB predict time:1.287s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_train, label=y_layer3_train)\n",
    "dtest = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtest)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE=np.mean(abs(np.exp(layer3_gbdt_predict) - np.exp(y_layer3_validation)))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer3'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:XGB'\n",
      "  'run:37663-75326:0' 'run:37663-75326:1' 'run:37663-75326:2'\n",
      "  'run:37663-75326:XGB' 'run:75326-112989:0' 'run:75326-112989:1'\n",
      "  'run:75326-112989:2' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:XGB'\n",
      "  'run:150652-188318:0' 'run:150652-188318:1' 'run:150652-188318:2'\n",
      "  'run:150652-188318:XGB' 'run:linearLayer2' 'run:KNNLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:KNNLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:KNNLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:KNNLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:KNNLayer2'\n",
      "  'run:XGBLayer2' 'run:XGBLayer3']\n",
      " ['1211.52551822' '1290.75202656' '1204.17380297' '1139.80489947'\n",
      "  '1210.48333368' '1287.08197303' '1205.09262736' '1135.0908639'\n",
      "  '1226.90780918' '1307.21195387' '1216.39230893' '1151.37934424'\n",
      "  '1222.2021374' '1313.24320368' '1212.29466026' '1143.70309357'\n",
      "  '1203.54064512' '1282.45345585' '1195.35407268' '1128.71143245'\n",
      "  '1139.11058658' '1153.92259118' '1156.03242354' '1134.20992252'\n",
      "  '1153.68920122' '1147.37009284' '1150.75173598' '1171.55359715'\n",
      "  '1163.78600535' '1144.30841174' '1163.37708117' '1158.08473347'\n",
      "  '1127.706756' '1150.16601015' '1141.87740624' '1141.59129869']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGDCAYAAAAWKgYNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXs8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9cbe4d2910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:130.822s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.727s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:115.123s\n",
      "XGB predict time:274.871s\n",
      "AVG column added - length of new row: 6\n",
      "Fold run time:410.426s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift +1 to account for sampling...\n",
      "kmeans round 2 time:44.695s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clusters sample:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([24, 33, 25,  6, 35, 20, 65,  2, 51, 73, 73, 41,  5, 59, 56], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.55936695,  7.37775   ,  7.63114118,  7.47324896,  7.51037677],\n",
       "       [ 7.77476437,  7.5985671 ,  7.75541898,  7.6608758 ,  7.69740656],\n",
       "       [ 9.06726831,  9.4391458 ,  9.0082327 ,  9.10076046,  9.15385182]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  7.55936695,   7.37775   ,   7.63114118,   7.47324896,\n",
       "          7.51037677,  24.        ],\n",
       "       [  7.77476437,   7.5985671 ,   7.75541898,   7.6608758 ,\n",
       "          7.69740656,  33.        ],\n",
       "       [  9.06726831,   9.4391458 ,   9.0082327 ,   9.10076046,\n",
       "          9.15385182,  25.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n",
      "run time:44.713s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear predict time:0.004s\n",
      "KNeighbors predict time:3.987s\n",
      "XGB predict time:3.944s\n",
      "AVG predict time:0.002s\n"
     ]
    }
   ],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1799.77560881\\n',\n",
       " '6,2199.89920503\\n',\n",
       " '9,8777.35550873\\n',\n",
       " '12,6108.66025994\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spit out that good scoring linear result...\n",
    "test_data['loss']=np.exp(layer3_predict_linear)-200\n",
    "\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_linear.csv\"\n",
    "display(writeData(result,output_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,2097.40454102\\n',\n",
       " '6,2163.87695312\\n',\n",
       " '9,8646.23828125\\n',\n",
       " '12,4329.33837891\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=np.exp(layer3_gbdt.predict(dtest))-200\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1658.669067\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
