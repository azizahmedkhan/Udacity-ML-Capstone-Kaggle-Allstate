{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):  # used the one above to get the # of clusters, using this for speed\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        grid_search.fit(x,y)\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####unused!, but for a reference idea\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "        \n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    combineddata[feat] = combineddata[comb[0]] + combineddata[comb[1]]\n",
    "    #combineddata[feat] = combineddata[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded\n"
     ]
    }
   ],
   "source": [
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "print(\"label encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, using it\n",
      "clusters loaded and attached\n",
      "0    27\n",
      "1    13\n",
      "2     0\n",
      "Name: clusters, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#predict the cluster for each row\n",
    "filename='clusters.npy'\n",
    "if os.path.isfile(filename):\n",
    "    print(\"File found, using it\")\n",
    "    combineddata['clusters']=joblib.load(filename)\n",
    "else:\n",
    "    print(\"no files, running clusters...\")\n",
    "    combineddata['clusters']=kmeansPlusmeanshift(combineddata.drop(['id','loss'],1))\n",
    "    joblib.dump(combineddata['clusters'],filename)\n",
    "print(\"clusters loaded and attached\")\n",
    "print(combineddata.head(3)['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled\n"
     ]
    }
   ],
   "source": [
    "hold_columns=combineddata[['loss','id']] #don't want to mess with these\n",
    "\n",
    "combineddata.drop(['loss','id'],1,inplace=True) \n",
    "columns=combineddata.columns #need these to recreate the DF\n",
    "\n",
    "#scale the columns we see\n",
    "scaler= MinMaxScaler()   \n",
    "values = scaler.fit_transform(combineddata.values)\n",
    "combineddata= pd.DataFrame(values, columns=columns)\n",
    "\n",
    "#put these back on for the moment!\n",
    "combineddata['loss']=hold_columns['loss'].tolist()\n",
    "combineddata['id']=hold_columns['id'].tolist()\n",
    "del hold_columns\n",
    "del values\n",
    "del scaler\n",
    "print \"Data scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "combineddata[\"cont1\"] = np.sqrt(combineddata[\"cont1\"])\n",
    "combineddata[\"cont4\"] = np.sqrt(combineddata[\"cont4\"])\n",
    "combineddata[\"cont5\"] = np.sqrt(combineddata[\"cont5\"])\n",
    "combineddata[\"cont8\"] = np.sqrt(combineddata[\"cont8\"])\n",
    "combineddata[\"cont10\"] = np.sqrt(combineddata[\"cont10\"])\n",
    "combineddata[\"cont11\"] = np.sqrt(combineddata[\"cont11\"])\n",
    "combineddata[\"cont12\"] = np.sqrt(combineddata[\"cont12\"])\n",
    "\n",
    "combineddata[\"cont6\"] = np.log(combineddata[\"cont6\"] + 0000.1)\n",
    "combineddata[\"cont7\"] = np.log(combineddata[\"cont7\"] + 0000.1)\n",
    "combineddata[\"cont9\"] = np.log(combineddata[\"cont9\"] + 0000.1)\n",
    "combineddata[\"cont13\"] = np.log(combineddata[\"cont13\"] + 0000.1)\n",
    "combineddata[\"cont14\"] = (np.maximum(combineddata[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "\n",
       "[3 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 133 entries, cat1 to id\n",
      "dtypes: float64(132), int64(1)\n",
      "memory usage: 191.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>clusters</th>\n",
       "      <th>loss</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>-0.255633</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.761787</td>\n",
       "      <td>-0.070392</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>2213.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>-0.792214</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.585682</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.343682</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>1283.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>-1.016372</td>\n",
       "      <td>0.571049</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>-1.211326</td>\n",
       "      <td>1.018094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3005.09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10 ...     cont8  \\\n",
       "0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   1.0    0.0 ...  0.296826   \n",
       "1   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0    1.0 ...  0.698552   \n",
       "2   0.0   1.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0    1.0 ...  0.220662   \n",
       "\n",
       "      cont9    cont10    cont11    cont12    cont13    cont14  clusters  \\\n",
       "0 -0.255633  0.916140  0.744792  0.761787 -0.070392  0.984628  0.341772   \n",
       "1 -0.792214  0.664384  0.560798  0.585682 -0.330645  0.343682  0.164557   \n",
       "2 -1.016372  0.571049  0.599347  0.591963 -1.211326  1.018094  0.000000   \n",
       "\n",
       "      loss  id  \n",
       "0  2213.18   1  \n",
       "1  1283.60   2  \n",
       "2  3005.09   5  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "display(data.info())\n",
    "display(data.head(3))\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "x_test_data=test_data.drop(['loss','id'],1) .values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "\n",
    "# we don't want the ID columns in X\n",
    "x=data.drop(['id','loss'],1).values\n",
    "# loss is our label\n",
    "y=data['loss'].values\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "display(data.info())\n",
    "display(data.head(3))\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del data,test_data\n",
    "#del combineddata\n",
    "#del scaler\n",
    "#del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 7, 10, 25, 50, 500]}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[5,7,10,25,50,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,7,10,25,50,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                               test_size=0.20,\n",
    "                                                                random_state=42)\n",
    "display(\"sample train data size:{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr3.pkl  exists, importing \n",
      "Full GridSearch run time:0.004s\n"
     ]
    }
   ],
   "source": [
    "start_time0 = time.time()\n",
    "for i in range(len(regrList)):\n",
    "    regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:2973.21+6.6702\ttest-mae:2973.19+20.3854\n",
      "[100]\ttrain-mae:1381.55+5.72146\ttest-mae:1438.97+19.3135\n",
      "[200]\ttrain-mae:1089.28+9.5155\ttest-mae:1242.44+13.0097\n",
      "[300]\ttrain-mae:997.453+12.0014\ttest-mae:1220.87+10.3515\n",
      "[400]\ttrain-mae:942.865+14.256\ttest-mae:1215.23+9.68692\n",
      "[500]\ttrain-mae:902.93+17.8892\ttest-mae:1211.54+9.5774\n",
      "[600]\ttrain-mae:871.755+22.5303\ttest-mae:1208.26+9.84651\n",
      "[700]\ttrain-mae:846.302+26.5719\ttest-mae:1205.44+10.2789\n",
      "[800]\ttrain-mae:825.859+29.2546\ttest-mae:1203.33+10.3927\n",
      "[900]\ttrain-mae:808.848+31.4377\ttest-mae:1202.29+10.9249\n",
      "[1000]\ttrain-mae:794.599+32.3203\ttest-mae:1201.58+11.2083\n",
      "[1100]\ttrain-mae:782.145+33.4889\ttest-mae:1201.37+11.5221\n",
      "[1200]\ttrain-mae:770.373+34.308\ttest-mae:1201.04+11.6488\n",
      "[1300]\ttrain-mae:758.829+34.6796\ttest-mae:1201+11.8179\n",
      "CV time:119.053s\n",
      "CV-Mean: 1200.9609375+11.800158931\n"
     ]
    }
   ],
   "source": [
    "# XGB!\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "#my first tries:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:85.495s\n",
      "Mean abs error: 1242.84\n",
      "-predict time:4.383s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.92s\n",
      "Mean abs error: 1337.12\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:75.618s\n",
      "Mean abs error: 1235.50\n",
      "-predict time:3.977s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:10.053s\n",
      "Mean abs error: 1311.75\n",
      "-predict time:439.627s\n",
      "XGB Mean abs error: 1179.12\n",
      "-XGB predict time:1.722s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "---Fold run time:788.558s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:87.125s\n",
      "Mean abs error: 1230.23\n",
      "-predict time:4.466s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.835s\n",
      "Mean abs error: 1326.11\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:76.722s\n",
      "Mean abs error: 1224.24\n",
      "-predict time:3.995s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:9.887s\n",
      "Mean abs error: 1307.52\n",
      "-predict time:462.007s\n",
      "XGB Mean abs error: 1165.11\n",
      "-XGB predict time:1.656s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 5)\n",
      "---Fold run time:813.021s\n",
      "---Fold:75326 to 112989 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:56: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:85.532s\n",
      "Mean abs error: 1244.71\n",
      "-predict time:4.398s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.853s\n",
      "Mean abs error: 1334.23\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:76.124s\n",
      "Mean abs error: 1236.16\n",
      "-predict time:3.993s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:10.327s\n",
      "Mean abs error: 1319.78\n",
      "-predict time:423.999s\n",
      "XGB Mean abs error: 1179.95\n",
      "-XGB predict time:1.75s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 5)\n",
      "---Fold run time:773.169s\n",
      "---Fold:112989 to 150652 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:88.825s\n",
      "Mean abs error: 1245.95\n",
      "-predict time:4.468s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.807s\n",
      "Mean abs error: 1340.40\n",
      "-predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:78.501s\n",
      "Mean abs error: 1240.03\n",
      "-predict time:4.101s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:9.875s\n",
      "Mean abs error: 1320.62\n",
      "-predict time:420.802s\n",
      "XGB Mean abs error: 1180.38\n",
      "-XGB predict time:1.677s\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 5)\n",
      "---Fold run time:775.103s\n",
      "---Fold:150652 to 188318 of: 188318\n",
      "\n",
      "---folding! len test 37666, len train 150652\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:87.733s\n",
      "Mean abs error: 1229.37\n",
      "-predict time:4.354s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.964s\n",
      "Mean abs error: 1328.74\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:79.441s\n",
      "Mean abs error: 1224.96\n",
      "-predict time:3.971s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:9.749s\n",
      "Mean abs error: 1302.20\n",
      "-predict time:426.45s\n",
      "XGB Mean abs error: 1165.41\n",
      "-XGB predict time:1.663s\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 5)\n",
      "---Fold run time:779.171s\n",
      "----Full run time:3929.023s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "if os.path.isfile('x_layer2.npy'):\n",
    "    print 'x_layer2.npy',\" exists, importing \"\n",
    "    #reuse the run\n",
    "    x_layer2=joblib.load('x_layer2.npy') \n",
    "    MAE_tracking=joblib.load('MAE_tracking.npy')\n",
    "else:\n",
    "    for fold_start,fold_end in folds:\n",
    "        print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "        start_time1 = time.time()\n",
    "        fold_result=[]\n",
    "\n",
    "        X_test = x[fold_start:fold_end].copy()\n",
    "        y_test = y[fold_start:fold_end].copy()\n",
    "        X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "        y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "        print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "        for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "            print(regrList[i])\n",
    "            start_time = time.time()\n",
    "            estimator=skclone(regrList[i], safe=True)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            start_time = time.time()\n",
    "            curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "            if fold_result == []:\n",
    "                fold_result = curr_predict\n",
    "            else:\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "            #show some stats on that last regressions run\n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "            print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "        #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "        if use_xgb == True:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            #gbdt=xgbfit(X_train,y_train)\n",
    "            gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "            # now do a prediction and spit out a score(MAE) that means something\n",
    "            start_time = time.time()\n",
    "            curr_predict=gbdt.predict(dtest)\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "            MAE=np.mean(abs(curr_predict - y_test))\n",
    "            MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "            print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "            print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        if x_layer2 == []:\n",
    "            x_layer2=fold_result\n",
    "        else:\n",
    "            x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "        print \"--layer2 length:\",len(x_layer2)\n",
    "        print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "        print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "    print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "    #preserve the run\n",
    "    joblib.dump(x_layer2,'x_layer2.npy') \n",
    "    joblib.dump(MAE_tracking,'MAE_tracking.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgd Mean abs error: 1210.13\n",
      "length of new row: 6\n"
     ]
    }
   ],
   "source": [
    "# add an avged column of all the runs\n",
    "\n",
    "avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "MAE=np.mean(abs(avg_column - y))\n",
    "print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "print(\"length of new row: {}\".format(len(x_layer2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-first 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2265.50284   ,   881.87145066,  2295.51764   ,  1986.33866667,\n",
       "         2084.86572266,  1902.819264  ],\n",
       "       [ 2081.16974   ,  2540.24239237,  1957.11594   ,  2854.52066667,\n",
       "         2041.30200195,  2294.8701482 ],\n",
       "       [ 4555.3641    ,  5184.9735887 ,  4381.52288   ,  4810.44866667,\n",
       "         4585.54003906,  4703.56985489]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of row: 6\n"
     ]
    }
   ],
   "source": [
    "display(\"test-first 3\",x_layer2[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put each in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:45.611s\n",
      "length of row: 6\n",
      "length of row: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_layer2_w_clusters.npy', 'x_layer2_w_clusters.npy_01.npy']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "      \n",
    "x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "joblib.dump(x_layer2,'x_layer2_w_clusters.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_layer2=joblib.load('x_layer2_w_clusters.npy') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_L2_Lin.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_L2_KNN.pkl  exists, importing \n",
      "Full GridSearch run time:0.002s\n"
     ]
    }
   ],
   "source": [
    "# grid search on layer 2\n",
    "\n",
    "        \n",
    "start_time0 = time.time()\n",
    "\n",
    "paramater_grid_Lin=dict(normalize = [True,False])\n",
    "layer2_Lin_regr=grid_search_wrapper(x_layer2,y,LinearRegression(),paramater_grid_Lin,regr_name='L2_Lin')   \n",
    "\n",
    "paramater_grid_KNN=dict(n_neighbors=[2,5,7,15,30],\n",
    "                    leaf_size =[3,10,15,25,30,50,100])\n",
    "layer2_KNN_regr=grid_search_wrapper(x_layer2,y,KNeighborsRegressor(n_jobs = -1),paramater_grid_KNN,regr_name='L2_KNN')   \n",
    "    \n",
    "print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_layer2, label=y)\n",
    "\n",
    "start_time = time.time()\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=30, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(layer2_Lin_regr)\n",
    "display(layer2_KNN_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1178.59\n",
      "Score: 0.57\n",
      "KNeighborsRegressor Mean abs error: 1196.11\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1184.15\n",
      "XGB predict time:1.406s\n",
      "AVG Mean abs error: 1178.89\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1161.32\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1181.85\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1171.54\n",
      "XGB predict time:1.334s\n",
      "AVG Mean abs error: 1164.77\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1176.40\n",
      "Score: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:63: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor Mean abs error: 1195.16\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1183.44\n",
      "XGB predict time:1.422s\n",
      "AVG Mean abs error: 1178.31\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "LinearRegression Mean abs error: 1177.01\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1198.92\n",
      "Score: 0.57\n",
      "XGB Mean abs error: 1185.92\n",
      "XGB predict time:1.319s\n",
      "AVG Mean abs error: 1180.78\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "LinearRegression Mean abs error: 1161.39\n",
      "Score: 0.59\n",
      "KNeighborsRegressor Mean abs error: 1184.12\n",
      "Score: 0.58\n",
      "XGB Mean abs error: 1172.19\n",
      "XGB predict time:1.369s\n",
      "AVG Mean abs error: 1165.86\n"
     ]
    }
   ],
   "source": [
    "x_layer3 = []\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_layer2_validation = x_layer2[fold_start:fold_end].copy()\n",
    "    y_layer2_validation = y[fold_start:fold_end].copy()\n",
    "    X_layer2_train=np.concatenate((x_layer2[:fold_start], x_layer2[fold_end:]), axis=0)\n",
    "    y_layer2_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_layer2_validation),len(X_layer2_train))\n",
    "    \n",
    "\n",
    "    layer2_Lin_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_linear=layer2_Lin_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_linear - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"LinearRegression Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_Lin_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = layer2_predict_linear\n",
    "    #with LinearReg: Mean abs error: 1172.67\n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    layer2_KNN_regr.fit(X_layer2_train,y_layer2_train)\n",
    "    layer2_predict_KNeighbors=layer2_KNN_regr.predict(X_layer2_validation)\n",
    "    #show some stats on that last regressions run    \n",
    "    MAE=np.mean(abs(layer2_predict_KNeighbors - y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "    print(\"KNeighborsRegressor Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"Score: {:.2f}\".format(layer2_KNN_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "    fold_result = np.column_stack((fold_result,layer2_predict_KNeighbors))  \n",
    "\n",
    "    #Mean abs error: 1291.64\n",
    "\n",
    "    # The XGB version of layer 2\n",
    "    dtrain = xgb.DMatrix(X_layer2_train, label=y_layer2_train)\n",
    "    dtest = xgb.DMatrix(X_layer2_validation)\n",
    "    layer2_gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    \n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    start_time = time.time()\n",
    "    layer2_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "    MAE=np.mean(abs(layer2_gbdt_predict- y_layer2_validation))\n",
    "    MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "    print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "    fold_result = np.column_stack((fold_result,layer2_gbdt_predict))  \n",
    "    \n",
    "    #XGB Mean abs error: 1154.25\n",
    "    \n",
    "    # ? average those weighted to XGB\n",
    "    layer2_avg_predict=(layer2_predict_linear+layer2_predict_KNeighbors+layer2_gbdt_predict+layer2_gbdt_predict)/4\n",
    "\n",
    "    MAE=np.mean(abs(layer2_avg_predict- y_layer2_validation))\n",
    "    print(\"AVG Mean abs error: {:.2f}\".format(MAE))\n",
    "    fold_result = np.column_stack((fold_result,layer2_avg_predict))  \n",
    "\n",
    "    #AVG Mean abs error: 1163.71\n",
    "    \n",
    "    if x_layer3 == []:\n",
    "        x_layer3=fold_result\n",
    "    else:\n",
    "        x_layer3=np.append(x_layer3,fold_result,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  train/validation split\n",
    "X_layer3_train, X_layer3_validation, y_layer3_train, y_layer3_validation = train_test_split( x_layer3,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "XGB Mean abs error: 1155.98\n",
      "XGB predict time:0.358s\n"
     ]
    }
   ],
   "source": [
    "# The XGB layer3?\n",
    "print len(x_layer3)\n",
    "print len(y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_layer3_train, label=y_layer3_train)\n",
    "dtest = xgb.DMatrix(X_layer3_validation)\n",
    "layer3_gbdt==xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer3_gbdt.predict(dtest)\n",
    "MAE=np.mean(abs(layer3_gbdt_predict- y_layer3_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#XGB Mean abs error: 1152.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:XGB' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:XGB'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:XGB' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:XGB' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:XGB'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2'\n",
      "  'run:linearLayer2' 'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2' 'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2'\n",
      "  'run:linearLayer2' 'run:linearLayer2' 'run:XGBLayer2' 'run:XGBLayer2']\n",
      " ['1242.84379646' '1337.11944058' '1235.50402739' '1311.74796396'\n",
      "  '1179.12496636' '1230.23493105' '1326.11306781' '1224.23808242'\n",
      "  '1307.51585151' '1165.11105907' '1244.70680597' '1334.22559827'\n",
      "  '1236.16122191' '1319.77523175' '1179.95175806' '1245.95209964'\n",
      "  '1340.40499486' '1240.02598087' '1320.61588521' '1180.38372527'\n",
      "  '1229.36660094' '1328.73756119' '1224.96004848' '1302.20116698'\n",
      "  '1165.40673209' '1178.59299538' '1196.10617099' '1184.15434071'\n",
      "  '1161.31817246' '1181.84982537' '1171.54039154' '1176.39701007'\n",
      "  '1195.16129927' '1183.43818545' '1177.01175955' '1198.92359475'\n",
      "  '1185.92435776' '1161.38779464' '1184.11625131' '1172.19351861'\n",
      "  '1155.98296461']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAGPCAYAAABCs5ejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6844f21f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:139.362s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:4.907s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:121.891s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-0bcdc8fe3051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# use the estimator from the training, but refit to the whole data set!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcurr_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict time:{}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/regression.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    397\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[0;32m    398\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 399\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m             )\n\u001b[0;32m    401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[1;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()            \n",
    "    estimator=skclone(regrList[i], safe=True)\n",
    "    print(estimator)\n",
    "    estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "    curr_predict=estimator.predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "# add an avged column of all the runs\n",
    "avg_column=np.mean(x_layer2_test, axis=1)\n",
    "x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### This section is outdated, but still usefull in a historical sense.\n",
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# use the new clusters number to predict each locations cluster\n",
    "print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "display(\"Clusters sample:\",final_clusters[:15])\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "\n",
    "x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "\n",
    "display(\"test-first 3\",x_layer2_test[:3])\n",
    "print(\"length of row: {}\".format(len(x_layer2_test[0])))\n",
    "print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Linear\n",
    "start_time = time.time()\n",
    "layer3_predict_linear=layer2_Lin_regr.predict(x_layer2_test)\n",
    "print(\"Linear predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = layer3_predict_linear\n",
    "\n",
    "#KNeighborsRegressor\n",
    "start_time = time.time()\n",
    "layer3_predict_KNeighbors=layer2_KNN_regr.predict(x_layer2_test)\n",
    "print(\"KNeighbors predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_predict_KNeighbors))  \n",
    "\n",
    "\n",
    "# The XGB version of layer 2\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "start_time = time.time()\n",
    "layer3_gbdt_predict=layer2_gbdt.predict(dtest)\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_gbdt_predict))  \n",
    "\n",
    "\n",
    "# ? average those weighted to XGB\n",
    "start_time = time.time()\n",
    "\n",
    "layer3_avg_predict=(layer3_predict_linear+layer3_predict_KNeighbors+layer3_gbdt_predict+layer3_gbdt_predict)/4\n",
    "print(\"AVG predict time:{}s\".format(round((time.time()-start_time), 3) ))    \n",
    "\n",
    "x_layer3_test = np.column_stack((x_layer3_test,layer3_avg_predict))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer3_test)\n",
    "test_data['loss']=layer3_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
