{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Model - Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "test_data_all_features.csv\n",
      "test_data_cats.csv\n",
      "test_data_conts.csv\n",
      "test_data_new.csv\n",
      "test_data_orig_only.csv\n",
      "train.csv\n",
      "train.csv.zip\n",
      "train_data_all_features.csv\n",
      "train_data_cats.csv\n",
      "train_data_conts.csv\n",
      "train_data_new.csv\n",
      "train_data_orig_only.csv\n",
      "\n",
      "clusters_cat.npy\n",
      "clusters_cat.npy_01.npy\n",
      "clusters_cat.npy_02.npy\n",
      "clusters_cont.npy\n",
      "clusters_cont.npy_01.npy\n",
      "clusters_cont.npy_02.npy\n",
      "clusters.npy\n",
      "clusters.npy_01.npy\n",
      "clusters.npy_02.npy\n",
      "grid_regr0-cats.pkl\n",
      "grid_regr0-conts.pkl\n",
      "grid_regr0-new.pkl\n",
      "grid_regr0-orig_only.pkl\n",
      "grid_regr1-cats.pkl\n",
      "grid_regr1-conts.pkl\n",
      "grid_regr1-new.pkl\n",
      "grid_regr1-orig_only.pkl\n",
      "grid_regr2-cats.pkl\n",
      "grid_regr2-conts.pkl\n",
      "grid_regr2-new.pkl\n",
      "MAE_tracking-all_features.npy\n",
      "MAE_tracking-cats.npy\n",
      "MAE_tracking-conts.npy\n",
      "MAE_tracking-new.npy\n",
      "MAE_tracking.npy\n",
      "MAE_tracking-orig_only.npy\n",
      "oldmodels\n",
      "stat_tracking-Linear.npy\n",
      "x_layer2all_features.npy\n",
      "x_layer2all_features.npy_01.npy\n",
      "x_layer2cats.npy\n",
      "x_layer2cats.npy_01.npy\n",
      "x_layer2conts.npy\n",
      "x_layer2conts.npy_01.npy\n",
      "x_layer2new.npy\n",
      "x_layer2new.npy_01.npy\n",
      "x_layer2.npy\n",
      "x_layer2.npy_01.npy\n",
      "x_layer2orig_only.npy\n",
      "x_layer2orig_only.npy_01.npy\n",
      "x_layer2_test_cats.npy\n",
      "x_layer2_test_cats.npy_01.npy\n",
      "x_layer2_test_conts.npy\n",
      "x_layer2_test_conts.npy_01.npy\n",
      "x_layer2_test_new_.npy\n",
      "x_layer2_test_new.npy\n",
      "x_layer2_test_new_.npy_01.npy\n",
      "x_layer2_test_new.npy_01.npy\n",
      "x_layer2_train_cats.npy\n",
      "x_layer2_train_cats.npy_01.npy\n",
      "x_layer2_train_conts.npy\n",
      "x_layer2_train_conts.npy_01.npy\n",
      "x_layer2_train_new.npy\n",
      "x_layer2_train-new.npy\n",
      "x_layer2_train_new.npy_01.npy\n",
      "x_layer2_train-new.npy_01.npy\n",
      "x_layer2_w_clusters-new.npy_01.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "#from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "cachedir=\"./cache/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", cachedir]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGB params:\n",
    "\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepdata(data_name,verbose=False):\n",
    "    ### and now, let's import the data\n",
    "    data = loadData(datadir,'train_data_'+data_name+'.csv')\n",
    "    if verbose==True:\n",
    "        display(data.info())\n",
    "        display(data.head(2))\n",
    "\n",
    "    test_data= loadData(datadir,'test_data_'+data_name+'.csv') \n",
    "    if verbose==True:\n",
    "        display(test_data.info())\n",
    "        display(test_data.head(2))\n",
    "    # we don't want the ID columns in X\n",
    "    x=data.drop(['id','loss'],1).values\n",
    "    # loss is our label\n",
    "    #y=data['loss'].values\n",
    "    y = np.log(data['loss']+shift).ravel()\n",
    "\n",
    "    return x,y,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(cachedir+filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(cachedir+filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs=-1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        print(\"debug 1\")\n",
    "        grid_search.fit(x,y)\n",
    "        print \"debug2\"\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,cachedir+filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the planned regressions for layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [3, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {'alpha': [0.05, 0.5, 1, 2, 4, 40, 140, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 7, 10, 25, 50, 200, 500]}]\n",
      "('number of scikitlearn regressors to use:', 3)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "#regrList.append([LinearRegression(), dict()]),\n",
    "#regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                       # dict(n_neighbors=[2,5,7,15],\n",
    "                             #leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_layer1_for_dataset(x,y,dataset):\n",
    "    #  train/validation split\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                    y,\n",
    "                                                                   test_size=0.20,\n",
    "                                                                    random_state=42)\n",
    "    display(\"sample train data: {}: size:{}\".format(dataset, len( y_train)))\n",
    "    start_time0 = time.time()\n",
    "    \n",
    "    for i in range(len(regrList)):\n",
    "        regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}-{}\".format(i,dataset))\n",
    "\n",
    "    print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "    # XGB CV\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    start_time = time.time()\n",
    "    #res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "    #             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "   \n",
    "    #best_nrounds = res.shape[0] - 1\n",
    "    #cv_mean = res.iloc[-1, 0]\n",
    "    #cv_std = res.iloc[-1, 1]\n",
    "    #print('CV-Mean: {0}+{1}, best rounds:{2}'.format(cv_mean, cv_std,best_nrounds))\n",
    "    best_nrounds=2000 # various tests, this always resulted, not going to run due to speed\n",
    "    #prepare the fold divisions\n",
    "\n",
    "    data_size=x.shape[0]\n",
    "    print \"size of train data:\",data_size\n",
    "    folds=[]\n",
    "    num_folds=5\n",
    "    fold_start=0\n",
    "    for k in range(num_folds-1):\n",
    "        fold_end=((data_size/num_folds)*(k+1))\n",
    "        folds.append((fold_start,fold_end))\n",
    "        fold_start=fold_end\n",
    "    folds.append((fold_start,data_size))\n",
    "    print \"folds at:\",folds\n",
    "    print \"fold size:\", (data_size/num_folds)\n",
    "    print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "    count=0\n",
    "    for i in folds:\n",
    "        count+=i[1]-i[0]\n",
    "    print count\n",
    "    \n",
    "    x_layer2=[]\n",
    "    start_time0 = time.time()\n",
    "    MAE_tracking=[]\n",
    "\n",
    "    if os.path.isfile(cachedir+'x_layer2'+dataset+'.npy'):\n",
    "        print 'x_layer2'+dataset+'.npy',\" exists, importing \"\n",
    "        #reuse the run\n",
    "        x_layer2=joblib.load(cachedir+'x_layer2'+dataset+'.npy') \n",
    "        MAE_tracking=joblib.load(cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "    else:\n",
    "        for fold_start,fold_end in folds:\n",
    "            print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "            start_time1 = time.time()\n",
    "            fold_result=[]\n",
    "\n",
    "            X_test = x[fold_start:fold_end].copy()\n",
    "            y_test = y[fold_start:fold_end].copy()\n",
    "            X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "            y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "            print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "            for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "                print(regrList[i])\n",
    "                start_time = time.time()\n",
    "                estimator=skclone(regrList[i], safe=True)\n",
    "                estimator.fit(X_train,y_train)\n",
    "                print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                start_time = time.time()\n",
    "                curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "                if fold_result == []:\n",
    "                    fold_result = curr_predict\n",
    "                else:\n",
    "                    fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "                #show some stats on that last regressions run\n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "                print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "            #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "            if use_xgb == True:\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dtest = xgb.DMatrix(X_test)\n",
    "                #gbdt=xgbfit(X_train,y_train)\n",
    "                gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "                # now do a prediction and spit out a score(MAE) that means something\n",
    "                start_time = time.time()\n",
    "                curr_predict=gbdt.predict(dtest)\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "                print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            if x_layer2 == []:\n",
    "                x_layer2=fold_result\n",
    "            else:\n",
    "                x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "            print \"--layer2 length:\",len(x_layer2)\n",
    "            print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "            print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "        print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "        #preserve the run\n",
    "        joblib.dump(x_layer2,cachedir+'x_layer2'+dataset+'.npy') \n",
    "        joblib.dump(MAE_tracking,cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "\n",
    "    avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "    #MAE=np.mean(abs(avg_column - y))\n",
    "    MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "    print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "    x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "    print(\"length of new row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(x_layer2)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "    x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2,cachedir+'x_layer2_train_{}.npy'.format(dataset)) \n",
    "    return regrList,xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,dataset):  \n",
    "    print \"predicting for dataset {}\".format(dataset)\n",
    "    x_layer2_test = []\n",
    "    start_time1 = time.time()\n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()            \n",
    "        estimator=skclone(regrList[i], safe=True)\n",
    "        print(estimator)\n",
    "        estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "        curr_predict=estimator.predict(x_test_data)\n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        if x_layer2_test == []:\n",
    "            x_layer2_test = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    best_nrounds=2000\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "    avg_column=np.mean(x_layer2_test, axis=1)\n",
    "    x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "    print(\"AVG column added - length of new row: {}\".format(len(x_layer2_test[0])))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "    \n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) )) \n",
    "\n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2_test,cachedir+'x_layer2_test_{}.npy'.format(dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Starting dataset:new for layer1-----------\n",
      "loading: ./input/train_data_new.csv\n",
      "Dataset has 188318 samples with 5 features each.\n",
      "loading: ./input/test_data_new.csv\n",
      "Dataset has 125546 samples with 5 features each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample train data: new: size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0-new.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1-new.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2-new.pkl  exists, importing \n",
      "Full GridSearch run time:0.003s\n",
      "CV time:0.0s\n",
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n",
      "x_layer2new.npy  exists, importing \n",
      "avgd Mean abs error: 1578.28\n",
      "length of new row: 5\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:6.859s\n",
      "length of row: 5\n",
      "length of row: 6\n",
      "predicting for dataset new\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=7, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.263s\n",
      "Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:10.727s\n",
      "XGB predict time:87.563s\n",
      "AVG column added - length of new row: 5\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:4.373s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'x_layer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f94423821484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mregrList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_layer1_for_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpredict_layer1_for_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregrList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Layer 1 done for all data sets!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-bbb0d313d7bc>\u001b[0m in \u001b[0;36mpredict_layer1_for_dataset\u001b[1;34m(x, y, x_test_data, regrList, xgb, dataset)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mfinal_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_means\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_layer2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kmeans round 2 time:{}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mx_layer2_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_layer2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fold run time:{}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'x_layer2' is not defined"
     ]
    }
   ],
   "source": [
    "shift=200\n",
    "\n",
    "data_sets=['new','conts','cats','orig_only','all_features']\n",
    "\n",
    "for data_name in data_sets:\n",
    "    print \"-----------Starting dataset:{} for layer1-----------\".format(data_name)\n",
    "    x,y,test_data=prepdata(data_name)\n",
    "    x_test_data=test_data.drop(['loss','id'],1).values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "    regrList,xgb=train_layer1_for_dataset(x,y,data_name)\n",
    "\n",
    "    predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,data_name) \n",
    "\n",
    "print \"Layer 1 done for all data sets!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the layer two stack\n",
    "x_layer2_train_final = None\n",
    "\n",
    "for data_name in data_sets[:]:\n",
    "    print \"-----------Starting {} for layer1-----------\".format(data_name)\n",
    "    \n",
    "    \n",
    "    x_layer2_train=joblib.load(cachedir+'x_layer2_train_{}.npy'.format(data_name)) \n",
    "    x_layer2_test=joblib.load(cachedir+'x_layer2_test_{}.npy'.format(data_name)) \n",
    "    print(\"Train:\",x_layer2_train.shape)\n",
    "    print(\"Test:\",x_layer2_test.shape)\n",
    "    if x_layer2_train_final is None:\n",
    "        x_layer2_train_final=x_layer2_train\n",
    "        x_layer2_test_final=x_layer2_test\n",
    "    else:\n",
    "        x_layer2_train_final = np.column_stack((x_layer2_train_final,x_layer2_train))\n",
    "        x_layer2_test_final = np.column_stack((x_layer2_test_final,x_layer2_test))\n",
    "\n",
    "joblib.dump(x_layer2_train_final,cachedir+'x_layer2_train_final.npy') \n",
    "joblib.dump(x_layer2_test_final,cachedir+'x_layer2_test_final.npy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train:\",x_layer2_train_final.shape)\n",
    "print(\"Test:\",x_layer2_test_final.shape)\n",
    "# output should be: (same,sum inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's get an idea of the layer 1 results.\n",
    "for data_name in data_sets[:]:\n",
    "    print \"-----------Starting graphs for {} in layer1-----------\".format(data_name)\n",
    "    MAE_tracking_graph=np.array(joblib.load(cachedir+'MAE_tracking-{}.npy'.format(data_name))) \n",
    "    print(MAE_tracking_graph.T)\n",
    "\n",
    "    plt.plot(MAE_tracking_graph.T[1])\n",
    "    plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
