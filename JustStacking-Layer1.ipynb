{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Model - Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "test_data_all_features.csv\n",
      "test_data_cats.csv\n",
      "test_data_conts.csv\n",
      "test_data_new.csv\n",
      "test_data_orig_only.csv\n",
      "train.csv\n",
      "train.csv.zip\n",
      "train_data_all_features.csv\n",
      "train_data_cats.csv\n",
      "train_data_conts.csv\n",
      "train_data_new.csv\n",
      "train_data_orig_only.csv\n",
      "\n",
      "clusters_cat.npy\n",
      "clusters_cat.npy_01.npy\n",
      "clusters_cat.npy_02.npy\n",
      "clusters_cont.npy\n",
      "clusters_cont.npy_01.npy\n",
      "clusters_cont.npy_02.npy\n",
      "clusters.npy\n",
      "clusters.npy_01.npy\n",
      "clusters.npy_02.npy\n",
      "grid_L2_KNN.pkl\n",
      "grid_L2_Lin.pkl\n",
      "grid_regr0.pkl\n",
      "grid_regr1.pkl\n",
      "grid_regr2.pkl\n",
      "grid_regr3.pkl\n",
      "MAE_tracking.npy\n",
      "oldmodels\n",
      "stat_tracking-Linear.npy\n",
      "x_layer2.npy\n",
      "x_layer2.npy_01.npy\n",
      "x_layer2_w_clusters.npy\n",
      "x_layer2_w_clusters.npy_01.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "#from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "cachedir=\"./cache/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", cachedir]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGB params:\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepdata(data_name,verbose=False):\n",
    "    ### and now, let's import the data\n",
    "    data = loadData(datadir,'train_data_'+data_name+'.csv')\n",
    "    if verbose==True:\n",
    "        display(data.info())\n",
    "        display(data.head(2))\n",
    "\n",
    "    test_data= loadData(datadir,'test_data_'+data_name+'.csv') \n",
    "    if verbose==True:\n",
    "        display(test_data.info())\n",
    "        display(test_data.head(2))\n",
    "    # we don't want the ID columns in X\n",
    "    x=data.drop(['id','loss'],1).values\n",
    "    # loss is our label\n",
    "    #y=data['loss'].values\n",
    "    y = np.log(data['loss']+shift).ravel()\n",
    "\n",
    "    return x,y,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(cachedir+filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(cachedir+filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs= -1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        print(\"debug 1\")\n",
    "        grid_search.fit(x,y)\n",
    "        print \"debug2\"\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,cachedir+filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the preprocesed datasets in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train_data_all_features.csv\n",
      "Dataset has 188318 samples with 135 features each.\n",
      "loading: ./input/test_data_all_features.csv\n",
      "Dataset has 125546 samples with 135 features each.\n"
     ]
    }
   ],
   "source": [
    "shift=200\n",
    "\n",
    "data_name='all_features'\n",
    "x,y,test_data=prepdata(data_name)\n",
    "x_test_data=test_data.drop(['loss','id'],1).values# didn't have the loss column before, make it go away! don't need ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the planned regressions for layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [3, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {'alpha': [0.05, 0.5, 1, 2, 4, 40, 140, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 7, 10, 25, 50, 200, 500]}]\n",
      "('number of scikitlearn regressors to use:', 3)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "#regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                       # dict(n_neighbors=[2,5,7,15],\n",
    "                             #leaf_size =[3,10,15,25,30,50,100])])\n",
    "#regrList.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_layer1_for_dataset(x,y,dataset):\n",
    "    #  train/validation split\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                    y,\n",
    "                                                                   test_size=0.20,\n",
    "                                                                    random_state=42)\n",
    "    display(\"sample train data size:{}\".format(len(y_train)))\n",
    "    start_time0 = time.time()\n",
    "    \n",
    "    for i in range(len(regrList)):\n",
    "        regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}\".format(i))\n",
    "\n",
    "    print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "    # XGB CV\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    start_time = time.time()\n",
    "    #res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "    #             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "   \n",
    "    #best_nrounds = res.shape[0] - 1\n",
    "    #cv_mean = res.iloc[-1, 0]\n",
    "    #cv_std = res.iloc[-1, 1]\n",
    "    #print('CV-Mean: {0}+{1}, best rounds:{2}'.format(cv_mean, cv_std,best_nrounds))\n",
    "    best_nrounds=2000\n",
    "    #prepare the fold divisions\n",
    "\n",
    "    data_size=x.shape[0]\n",
    "    print \"size of train data:\",data_size\n",
    "    folds=[]\n",
    "    num_folds=5\n",
    "    fold_start=0\n",
    "    for k in range(num_folds-1):\n",
    "        fold_end=((data_size/num_folds)*(k+1))\n",
    "        folds.append((fold_start,fold_end))\n",
    "        fold_start=fold_end\n",
    "    folds.append((fold_start,data_size))\n",
    "    print \"folds at:\",folds\n",
    "    print \"fold size:\", (data_size/num_folds)\n",
    "    print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "    count=0\n",
    "    for i in folds:\n",
    "        count+=i[1]-i[0]\n",
    "    print count\n",
    "    \n",
    "    x_layer2=[]\n",
    "    start_time0 = time.time()\n",
    "    MAE_tracking=[]\n",
    "\n",
    "    if os.path.isfile(cachedir+'x_layer2'+dataset+'.npy'):\n",
    "        print 'x_layer2'+dataset+'.npy',\" exists, importing \"\n",
    "        #reuse the run\n",
    "        x_layer2=joblib.load(cachedir+'x_layer2'+dataset+'.npy') \n",
    "        MAE_tracking=joblib.load(cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "    else:\n",
    "        for fold_start,fold_end in folds:\n",
    "            print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "            start_time1 = time.time()\n",
    "            fold_result=[]\n",
    "\n",
    "            X_test = x[fold_start:fold_end].copy()\n",
    "            y_test = y[fold_start:fold_end].copy()\n",
    "            X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "            y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "            print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "            for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "                print(regrList[i])\n",
    "                start_time = time.time()\n",
    "                estimator=skclone(regrList[i], safe=True)\n",
    "                estimator.fit(X_train,y_train)\n",
    "                print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                start_time = time.time()\n",
    "                curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "                if fold_result == []:\n",
    "                    fold_result = curr_predict\n",
    "                else:\n",
    "                    fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "                #show some stats on that last regressions run\n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "                print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "            #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "            if use_xgb == True:\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dtest = xgb.DMatrix(X_test)\n",
    "                #gbdt=xgbfit(X_train,y_train)\n",
    "                gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "                # now do a prediction and spit out a score(MAE) that means something\n",
    "                start_time = time.time()\n",
    "                curr_predict=gbdt.predict(dtest)\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "                print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            if x_layer2 == []:\n",
    "                x_layer2=fold_result\n",
    "            else:\n",
    "                x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "            print \"--layer2 length:\",len(x_layer2)\n",
    "            print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "            print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "        print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "        #preserve the run\n",
    "        joblib.dump(x_layer2,cachedir+'x_layer2'+dataset+'.npy') \n",
    "        joblib.dump(MAE_tracking,cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "\n",
    "    avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "    #MAE=np.mean(abs(avg_column - y))\n",
    "    MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "    print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "    x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "    print(\"length of new row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(x_layer2)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "    x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2,cachedir+'x_layer2_w_clusters-'+dataset+'.npy') \n",
    "    return regrList,xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,dataset):  \n",
    "    x_layer2_test = []\n",
    "    start_time1 = time.time()\n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()            \n",
    "        estimator=skclone(regrList[i], safe=True)\n",
    "        print(estimator)\n",
    "        estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "        curr_predict=estimator.predict(x_test_data)\n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        if x_layer2_test == []:\n",
    "            x_layer2_test = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    best_nrounds=2000\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "    avg_column=np.mean(x_layer2_test, axis=1)\n",
    "    x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "    print(\"AVG column added - length of new row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) )) \n",
    "\n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2_final,cachedir+'x_layer2_final.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample train data size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0.pkl  exists, importing \n",
      "In:Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2.pkl  exists, importing \n",
      "Full GridSearch run time:0.003s\n",
      "CV time:0.0s\n",
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n",
      "---Fold:0 to 37663 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:83.734s\n",
      "Mean abs error: 1211.88\n",
      "-predict time:4.763s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:4.105s\n",
      "Mean abs error: 1278.46\n",
      "-predict time:0.014s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:78: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:73.652s\n",
      "Mean abs error: 1203.76\n",
      "-predict time:4.193s\n",
      "XGB Mean abs error: 1138.36\n",
      "-XGB predict time:2.746s\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 4)\n",
      "---Fold run time:436.245s\n",
      "---Fold:37663 to 75326 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:85.542s\n",
      "Mean abs error: 1211.57\n",
      "-predict time:4.653s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.944s\n",
      "Mean abs error: 1273.58\n",
      "-predict time:0.013s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.557s\n",
      "Mean abs error: 1204.21\n",
      "-predict time:4.372s\n",
      "XGB Mean abs error: 1133.70\n",
      "-XGB predict time:2.795s\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 4)\n",
      "---Fold run time:435.635s\n",
      "---Fold:75326 to 112989 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:106: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:85.513s\n",
      "Mean abs error: 1225.95\n",
      "-predict time:4.764s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.982s\n",
      "Mean abs error: 1292.36\n",
      "-predict time:0.014s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "fit time:73.971s\n",
      "Mean abs error: 1215.81\n",
      "-predict time:4.178s\n",
      "XGB Mean abs error: 1149.70\n",
      "-XGB predict time:2.656s\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 4)\n",
      "---Fold run time:431.757s\n",
      "---Fold:112989 to 150652 of: 188318\n",
      "\n",
      "---folding! len test 37663, len train 150655\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:80.324s\n",
      "Mean abs error: 1221.66\n",
      "-predict time:4.497s\n",
      "Ridge(alpha=40, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:3.794s\n",
      "Mean abs error: 1296.75\n",
      "-predict time:0.012s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "regrList,xgb=train_layer1_for_dataset(x,y,data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,data_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
