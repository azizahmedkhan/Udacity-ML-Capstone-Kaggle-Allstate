{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Model - Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "test.csv.zip\n",
      "test_data_all_features.csv\n",
      "test_data_cats.csv\n",
      "test_data_conts.csv\n",
      "test_data_new.csv\n",
      "test_data_orig_only.csv\n",
      "test_data_validation_train_all_features.csv\n",
      "test_data_validation_train_cats.csv\n",
      "test_data_validation_train_conts.csv\n",
      "test_data_validation_train_new.csv\n",
      "test_data_validation_train_orig_only.csv\n",
      "train.csv\n",
      "train.csv.zip\n",
      "train_data_all_features.csv\n",
      "train_data_cats.csv\n",
      "train_data_conts.csv\n",
      "train_data_new.csv\n",
      "train_data_orig_only.csv\n",
      "train_data_validation_train_all_features.csv\n",
      "train_data_validation_train_cats.csv\n",
      "train_data_validation_train_conts.csv\n",
      "train_data_validation_train_new.csv\n",
      "train_data_validation_train_orig_only.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "#from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "from subprocess import check_output\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "#XGB params:\n",
    "\n",
    "#params from:\n",
    "#https://www.kaggle.com/mnabaee/allstate-claims-severity/labelencoding-and-xgb-cv/discussion\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.3085,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 10,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 4.2922,\n",
    "    'eval_metric': 'mae',\n",
    "    'eta':0.001,\n",
    "    'gamma': 0.5290,\n",
    "    'subsample':0.9930,\n",
    "    'max_delta_step':0,\n",
    "    'booster':'gbtree',\n",
    "    'nrounds': 1001\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "\n",
    "use_xgb=True #disable for speed\n",
    "\n",
    "shift=200\n",
    "data_sets=['new','conts','cats','orig_only','all_features']\n",
    "\n",
    "datadir=\"./input/\"\n",
    "cachedir=\"./cache/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "#print(check_output([\"ls\", cachedir]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeansPlusmeanshift(data):\n",
    "    start_time = time.time()\n",
    "    startingClusterSize=int(len(data)*.075)\n",
    "    print \"kmeans.... for {} clusters\".format(startingClusterSize)\n",
    "    k_means =KMeans(n_clusters=startingClusterSize,n_jobs=10)\n",
    "    k_means.fit(data.sample(frac=0.35).values)\n",
    "    clusters=k_means.cluster_centers_\n",
    "    print(\"kmeans round 1 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print clusters[:15]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #use the cluster centers of the guessed clusters to get an estimate of actual numbers of clusters. doing this for speed increase!\n",
    "    print \"\\nmeanshift...\"\n",
    "    meanshift=MeanShift(n_jobs=10)\n",
    "    meanshift.fit(clusters)\n",
    "    newcenters=meanshift.cluster_centers_\n",
    "    print(\"meanshift time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    print newcenters[:15], \"\\nnum of clusters from meanshift:\",len(newcenters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift +1 to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=len(newcenters)+1,n_jobs=10)\n",
    "    final_clusters=k_means.fit_predict(data.values)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepdata(data_name,verbose=False):\n",
    "    ### and now, let's import the data\n",
    "    data = loadData(datadir,'train_data_'+data_name+'.csv')\n",
    "    if verbose==True:\n",
    "        display(data.info())\n",
    "        display(data.head(2))\n",
    "\n",
    "    test_data= loadData(datadir,'test_data_'+data_name+'.csv') \n",
    "    if verbose==True:\n",
    "        display(test_data.info())\n",
    "        display(test_data.head(2))\n",
    "    # we don't want the ID columns in X\n",
    "    x=data.drop(['id','loss'],1).values\n",
    "    # loss is our label\n",
    "    #y=data['loss'].values\n",
    "    y = np.log(data['loss']+shift).ravel()\n",
    "\n",
    "    return x,y,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(x,y,regr,param,regr_name='BLANK'):\n",
    "    start_time = time.time()\n",
    "    print(\"In:{}\".format(regr))\n",
    "    filename= 'grid_{}.pkl'.format(regr_name)\n",
    "    if os.path.isfile(cachedir+filename):\n",
    "        print filename,\" exists, importing \"\n",
    "        return joblib.load(cachedir+filename) \n",
    "    else:\n",
    "        print(\"{} not present, running a gridsearch\".format(filename))\n",
    "        #search the param_grid for best params based on the f1 score\n",
    "        grid_search = GridSearchCV(regr,\n",
    "                                   param_grid= param,\n",
    "                                   n_jobs=-1,\n",
    "                                   scoring=make_scorer(mean_absolute_error,greater_is_better=False)) \n",
    "        print(\"debug 1\")\n",
    "        grid_search.fit(x,y)\n",
    "        print \"debug2\"\n",
    "        #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "        params={}\n",
    "        for p in grid_search.best_params_:\n",
    "            params[p]=grid_search.best_params_[p]\n",
    "        regr.set_params(**params)\n",
    "        print(\"run time:{}s\".format(round((time.time()-start_time), 3) ))   \n",
    "        joblib.dump(regr,cachedir+filename) \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the planned regressions for layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      " SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)]\n",
      "[ {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [3, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {'alpha': [0.05, 0.5, 1, 2, 4, 40, 140, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 7, 10, 25, 50, 200, 500]}\n",
      " {}\n",
      " {'n_neighbors': [2, 5, 7, 15], 'leaf_size': [3, 10, 15, 25, 30, 50, 100]}\n",
      " {}]\n",
      "('number of scikitlearn regressors to use:', 6)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[3,5,7,10,25,50,200,500],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.05,.5,1,2,4,40,140,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[2,5,7,10,25,50,200,500],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([LinearRegression(), dict()])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7,15],\n",
    "                             leaf_size =[3,10,15,25,30,50,100])])\n",
    "regressor_w_grid.append([SVR(), dict()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_layer1_for_dataset(x,y,dataset):\n",
    "    #  train/validation split\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                    y,\n",
    "                                                                   test_size=0.20,\n",
    "                                                                    random_state=42)\n",
    "    display(\"sample train data: {}: size:{}\".format(dataset, len( y_train)))\n",
    "    start_time0 = time.time()\n",
    "    \n",
    "    for i in range(len(regrList)):\n",
    "        regrList[i]=grid_search_wrapper(X_train,y_train,regrList[i],paramater_grid[i],regr_name=\"regr{}-{}\".format(i,dataset))\n",
    "\n",
    "    print(\"Full GridSearch run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "\n",
    "\n",
    "    # XGB CV\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    start_time = time.time()\n",
    "    #res = xgb.cv(xgb_params, dtrain, num_boost_round=2001, nfold=4, seed=42, stratified=False,\n",
    "    #             early_stopping_rounds=50, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"CV time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "   \n",
    "    #best_nrounds = res.shape[0] - 1\n",
    "    #cv_mean = res.iloc[-1, 0]\n",
    "    #cv_std = res.iloc[-1, 1]\n",
    "    #print('CV-Mean: {0}+{1}, best rounds:{2}'.format(cv_mean, cv_std,best_nrounds))\n",
    "    best_nrounds=2000 # various tests, this always resulted, not going to run due to speed\n",
    "    #prepare the fold divisions\n",
    "\n",
    "    data_size=x.shape[0]\n",
    "    print \"size of train data:\",data_size\n",
    "    folds=[]\n",
    "    num_folds=5\n",
    "    fold_start=0\n",
    "    for k in range(num_folds-1):\n",
    "        fold_end=((data_size/num_folds)*(k+1))\n",
    "        folds.append((fold_start,fold_end))\n",
    "        fold_start=fold_end\n",
    "    folds.append((fold_start,data_size))\n",
    "    print \"folds at:\",folds\n",
    "    print \"fold size:\", (data_size/num_folds)\n",
    "    print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "    count=0\n",
    "    for i in folds:\n",
    "        count+=i[1]-i[0]\n",
    "    print count\n",
    "    \n",
    "    x_layer2=[]\n",
    "    start_time0 = time.time()\n",
    "    MAE_tracking=[]\n",
    "\n",
    "    if os.path.isfile(cachedir+'x_layer2'+dataset+'.npy'):\n",
    "        print 'x_layer2'+dataset+'.npy',\" exists, importing \"\n",
    "        #reuse the run\n",
    "        x_layer2=joblib.load(cachedir+'x_layer2'+dataset+'.npy') \n",
    "        MAE_tracking=joblib.load(cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "    else:\n",
    "        for fold_start,fold_end in folds:\n",
    "            print(\"---Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "            start_time1 = time.time()\n",
    "            fold_result=[]\n",
    "\n",
    "            X_test = x[fold_start:fold_end].copy()\n",
    "            y_test = y[fold_start:fold_end].copy()\n",
    "            X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "            y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "            print \"\\n---folding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "\n",
    "            for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "                print(regrList[i])\n",
    "                start_time = time.time()\n",
    "                estimator=skclone(regrList[i], safe=True)\n",
    "                estimator.fit(X_train,y_train)\n",
    "                print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                start_time = time.time()\n",
    "                curr_predict=np.array(estimator.predict(X_test)).copy()\n",
    "                if fold_result == []:\n",
    "                    fold_result = curr_predict\n",
    "                else:\n",
    "                    fold_result = np.column_stack((fold_result,curr_predict))  \n",
    "                #show some stats on that last regressions run\n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "                print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "                #print(\"Score: {:.2f}\".format(estimator.score(X_test, y_test))) #delays the run...\n",
    "\n",
    "            #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "            if use_xgb == True:\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dtest = xgb.DMatrix(X_test)\n",
    "                #gbdt=xgbfit(X_train,y_train)\n",
    "                gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "                # now do a prediction and spit out a score(MAE) that means something\n",
    "                start_time = time.time()\n",
    "                curr_predict=gbdt.predict(dtest)\n",
    "                fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "                #MAE=np.mean(abs(curr_predict - y_test))\n",
    "                MAE=np.mean(abs(np.exp(curr_predict) - np.exp(y_test)))\n",
    "                MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "                print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "                print(\"-XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "            if x_layer2 == []:\n",
    "                x_layer2=fold_result\n",
    "            else:\n",
    "                x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "\n",
    "            print \"--layer2 length:\",len(x_layer2)\n",
    "            print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "            print(\"---Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "        print(\"----Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n",
    "        #preserve the run\n",
    "        joblib.dump(x_layer2,cachedir+'x_layer2'+dataset+'.npy') \n",
    "        joblib.dump(MAE_tracking,cachedir+'MAE_tracking-'+dataset+'.npy')\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "\n",
    "    avg_column=np.mean(x_layer2, axis=1)\n",
    "\n",
    "    #MAE=np.mean(abs(avg_column - y))\n",
    "    MAE=np.mean(abs(np.exp(avg_column) - np.exp(y)))\n",
    "    print(\"avgd Mean abs error: {:.2f}\".format(MAE))\n",
    "    x_layer2=np.column_stack((x_layer2,avg_column))\n",
    "    print(\"length of new row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    # use the new clusters number to predict each locations cluster\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(x_layer2)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "\n",
    "    x_layer2=np.column_stack((x_layer2,final_clusters))\n",
    "    print(\"length of row: {}\".format(len(x_layer2[0])))\n",
    "    \n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2,cachedir+'x_layer2_train_{}.npy'.format(dataset)) \n",
    "    return regrList,xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,dataset):  \n",
    "    print \"predicting for dataset {}\".format(dataset)\n",
    "    x_layer2_test = []\n",
    "    start_time1 = time.time()\n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()            \n",
    "        estimator=skclone(regrList[i], safe=True)\n",
    "        print(estimator)\n",
    "        estimator.fit(x,y) # use the estimator from the training, but refit to the whole data set!\n",
    "        curr_predict=estimator.predict(x_test_data)\n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        if x_layer2_test == []:\n",
    "            x_layer2_test = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "\n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    best_nrounds=2000\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    gbdt=xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    # add an avged column of all the runs\n",
    "    avg_column=np.mean(x_layer2_test, axis=1)\n",
    "    x_layer2_test=np.column_stack((x_layer2_test,avg_column))\n",
    "    print(\"AVG column added - length of new row: {}\".format(len(x_layer2_test[0])))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print \"\\nredo kmeans with new cluster number from meanshift to account for sampling...\"\n",
    "    k_means =KMeans(n_clusters=80,n_jobs=12)\n",
    "    final_clusters=k_means.fit_predict(x_layer2_test)\n",
    "    print(\"kmeans round 2 time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    x_layer2_test=np.column_stack((x_layer2_test,final_clusters))\n",
    "    \n",
    "    # save the results!\n",
    "    joblib.dump(x_layer2_test,cachedir+'x_layer2_test_{}.npy'.format(dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Starting dataset:new for layer1-----------\n",
      "loading: ./input/train_data_new.csv\n",
      "Dataset has 188318 samples with 5 features each.\n",
      "loading: ./input/test_data_new.csv\n",
      "Dataset has 125546 samples with 5 features each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample train data: new: size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0-new.pkl  exists, importing \n",
      "In:Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1-new.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2-new.pkl  exists, importing \n",
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_regr3-new.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr4-new.pkl  exists, importing \n",
      "In:SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "grid_regr5-new.pkl  exists, importing \n",
      "Full GridSearch run time:0.041s\n",
      "CV time:0.0s\n",
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n",
      "x_layer2new.npy  exists, importing \n",
      "avgd Mean abs error: 1578.28\n",
      "length of new row: 5\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:3.99s\n",
      "length of row: 5\n",
      "length of row: 6\n",
      "predicting for dataset new\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=7, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.228s\n",
      "Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:3.139s\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "predict time:0.021s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=15, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:7.698s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:2612.583s\n",
      "XGB predict time:2664.078s\n",
      "AVG column added - length of new row: 8\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:2.717s\n",
      "-----------Starting dataset:conts for layer1-----------\n",
      "loading: ./input/train_data_conts.csv\n",
      "Dataset has 188318 samples with 16 features each.\n",
      "loading: ./input/test_data_conts.csv\n",
      "Dataset has 125546 samples with 16 features each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample train data: conts: size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=7, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0-conts.pkl  exists, importing \n",
      "In:Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1-conts.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2-conts.pkl  exists, importing \n",
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_regr3-conts.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=15, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr4-conts.pkl  exists, importing \n",
      "In:SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "grid_regr5-conts.pkl  exists, importing \n",
      "Full GridSearch run time:0.079s\n",
      "CV time:0.0s\n",
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n",
      "x_layer2conts.npy  exists, importing \n",
      "avgd Mean abs error: 1841.73\n",
      "length of new row: 5\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:24.13s\n",
      "length of row: 5\n",
      "length of row: 6\n",
      "predicting for dataset conts\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:23.618s\n",
      "Ridge(alpha=4, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.337s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:46.464s\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "predict time:0.263s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "predict time:4.943s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:3364.246s\n",
      "XGB predict time:3609.783s\n",
      "AVG column added - length of new row: 8\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:65.331s\n",
      "-----------Starting dataset:cats for layer1-----------\n",
      "loading: ./input/train_data_cats.csv\n",
      "Dataset has 188318 samples with 118 features each.\n",
      "loading: ./input/test_data_cats.csv\n",
      "Dataset has 125546 samples with 118 features each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample train data: cats: size:150654'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0-cats.pkl  exists, importing \n",
      "In:Ridge(alpha=4, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1-cats.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2-cats.pkl  exists, importing \n",
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_regr3-cats.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr4-cats.pkl  exists, importing \n",
      "In:SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "grid_regr5-cats.pkl  exists, importing \n",
      "Full GridSearch run time:0.055s\n",
      "CV time:0.0s\n",
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n",
      "x_layer2cats.npy  exists, importing \n",
      "avgd Mean abs error: 1189.88\n",
      "length of new row: 5\n",
      "\n",
      "redo kmeans with new cluster number from meanshift to account for sampling...\n",
      "kmeans round 2 time:25.159s\n",
      "length of row: 5\n",
      "length of row: 6\n",
      "predicting for dataset cats\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:32.387s\n",
      "Ridge(alpha=4, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ac1c70e18868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mregrList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_layer1_for_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpredict_layer1_for_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregrList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Layer 1 done for all data sets!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-2d90ff589031>\u001b[0m in \u001b[0;36mpredict_layer1_for_dataset\u001b[1;34m(x, y, x_test_data, regrList, xgb, dataset)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregrList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# use the estimator from the training, but refit to the whole data set!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcurr_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "\n",
    "for data_name in data_sets:\n",
    "    print \"-----------Starting dataset:{} for layer1-----------\".format(data_name)\n",
    "    x,y,test_data=prepdata(data_name)\n",
    "    x_test_data=test_data.drop(['loss','id'],1).values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "    regrList,xgb=train_layer1_for_dataset(x,y,data_name)\n",
    "\n",
    "    predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,data_name) \n",
    "\n",
    "print \"Layer 1 done for all data sets!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the layer two stack\n",
    "x_layer2_train_final = None\n",
    "\n",
    "for data_name in data_sets[:]:\n",
    "    print \"-----------Starting {} for layer1-----------\".format(data_name)\n",
    "    \n",
    "    \n",
    "    x_layer2_train=joblib.load(cachedir+'x_layer2_train_{}.npy'.format(data_name)) \n",
    "    x_layer2_test=joblib.load(cachedir+'x_layer2_test_{}.npy'.format(data_name)) \n",
    "    print(\"Train:\",x_layer2_train.shape)\n",
    "    print(\"Test:\",x_layer2_test.shape)\n",
    "    if x_layer2_train_final is None:\n",
    "        x_layer2_train_final=x_layer2_train\n",
    "        x_layer2_test_final=x_layer2_test\n",
    "    else:\n",
    "        x_layer2_train_final = np.column_stack((x_layer2_train_final,x_layer2_train))\n",
    "        x_layer2_test_final = np.column_stack((x_layer2_test_final,x_layer2_test))\n",
    "\n",
    "joblib.dump(x_layer2_train_final,cachedir+'x_layer2_train_final.npy') \n",
    "joblib.dump(x_layer2_test_final,cachedir+'x_layer2_test_final.npy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train:\",x_layer2_train_final.shape)\n",
    "print(\"Test:\",x_layer2_test_final.shape)\n",
    "# output should be: (same,sum inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's get an idea of the layer 1 results.\n",
    "plt.figure(figsize=(16,6))\n",
    "for data_name in data_sets[:]:\n",
    "    print \"-----------Starting graphs for {} in layer1-----------\".format(data_name)\n",
    "    MAE_tracking_graph=np.array(joblib.load(cachedir+'MAE_tracking-{}.npy'.format(data_name))) \n",
    "    print(MAE_tracking_graph.T)\n",
    "    plt.plot(MAE_tracking_graph.T[1],label=data_name)\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Track the MAE for each run/regressor\")\n",
    "plt.xticks(range(len(MAE_tracking_graph.T[0])),MAE_tracking_graph.T[0], rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Starting dataset:validation_train_new for layer1-----------\n",
      "loading: ./input/train_data_validation_train_new.csv\n",
      "Dataset has 150654 samples with 5 features each.\n",
      "loading: ./input/test_data_validation_train_new.csv\n",
      "Dataset has 37664 samples with 5 features each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample train data: validation_train_new: size:120523'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "grid_regr0-validation_train_new.pkl  exists, importing \n",
      "In:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "grid_regr1-validation_train_new.pkl  exists, importing \n",
      "In:RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "grid_regr2-validation_train_new.pkl  exists, importing \n",
      "In:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "grid_regr3-validation_train_new.pkl  exists, importing \n",
      "In:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "grid_regr4-validation_train_new.pkl  exists, importing \n",
      "In:SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "grid_regr5-validation_train_new.pkl  exists, importing \n",
      "Full GridSearch run time:0.004s\n",
      "CV time:0.0s\n",
      "size of train data: 150654\n",
      "folds at: [(0, 30130), (30130, 60260), (60260, 90390), (90390, 120520), (120520, 150654)]\n",
      "fold size: 30130\n",
      "train size: 120520\n",
      "150654\n",
      "---Fold:0 to 30130 of: 150654\n",
      "\n",
      "---folding! len test 30130, len train 120524\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "fit time:0.121s\n",
      "Mean abs error: 1556.43\n",
      "-predict time:0.133s\n",
      "Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "fit time:0.006s\n",
      "Mean abs error: 1822.14\n",
      "-predict time:0.003s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:78: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:1.963s\n",
      "Mean abs error: 1556.33\n",
      "-predict time:0.381s\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "\n",
      "fit time:0.014s\n",
      "Mean abs error: 1822.14\n",
      "-predict time:0.003s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "          weights='uniform')\n",
      "\n",
      "fit time:0.397s\n",
      "Mean abs error: 1591.48\n",
      "-predict time:1.323s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# for validation data\n",
    "for data_name in data_sets:\n",
    "    data_name=\"validation_train_\"+data_name\n",
    "    print \"-----------Starting dataset:{} for layer1-----------\".format(data_name)\n",
    "    x,y,test_data=prepdata(data_name)\n",
    "    x_test_data=test_data.drop(['loss','id'],1).values# didn't have the loss column before, make it go away! don't need ID!\n",
    "\n",
    "    regrList,xgb=train_layer1_for_dataset(x,y,data_name)\n",
    "\n",
    "    predict_layer1_for_dataset(x,y,x_test_data,regrList,xgb,data_name) \n",
    "\n",
    "print \"Layer 1 done for all data sets!\"\n",
    "\n",
    "# build the layer two stack\n",
    "x_layer2_train_final = None\n",
    "\n",
    "for data_name in data_sets[:]:\n",
    "    print \"-----------Starting {} for layer1-----------\".format(data_name)\n",
    "    data_name=\"validation_train_\"+data_name\n",
    "    \n",
    "    x_layer2_train=joblib.load(cachedir+'x_layer2_train_{}.npy'.format(data_name)) \n",
    "    x_layer2_test=joblib.load(cachedir+'x_layer2_test_{}.npy'.format(data_name)) \n",
    "    print(\"Train:\",x_layer2_train.shape)\n",
    "    print(\"Test:\",x_layer2_test.shape)\n",
    "    if x_layer2_train_final is None:\n",
    "        x_layer2_train_final=x_layer2_train\n",
    "        x_layer2_test_final=x_layer2_test\n",
    "    else:\n",
    "        x_layer2_train_final = np.column_stack((x_layer2_train_final,x_layer2_train))\n",
    "        x_layer2_test_final = np.column_stack((x_layer2_test_final,x_layer2_test))\n",
    "\n",
    "joblib.dump(x_layer2_train_final,cachedir+'x_layer2_train_validation.npy') \n",
    "joblib.dump(x_layer2_test_final,cachedir+'x_layer2_test_validation.npy')\n",
    "\n",
    "train_data_validation_train_new.csv\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
